\section{Introduction}

% Introduction motivates the readers with the following aspects.
% - Bare-metal cloud or hardware as a service (HaaS)
% - Server support for HaaS
% - Network support for HaaS
% - Apply the virtualization to enhance the manageability of
%   servers in a HaaS
% - Single-VM virtualization requirements
%   - Direct device assignment for all PCIe devices
%   - Direct interrupt delivery
%   - Migration of bare-metal server
%   - VM introspection for the security and better visibility
\mycomment{
Infrastructure as a service (IaaS), which was popularized by AWS's EC2 service~\cite{ec2}, has evolved and morphed into multiple forms over the last decade.
The basic compute unit for IaaS began as a {\em virtual machine} (VM), which represents a slice of a physical machine carved out by a hardware-abstraction-layer, called the hypervisor.
A {\em container} is another basic compute unit, where a common operating system (OS) delimits the addressable system resources (or namespaces) for a group of processes and enforces usage limits.
A more recent IaaS compute unit is a {\em function}, which comes with a complete operating environment consisting of an OS and a middleware layer, and is created on demand.  
}

%Lately, to avoid multi-tenancy and security issues, a physical machine itself is 
%treated as a basic compute unit in {\em bare-metal cloud service}~\cite{bms-wiki}.
%or {\em hardware-as-a-service} (HaaS). 
%Infrastructure as a service (IaaS), which was popularized by AWS's EC2 service, has evolved and morphed into multiple forms over the last decade.
%In the beginning, the basic compute unit for IaaS was a {\em virtual machine}, which represents a slice of a physical machine carved out by a hardware-abstraction-layer hypervisor.
%Then the basic compute unit could also be a {\em container}, which is pre-configured with an operating system and corresponds to a piece of a physical machine delimited by that OS.
%%A more recent option for IaaS's basic compute unit is a {\em function}, which comes with a complete operating environment constsing of an OS and a middleware layer, and is created on demand.  
%Lately, even a physical machine could serve as the basic compute unit. This type of IaaS is known as {\em bare-metal cloud service} or {\em hardware as a service} (HaaS). 
%In the past three years, we have been developing a HaaS operating system called {\em ITRI HaaS OS} or \sna.  
%The focus of this paper is on \sna's virtualization support that enhances the manageability and serviceability required of a modern bare-metal cloud service. 


Conventional multi-tenant cloud services~\cite{ec2,azure,gcp} enable
users to rent virtual machines (VMs) or containers to scale 
up their IT infrastructure to the cloud. However, virtualization
introduces both performance overheads and security concerns
arising from co-located workloads of other users.
To address this concern, cloud operators 
such as  IBM SoftLayer~\cite{softlayer} and Oracle~\cite{oracle},
have begun to offer bare-metal cloud service, or Hardware-as-a-Service (HaaS),
%In the case of traditional multi-tenant IaaS, cloud operators own and manage 
%the physical machines, which are shared among multiple users.
%In contrast, bare-metal cloud operators, 
which allow users to rent dedicated  physical machines.
HaaS clouds enables users combine the benefits of 
scaling up their operations in the cloud with having dedicated 
hardware; users are assured stronger isolation than multi-tenant clouds and 
bare-metal performance for critical workloads 
such as high-performance computing, big data analytics, and AI.
%Other use cases of bare-metal cloud services include a preferred hypervisor 
%or OS that is not supported by cloud operators or special hardware for which virtualization 
%is not sufficiently mature, such as 
%GPUs, SoC-based micro-servers, and application-specific FPGA accelerators.

However, common management functions available on multi-tenant clouds,
such as live migration and introspection-based 
application performance management, are difficult to 
duplicate on HaaS servers, because HaaS providers typically 
do not install any software on these dedicated servers.

To address this manageability gap of existing HaaS platforms, 
we have been developing a HaaS management system, called 
%TODO: uncomment later
%\fullname (\sna) 
IHO,
with the goal of enhancing the manageability and serviceability 
of bare-metal cloud services.
The focus of this paper is on IHO's virtualization support
in the form of a specialized hypervisor, called the {\em Single VM
hypervisor (\sna)}.


\figw{architecture}{8.75}{Architecture of \na for single-VM
virtualization: The \na runs as a thin hypervisor that
provides manageability services such as live migration and VM
monitoring. 
The HaaS VM runs with dedicated CPU cores, memory, and I/O devices.
All interrupts, including device, local timers, and IPIs
are directly deivered from the hardware to the HaaS VM. }

Figure~\ref{fig:architecture} shows the high-level
architecture of \sna, which runs as a thin hypervisor on each
physical server. \na is optimized to run one VM, called the 
{\em HaaS VM}\footnote{The \na could conceptually be an extension of a physical server's trusted BIOS.}, 
on which  a user install a preferred OS and applications. 

Unlike traditional hypervisors, which are designed to limit and control a VM's 
access to physical resources, \na is designed to maximize the HaaS VM's 
access to physical hardware.
During normal execution, the \na allows 
the HaaS VM to directly interact with physical I/O devices and processor hardware
without the hypervisor's intervention, 
as if it runs directly on a physical server with near bare-metal performance.
\na primarily provides value-added manageability 
features of conventional clouds, such as live
migration, introspection, and performance monitoring.
A small HaaS agent in the guest (a self-contained kernel module)
transparently coordinates these services with the \sna.
Specifically, \na provides the following features for a HaaS VM. 

%Bare-metal cloud operators provide a user with a physical data center instance (PDCI), which is 
%composed of a set of physical machines connected in a way specified by the user. 
%In the past three years, we have been developing a HaaS operating system called 
%
%
%
%
%A HaaS user or tenant makes a HaaS service request to \na by specifying a PDCI, which consists of 
%The HaaS offerings from cloud operators such as IBM (SoftLayer) and Oracle provide a user a physical data center instance (PDCI), which is composed of a set of physical machines connected in a way specified by the user. HaaS users prefer physical machines to virtual machines primarily because they want to make the best of the underlying hardware resources for workloads that do not need the flexibility afforded by virtualization, such as HPC computation, big data analytics or AI training.
%Other HaaS use cases include that users have a preferred hypervisor or operating system which is not supported by cloud operators, and 
%that users need special hardware for which virtualization is not sufficiently mature, such as ARM SOC-based micro-server and GPU/FPGA cluster.
%
%In the case of IaaS, cloud operators own and manage the physical machines.
%In contrast, for HaaS, cloud operators own the physical machines but users manage them. 
%This way, HaaS users are still able to enjoy the multiplexing benefits of cloud computing that are due to sharing of hardware and facilities.
%A HaaS user or tenant makes a HaaS service request to \na by specifying a PDCI, which consists of 
%\begin{itemize} 
%\parskip 0mm
%\itemsep 0mm
%\item A set of physical servers, each with its CPU/memory/PCIe device specification, and configurations on the BIOS, and PCI devices,
%
%\item A set of storage volumes that exist in local or shared storage, and are attached to the servers,
%
%\item A set of IP subnets that describe how the servers are connected with one another and to the Internet, and  
%
%\item A set of public IP addresses to be bound to some of the servers facing the Internet, and their firewall policies. 
%
%\end{itemize}
%\na processes each PDCI request by first making corresponding allocations for server, network and storage resources, and 
%then setting up the PDCI's required network connectivity.  Because a HaaS operator cannot install any agent software on the physical servers rented out on a PDCI, the only way for \na to programmatically build a virtual network that meets a PDCI's IP subset specification is to leverage the VLAN and VXLAN capabilities in modern network switches and routers by properly configuring them according to the network connectivity specification.  
%Moreover, \na allows a HaaS tenant to {\em remotely} check, configure, and update the firmware on the physical servers, as well as install the desired operating systems and applications 
%on them, in a way that minimizes human errors and the adverse side effects that come with these errors. 
%Finally, \na enables a HaaS tenant to monitor the hardware status of the physical servers and the network traffic among them with full visibility, without revealing anything associated with other co-located tenants. 
%
%
%For the HaaS use case in which a tenant installs an operating system (Linux or Windows) rather than a 
%proprietary hypervisor on the physical servers of its PDCI, 
%

\begin{itemize} 
%\setlength\itemsep{-0.04in}
\parskip 0mm
\itemsep 0mm

\item {\bf Direct access to dedicated hardware:} 
\na assigns a HaaS VM with 
dedicated hardware resources, namely 
physical CPUs, memory, local APIC timers, and passthrough I/O devices,
which the VM can access directly 
during its normal execution without hypervisor intervention. 
The \sna, however, maintains its ability to
regain control over physical resources when needed, such 
before live migration of the HaaS VM.


\item {\bf Direct delivery of all interrupts:}
\na enables a HaaS VM to directly receive all hardware interrupts from 
its assigned PCIe devices and CPU cores (for timer interrupts and IPIs) 
without interception and emulation by the hypervisor.
Direct interrupt delivery to the HaaS VM greatly
reduces interrupt processing latency, which is imoportant for running 
latency-critical applications on bare-metal clouds.
In contrast, existing approaches~\cite{vfio,postedinterrupt,amit:2015,tu:2015}
focus only on direct delivery of PCIe device interrupts.


\item {\bf Seamless live migration:}
\na provides on-demand live migration of a HaaS VM having direct access to 
physical network devices and local APIC timers. 
%through coordinated switching to para-virtual I/O during migration.
\na seamlessly disables the HaaS VM's physical hardware access at the 
source machine before migration and 
re-establishes access at the destination after migration,
with minimal disruption to the VM's liveness and performance.
Unlike existing live migration~\cite{vfio-live-migration,blmvisor-journal,ondemand} 
approaches for VMs with pasthrough 
I/O access, \na does not require device-specific state capture and migration code 
nor does it require the hypervisor to trust the guest OS. 
%Yet \na matches the liveness and performance of traditional live migration.

\item {\bf Baremetal performance:}  
To match bare-metal performance, \na implements mechanisms
for reducing the its CPU, I/O, and memory footprint
during runtime and minimizing interference with HaaS VM's execution.
\na has a small execution footprint, currently one 
CPU core and around 100MB of RAM, with room for further reductions.
Even with passthrough I/O support~\cite{intelvtd-paper,intelvtd-manual}, 
VM exits can still lower the I/O throughput and increase CPU usage of the host.
\na includes mechanisms to systematically eliminate
most VM exits for a HaaS VM to match bare-metal I/O performance and CPU utilization.
Other than during startup and live migration, the \na is not involved in the
HaaS VM's normal execution.

\end{itemize}

%With such a hypervisor installed on each physical server, \na can monitor each 
%server's internal user activities using VM introspection, and seamlessly migrate the user state 
%on the server from one physical machine to another. 

The rest of this paper is organized as follows.
We first describe the design and implementation of \sna,
specifically the detailed description of the above features.
Next, we evaluate the performance of our \na prototype
on the Linux and KVM/QEMU platform.
Finally, we discuss related work followed by conclusions.


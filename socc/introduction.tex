\section{Introduction}

% Introduction motivates the readers with the following aspects.
% - Bare-metal cloud or hardware as a service (HaaS)
% - Server support for HaaS
% - Network support for HaaS
% - Apply the virtualization to enhance the manageability of
%   servers in a HaaS
% - Single-VM virtualization requirements
%   - Direct device assignment for all PCIe devices
%   - Direct interrupt delivery
%   - Migration of bare-metal server
%   - VM introspection for the security and better visibility
\mycomment{
Infrastructure as a service (IaaS), which was popularized by AWS's EC2 service~\cite{ec2}, has evolved and morphed into multiple forms over the last decade.
The basic compute unit for IaaS began as a {\em virtual machine} (VM), which represents a slice of a physical machine carved out by a hardware-abstraction-layer, called the hypervisor.
A {\em container} is another basic compute unit, where a common operating system (OS) delimits the addressable system resources (or namespaces) for a group of processes and enforces usage limits.
A more recent IaaS compute unit is a {\em function}, which comes with a complete operating environment consisting of an OS and a middleware layer, and is created on demand.  
}

%Lately, to avoid multi-tenancy and security issues, a physical machine itself is 
%treated as a basic compute unit in {\em bare-metal cloud service}~\cite{bms-wiki}.
%or {\em hardware-as-a-service} (HaaS). 
%Infrastructure as a service (IaaS), which was popularized by AWS's EC2 service, has evolved and morphed into multiple forms over the last decade.
%In the beginning, the basic compute unit for IaaS was a {\em virtual machine}, which represents a slice of a physical machine carved out by a hardware-abstraction-layer hypervisor.
%Then the basic compute unit could also be a {\em container}, which is pre-configured with an operating system and corresponds to a piece of a physical machine delimited by that OS.
%%A more recent option for IaaS's basic compute unit is a {\em function}, which comes with a complete operating environment constsing of an OS and a middleware layer, and is created on demand.  
%Lately, even a physical machine could serve as the basic compute unit. This type of IaaS is known as {\em bare-metal cloud service} or {\em hardware as a service} (HaaS). 
%In the past three years, we have been developing a HaaS operating system called {\em ITRI HaaS OS} or \sna.  
%The focus of this paper is on \sna's virtualization support that enhances the manageability and serviceability required of a modern bare-metal cloud service. 


Conventional multi-tenant cloud services~\cite{ec2,azure,gcp} enable
users to rent virtual machines (VMs) or containers to scale 
up their IT infrastructure to the cloud. However, virtualization
introduces both performance overheads and security concerns
arising from co-located workloads of other users.
To address this concern, cloud operators 
such as  IBM SoftLayer~\cite{softlayer} and Oracle~\cite{oracle},
have begun to offer bare-metal cloud service, or Hardware-as-a-Service (HaaS),
%In the case of traditional multi-tenant IaaS, cloud operators own and manage 
%the physical machines, which are shared among multiple users.
%In contrast, bare-metal cloud operators, 
which allow users to rent dedicated  physical machines.
HaaS clouds enables users combine the benefits of 
scaling up their operations in the cloud and having dedicated 
hardware; users are assured stronger isolation than multi-tenant clouds and 
bare-metal performance for critical workloads 
such as high-performance computing, big data analytics, and AI.
%Other use cases of bare-metal cloud services include a preferred hypervisor 
%or OS that is not supported by cloud operators or special hardware for which virtualization 
%is not sufficiently mature, such as 
%GPUs, SoC-based micro-servers, and application-specific FPGA accelerators.

However, common management functions available on multi-tenant clouds,
such as live migration and introspection-based 
application performance management, are difficult to 
duplicate on HaaS servers, because HaaS providers typically 
do not install any software on these dedicated servers.

To address this manageability gap of existing HaaS platforms, 
we have been developing a HaaS management system, called 
%TODO: uncomment later
%\fullname (\sna) 
\sna,
with the goal of enhancing the manageability and serviceability 
of modern bare-metal cloud services.
The focus of this paper is on \sna's support for a 
specialized hypervisor,  called the Single VM
hypervisor (SVMvisor)\footnote{The SVMvisor could conceptually be an extension of a physical server's trusted BIOS.},
that runs on each physical server 
and is optimized to run a single VM, called the HaaS VM.

During normal execution, the SVMvisor allows 
the HaaS VM to directly interact with physical I/O devices and processor hardware
without the hypervisor's intervention, 
as if it runs directly on a physical server with negligible performance overheads.
At the same time, the SVMvisor can provide the HaaS VM with 
features of conventional clouds, such as 
live migration and VM introspection.

%Bare-metal cloud operators provide a user with a physical data center instance (PDCI), which is 
%composed of a set of physical machines connected in a way specified by the user. 
%In the past three years, we have been developing a HaaS operating system called 
%
%
%
%
%A HaaS user or tenant makes a HaaS service request to \na by specifying a PDCI, which consists of 
%The HaaS offerings from cloud operators such as IBM (SoftLayer) and Oracle provide a user a physical data center instance (PDCI), which is composed of a set of physical machines connected in a way specified by the user. HaaS users prefer physical machines to virtual machines primarily because they want to make the best of the underlying hardware resources for workloads that do not need the flexibility afforded by virtualization, such as HPC computation, big data analytics or AI training.
%Other HaaS use cases include that users have a preferred hypervisor or operating system which is not supported by cloud operators, and 
%that users need special hardware for which virtualization is not sufficiently mature, such as ARM SOC-based micro-server and GPU/FPGA cluster.
%
%In the case of IaaS, cloud operators own and manage the physical machines.
%In contrast, for HaaS, cloud operators own the physical machines but users manage them. 
%This way, HaaS users are still able to enjoy the multiplexing benefits of cloud computing that are due to sharing of hardware and facilities.
%A HaaS user or tenant makes a HaaS service request to \na by specifying a PDCI, which consists of 
%\begin{itemize} 
%\parskip 0mm
%\itemsep 0mm
%\item A set of physical servers, each with its CPU/memory/PCIe device specification, and configurations on the BIOS, and PCI devices,
%
%\item A set of storage volumes that exist in local or shared storage, and are attached to the servers,
%
%\item A set of IP subnets that describe how the servers are connected with one another and to the Internet, and  
%
%\item A set of public IP addresses to be bound to some of the servers facing the Internet, and their firewall policies. 
%
%\end{itemize}
%\na processes each PDCI request by first making corresponding allocations for server, network and storage resources, and 
%then setting up the PDCI's required network connectivity.  Because a HaaS operator cannot install any agent software on the physical servers rented out on a PDCI, the only way for \na to programmatically build a virtual network that meets a PDCI's IP subset specification is to leverage the VLAN and VXLAN capabilities in modern network switches and routers by properly configuring them according to the network connectivity specification.  
%Moreover, \na allows a HaaS tenant to {\em remotely} check, configure, and update the firmware on the physical servers, as well as install the desired operating systems and applications 
%on them, in a way that minimizes human errors and the adverse side effects that come with these errors. 
%Finally, \na enables a HaaS tenant to monitor the hardware status of the physical servers and the network traffic among them with full visibility, without revealing anything associated with other co-located tenants. 
%
%
%For the HaaS use case in which a tenant installs an operating system (Linux or Windows) rather than a 
%proprietary hypervisor on the physical servers of its PDCI, 
%

Unlike traditional hypervisors, which are designed to limit and control a VM's 
access to physical resources, the SVMvisor is designed to maximize the HaaS VM's 
access to hardware while providing value-added services, such as live migration.
Specifically, SVMvisor provides the following specialized support for a HaaS VM.
\begin{itemize} 
%\setlength\itemsep{-0.04in}
\parskip 0mm
\itemsep 0mm
\item Dedicated CPU and memory resources.
\item Direct access to all PCIe devices without involving the hypervisor in data plane I/O processing.
\item Direct delivery of all interrupts, including PCIe device interrupts, local timer interrupts, and inter-processor interrupts (IPI).
\item Elimination of almost all VM exits during a HaaS VM's execution.
\item Seamless live migration of a HaaS VM that has passthrough access to  physical devices.
\item Minimal execution and memory footprint of the hypervisor, currently limited to one (non-dedicated) CPU core and around 100MB of RAM.
\end{itemize}

Compared to existing techniques~\cite{}, SVMvisor is novel because... TODO.
%Except during VM migration, this hypervisor is not involved in the data-plane processing of any I/O operations, 
%and thus imposes a negligible performance overhead at run time.  
%With such a hypervisor installed on each physical server, \na can monitor each 
%server's internal user activities using VM introspection, and seamlessly migrate the user state 
%on the server from one physical machine to another. 

The rest of this paper is organized as follows.
We first describe the design of SVMvisor,
specifically its support for direct hardware access by a HaaS VM and live migration.
Next, we present the implementation evaluation of our SVMvisor prototype
on the Linux and KVM/QEMU platform.
Finally, we discuss related work followed by conclusions.



\section{Bare-metal Performance}
\vspace{-0.05in}
While using the VT-d and posted-interrupt mechanisms removes
the hypervisor of from I/O data plane path of the VM, these alone are
not sufficient to reduce hypervisor overheads.
To ensure that a VM using VT-d and VFIO can achieve bare-metal
I/O performance and with least overheads, \na implements
the following overhead reduction mechanisms.

%\figw{architecture}{8.75}{\na leverages VT-d passthrough I/O and posted interrupts to provide HaaS VM with direct access to PCIe I/O devices and direct delivery of I/O, timer, and inter-processor interrupts.}
{\bf Dedicated Physical CPUs and Memory:}
First, the \na is assigned one physical CPU core
(CPU 0 in our implementation)
where it executes all of VM management operations.
The HaaS VM is assigned all the remaining physical
CPU cores by pinning its VCPUs to
dedicated physical cores one-to-one.
This prevents the hypervisor's threads
from competing with the VM's VCPUs and
also removes the need to re-route interrupts
from the VM's assigned devices.
To avoid unnecessary VM exits on the HaaS VM's CPUs,
all external interrupts which are not directly handled
by the HaaS VM are delivered to the \sna's physical
CPU 0 by the IOAPIC.
\na also pre-allocates physical memory to the HaaS VM
to avoid the overhead of handling EPT faults during 
application's execution.
To eliminate VM exits triggered by EPT faults, a HaaS agent in 
the guest kernel allocates and writes to all guest-physical pages 
at initialization time. This triggers \na to populate the 
corresponding EPT entries during initialization.

{\bf Disabling  HLT-triggered VM exits:}
The second optimization relates to reducing hypervisor's CPU utilization
under the following scenario.
When handling frequent I/O operations,
a guest VCPU  may become idle for brief intervals, and
run a special idle thread.
On X86, this idle thread consists of a loop of {\tt HLT} instructions,
each of which is {\em intended} to place the CPU in the C1 energy-saving mode
until an external interrupt occurs.
However, since {\tt HLT} is a privileged instruction, it triggers a
VM exit when executed by the guest.
To emulate the {\tt HLT} instruction, currently, the KVM hypervisor
polls for external events for a short time
(adaptively between 0-200ns). If no events occur during this
polling period, the guest VCPU is blocked and the physical CPU
is placed in the low-power state.

The problem we observe is that, when the VM sends or receives network packets
at a high rate, the hypervisor-level CPU utilization increases
dramatically. This is because frequent {\tt HLT}-triggered VM exits
result in frequent invocation of the event polling loop in the hypervisor.
Often, the physical CPU re-enters the guest mode directly
from the polling loop as soon as the next network packet
or interrupt arrives for the VM, without ever entering the low-power mode.

Although peak network throughput is not reduced when
using VFIO (see Table~\ref{tab:cpu_network_io}),
both VM exits and polling loop cause significant
CPU utilization by the hypervisor
(see Table~\ref{tab:cpu_network_io}).
Ironically, this overhead occurs even when using pass-through I/O and
posted interrupts which were meant to reduce hypervisor overheads.
Eliminating this event polling alone is insufficient because {\tt HLT}-triggered VM exits
are also expensive by themselves; the guest VCPU thread is blocked and
must make a trip through the hypervisor's CPU scheduler before
re-entry into guest mode.

Since \na is designed for a single VM setting, {\em we choose to
disable {\tt HLT}-triggered exits altogether}
by modifying the corresponding execution control fields in the VM's VMCS.
As a result, whenever a CPU becomes idle, no {\tt HLT}-triggered
VM exits occur, the CPU lowers its clock frequency while in guest mode,
the total energy consumption is reduced, and importantly,
hypervisor code is not invoked in I/O data path.

Another interesting side effect of disabling {\tt HLT}-triggered VM exits
is that the HaaS VM is observed to receive more interrupts
from its direct-assigned device, possibly due to
faster EOI response and hence less interrupt coalescing by
the hardware, similar to the behavior on a bare-metal OS.

{\bf Eliminating other VM Exits:}
Besides eliminating EPT faults and HLT-triggered VM exits, we 
also eliminate other common sources of VM exits for a HaaS VM.
We remove all unnecessary para-virtual and emulated I/O devices 
(such as emulatedfloppy disks and CD-ROMs); these
tend to communicate with QEMU by trapping on programmed I/O instructions
that result in VM exits.
For accessing local APIC timer (as detailed later), 
we also disable the VM exits due to MSR
writes to local APIC's initial count and divide configuration
registers. For IPIs, we disable VM exits due to MSR writes to
the interrupt command register.  Finally, external device interrupts
from any I/O devices that are not assigned to the HaaS VM
are delivered to CPU0 where the \na handles them without
interrupting the HaaS VM.


\vspace{-0.1in}
\section{Direct Local Interrupt Delivery}
\vspace{-0.05in}
\label{sec:shared_pid_dtid}
\figw{dtid}{8.5}{Direct delivery of timer interrupts in \sna.
%TODO: mention disabling WRMSR VM Exits.
The hypervisor configures each guest LAPIC to transform the local timer
interrupt into a PIN interrupt and allows guest OS to directly update the PID page.}

Unlike an external interrupt, a local interrupt is delivered to a CPU core's local APIC without going through the IOMMU's Interrupt Remapping Table.
Therefore, such local interrupts cannot be delivered to a VM via the posted interrupt mechanism.
In this section, we describe how to achieve direct delivery of two types of local interrupts, {\em timer interrupt} and {\em inter-processor interrupt} (IPI).

{\bf Direct Timer Interrupt Delivery:} To enable a timer interrupt to be delivered directly to a HaaS VM running on a CPU core,
\na first leverages the {\em local vector table} in the local APIC to turn each timer interrupt into a PIN interrupt, and
then programmatically sets a particular bit in the CPU core's PIR bitmap to indicate to the target VM that underlying the PIN interrupt is a timer interrupt.

If the PIR bit associated with a timer interrupt is set way ahead of the interrupt's next expiration time,
the interrupt's target VM may receive spurious timer interrupts.
For example, suppose the next expiration time of a timer interrupt is $T_1$, and a NIC interrupt is delivered via the posted interrupt mechanism to the target VM at $T_2$, where ${T_2} < {T_1}$.
When the target  VM processes the PIN interrupt triggered by the NIC interrupt, it finds in the vIRR bitmap that two bits are turned on, one for the timer interrupt and the other for the NIC
interrupt, {\em even though the timer interrupt is spurious} because the real local timer has not yet expired.

To solve this spurious timer interrupt problem, a HaaS VM's guest OS is modified to keep track of the next expiration time of every timer interrupt, ignore an ostensible timer interrupt when
the current time is substantially smaller than the next expiration time,
and set the timer interrupt bit in the PIR because the PIR bitmap is cleared
after a PIN interrupt is delivered.
The last step is required to ensure that a timer interrupt that is considered spurious and thus ignored because of an external interrupt,  still has a chance to be delivered directly when the timer truly expires.

To avoid VM exit when a HaaS VM modifies the PIR of the CPU core it runs on,
\na allocates a separate page to house the PIR and makes the page accessible to the VM.
With this set-up, a HaaS VM can write to the PIR of the CPU core it runs on without triggering any VM exit.

After a timer interrupt handler services a timer interrupt, it may need to configure the next timer expiry by updating the initial counter (TMICT) register of the local APIC timer,
which would normally cause a VM exit. To avoid this VM exit, \na leverages VT-d's hardware-assisted APIC virtualization by properly configuring the MSR bitmap in the associated VMCS.
As a result, the KVM's  intercept of any TMICT MSR update is disabled. When a VM writes to the TMICT, the change is written to the associated register directly without triggering any VM exit.

{\bf Direct Inter-processor Interrupts (IPI) Delivery:}
Just as with direct delivery of timer interrupts, the \na uses the posted interrupt mechanism 
for direct delivery of IPIs among VCPUs of the HaaS VMs, i.e. without trapping to \na 
for emulating IPI delivery.
When a source CPU sends an IPI to a destination CPU, 
the source configures the Interrupt Command Register (ICR) in its local APIC
and, after the low double-word of the ICR is written to, triggers an IPI message 
to be sent to the destination CPU's local APIC through an inter-APIC system bus.
To enable an IPI to be delivered directly to the HaaS VM, \na 
configures the source CPU core's ICR so that the resulting 
interrupt message carries a PIN vector,
and then sets a certain bit in the PIR of the destination CPU core 
to indicate that the PIN interrupt corresponds to an IPI.
The HaaS agent in the guest assists by retrieving the APIC ID of 
the physical CPUs assigned to the HaaS VM; these retrieved APIC IDs are 
to identify the target CPU core during direct IPI delivery.

{\bf Security considerations:}
Finally, from a security viewpoint, control over the timers and IPIs 
is considered important for a hypervisor or OS to maintain control 
over all physical CPUs.
In \na, although the HaaS VM can directly control the local timer hardware on 
its assigned CPUs, the hypervisor can still regain
control over the CPUs when needed, such as before live migration.
The hypervisor always controls the CPU0 and its local timer.
To regain control over other physical CPUs assigned to the VM, the hypervisor
simply disables direct timer access for the VM by reconfiguring the IOMMU and VMCS to
disable PIN and trigger VM exits for timer interrupts; the hypervisor then delivers
emulated virtual interrupts to the guest.
To prevent the HaaS VM from sending unwanted IPIs to the \sna-controlled physical CPU 0,
the \na does not expose the APIC ID of CPU 0 to the HaaS agent in the guest.

Combining all the above techniques, a HaaS VM can directly receive  and service
local timer interrupts and IPIs from processor hardware,
all without causing any VM exits and while maintaining control over all physical CPUs.





\vspace{-0.1in}
\section{Seamless Device State Migration}
\vspace{-0.05in}

Because the states associated with directly accessible devices complicate the migration process~\cite{zhai:2008},
\na augments KVM's VM migration capability with additional mechanisms to support seamless migration of HaaS VMs.
More specifically, \na adopts the following unified strategy to hide the states of directly accessed devices from KVM's VM migration logic:
In the normal mode, a HaaS VM directly accesses PCIe devices and timers; immediately before and during when the HaaS VM is migrated,  the VM
accesses PCIe devices and timers indirectly; after the migration, the migrated HaaS VM accesses PCIe devices and timers directly again.

Moreover, \na assumes that the source and destination physical servers involved in a migration have an identical
set of hardware devices with which a HaaS VM interacts directly.
This way, the driver code directly interacting with these devices has a chance to properly work on
both the source and destination server.
In this section, we focus on two types of directly accessed devices: NIC and timer.



\vspace{-0.1in}
\subsection{NIC State Migration}
\label{sec:nic_migration}
\vspace{-0.05in}
\figw{nic_bonding}{8}{Bonding of pass-through and para-virtual NICs in a HaaS VM to support live migration in \na.}

We assume each HaaS physical server is equipped with an SR-IOV Ethernet NIC~\cite{dong:2008},
which provides one physical function and multiple virtual functions, each with its own MAC address.
As shown in Figure~\ref{fig:nic_bonding},
\na sets up a directly accessed pass-through NIC using one of the virtual functions
and an indirectly accessed para-virtual NIC using another virtual function.
A NIC bonding driver~\cite{bond-dri} in the guest OS teams up the pass-through and para-virtual NICs in an active-backup mode.
The HaaS VM running on every \na physical server sends and receives network packets through such a bonded interface.

During normal execution, the pass-through NIC is the Active slave and the para-virtual NIC is the Backup slave,
so that a HaaS VM can make full use of the underlying physical NIC's capability.
Before a HaaS VM is to be migrated, \na hot-unplugs the pass-through NIC so as to fail the current Active slave.
Upon detecting the Active slave's failure, the bonding driver immediately switches network traffic to the Backup slave.
From this point on, all network traffic goes through the para-virtual NIC.
KVM captures the state of the para-virtual NIC, which is easier to migrate than the state of a pass-through NIC.
Then \na kicks off a VM migration transaction for the HaaS VM, which includes the transfer its network state.

After the migration transaction is completed, the HaaS VM is resumed on the destination server and still continues to use the para-virtual NIC.
In the mean time, \na hot-plugs the pass-through NIC on the destination  server
so that the pass-through NIC becomes the new Active slave.



The most important performance metric for VM migration is the service disruption time.
The additional service disruption time that \na introduces is attributed to the transition from
the pass-through NIC to the para-virtual NIC on the source server, and the transition from
the para-virtual NIC to the pass-through NIC on the destination server.
Measurements on a earlier \na prototype suggested that the transition from the
para-virtual to the pass-through NIC introduces non-trivial service disruption time (about 300ms),
when hot-plugging the pass-through NIC on the destination server.

A deeper analysis shows that the hot-plug operation consists of three steps:
(1) QEMU prepares a software object to represent the pass-through
NIC, (2) then It populates this software object with parameter values extracted
from the PCIe configuration space of the NIC, (3) and finally it
resets the software NIC object to set up the BAR and interrupt forwarding
information. The first and third steps must take place in QEMU's main event
loop when the entire HaaS VM requesting the hot-plug operation must be paused.

To minimize this service disruption time due to hot-plug,
\na performs the first and second steps of the hot-plugging the pass-through NIC
on the destination server while the HaaS VM is being migrated and before it is resumed,
and performs the third step after the HaaS VM is migrated and resumed.


\vspace{-0.1in}
\subsection{Timer State Migration}
\vspace{-0.05in}

To leverage the posted interrupt mechanism to directly deliver local APIC timer interrupts,
\na allows a HaaS VM to directly access the following device state:
(1) setting the timer interrupt bit in the PIR associated with the CPU core's local APIC ,  (2)
using a WRMSR to modify the initial counter register of the local APIC timer, and (3)
computing the next timer expiration time from conversions between clock cycles and nano-seconds, the multiplication
and shift factor of the calibrated timer, etc., which are provided by the hypervisor.

During the normal run time, timer interrupts are delivered to a HaaS VM directly.
Before a HaaS VM is to be migrated, \na notifies the HaaS VM to stop the
direct timer interrupt delivery (DTID) mechanism, unmaps the PIR page,
enables the TMICT WRMSR VM exit, configures the local vector table in the local APIC to
fire timer interrupts as they are rather than as posted-interrupt notification interrupts.
Upon receiving this notification, the HaaS VM uses a different set of
multiplication and shift parameters to compute the next timer value,
and convey the resulting value to the hypervisor via the \texttt{hrtimer} object
when control is transferred to the hypervisor upon a TMICT WRMSR VM exit.
After a HaaS VM is successfully migrated and resumed, both the hypervisor and the VM restart
the DTID mechanism. 
%by running the aforementioned steps in reverse order.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mycomment{
The direct timer interrupt delivery depends the following
factors. First, the host shares the posted-interrupt
descriptor with the guest. When the guest updates the
timer-interrupt bit of posted-interrupt request through EPT,
it does not trigger the EPT violations and the local timer
interrupt is delivered as the posted interrupt. Second, the
guest directly configures the timer initial count register
without a WRMSR VM exit. This is achieved by updating the MSR
bitmap of VM control structure. Third, the guest needs to
correctly compute its next timer events with the
host-calibrated LAPIC timer. To compute the next timer event
from the nano-seconds to the clock cycles, the multiplication
and shift factor of the calibrated timer are required. The
host needs to convey such information to the guest. The guest
configures the TMICT with the correct timer event in clock
cycles.

Before the migration starts, both host and guest need to tear
down the DTID. In the host, it
}

\mycomment{
It does not only support the direct PCI device
assignment to the userspace processes of VM, but also the
platform devices. Why do we need to allow the userspace
programs to gain the control of physical devices? For the
field of high performance computing, the I/O performances has
a great impact on the overall system performance. The
performance congestion comes, When the rate of data being read
is slower than the rate of data being consumed. Or it happens,
when the rate of data being written is slower than the rate of
data being computed and produced.

The VFIO needs to fulfill the three requirements of device
assignment to a userspace process. First, the userspace driver
can access to the device resources such as I/O ports. Second,
the userspace driver can perform the DMA securely. This is
provided by the IOMMU protection mechanism. Third, the device
interrupts is delivered to the device owner in the userspace.
The way how the VFIO fulfills the three requirements and
applies them with QEMU is briefly described below.
}

\mycomment{
The hardware-assisted direct device assignment helps the guest
achieve the baremetal I/O performance without the additional
virtualization overhead using the VT-x and VT-d support.
After proper VMCS and EPT configuration, the guest gains
the control of assigned device
by MMIO/PIO without the help of hypervisor. The VT-x APIC
virtualization permits the guest to write to the
end-of-interrupt register without a VM exit. With the VT-d
support, the guest performs DMA with the enhanced security and
eliminates the VM exit overhead due to the device interrupts by using
the posted-interrupt mechanism. In addition to the hardware
support, VFIO provides the software framework for the
userspace device drivers. It works with VT-d and QEMU and sets
up the direct PCI device assignment. Although it is expected
to move the hypervisor out of the guest I/O path, the
hypervisor still induces high CPU utilization due to the
HLT emulation. This greatly deviates our goal of guest having
its own dedicated cores. Nonetheless, our CPU optimization
strategies remedy such a problem.
}

%The design goal of \na is to eliminate the hypervisor from
%the guest I/O path while achieving bare-metal performance in
%the guest. To achieve this \na considers direct device
%assignment to the guest using Intel's VT-d support. First, the
%guest meets the bare-metal network and disk I/O performance
%with direct device assignment. We further apply optimizations
%to reduce the CPU utilization by hypervisor. Second, the timer
%interrupts are transformed into the posted interrupts, which
%are directly delivered to the guest by the logical processor
%without causing any VM exits. Finally, we present the
%migration of virtual machine with directly assigned devices.

%To enhance the manageability of physical servers offered in a bare-metal cloud service, \na installs
%VFIO retrieves a PCIe device's device-specific information such as BARs from its
%configuration register space, and maps them into distinct regions in a special file.
%By reading and writing specific regions of this special file, a user-space program
%is able to directly interact a particular PCIe device.
%VFIO provides user-space programs a PCIe device read/write API, and converts these API calls
%into file read/write operations against the special PCIe device file.
%
%The userspace driver uses
%the device file descriptor and offset to access each region
%and retrieve the device information. The VFIO decomposes the
%physical device to a software interface. Such software
%interface is turned into the assigned device by QEMU.
%Essentially, the device read and write handler in the QEMU
%memory API is forwarded to the VFIO read and write handler.
%Not all accesses to a PCIe device's associated memory regions by user-space processes are direct,
%because some parts of a PCIe configuration register space are privileged, such as
%message signalled interrupts (MSI), BARs, and ROM,  and accessing them
%requires KVM's or QEMU's emulation.
%
%Nonetheless, accessing some parts of PCI configuration
%PCI configuration space is
%not handled as memory regions in QEMU. Some of the accesses to
%the PCI configuration space is passthroughed directly, while
%others, such as MSI, BARs, and ROM, need to be emulated.
%
\mycomment{
When a user-space program sets up a DMA operation, it only knows the virtual address of the associated buffer and thus places
this address in the DMA command, essentially treating this virtual address as a device address.
Given such a DMA command, VFIO consults with the page table associated with this user-space program to extract the physical address corresponding
to the DMA buffer's virtual address, and creates an IOMMU entry that links the buffer's virtual (device) address with its associated physical address.
When this DMA command is executed at run time, its buffer's device address is correctly translated to its corresponding physical address.
}

\mycomment{
Second, the VFIO programs the IOMMU to transfer the data
between the userspace driver and device in a hardware
protected manner. The VT-d IOMMU provides the device isolation
using the per-device IOVA and paging structure. The virtual
virtual address requested by the device is translated to the
physical address through the set of paging structures by
IOMMU. In the case of VM, the address space of assigned device
is embedded within the guest address space. The IOMMU is
programmed to translate such IOVA to the host physical address
which is mapped to the guest address space. Such translation
is both realized and protected by the IOMMU.

Third, the VFIO has a mechanism to describe and register the
device interrupt to signal its userspace driver. When the VM
accesses the device configuration space, it is trapped through
QEMU. QEMU configures the IRQs by the VFIO interrupt ioctls
and sets up the event notifiers between the kernel, QEMU and
guest. When the kernel signals the IRQ to QEMU, QEMU injects
it into the VM. The interrupt signaling is further speeded up
by moving QEMU out of the way. KVM supports both ioeventfd and
irqfd. ioeventfd registers PIO and MMIO regions to trigger an
event notification, when written by the VM. irqfd allows to
inject a specific interrupt to the VM by KVM. Once ioeventfd
and irqfd are coupled together, the interrupt pathway remains
in the host kernel without exiting to the userspace QEMU.
Using the VT-d, the KVM and QEMU is completed removed from the
signaling path way. It enables the direct interrupt delivery
from the assigned device to its VM without a VM exit.
}

\mycomment{
Our design uses the hardware-assisted posted interrupt
mechanism and achieves the direct interrupt delivery of
assigned devices and local timers without the intervention of
hypervisor. It reduces the hypervisor CPU utilization and
dedicates the CPU time to the guest. Under the normal
circumstances, the guest can not handle physical interrupts
without the hypervisor.
}
Traditional hypervisors intercept most hardware interrupts and
emulate the delivery of resultant events to VMs, so as to
restrict VMs' access to physical hardware in multi-tenant settings.
A hardware interrupt delivered to a CPU core
triggers a VM exit if the CPU core is not in the
root mode and control is transferred to the hypervisor.
%Saving and loading the execution context between the root and
%non-root mode waste the CPU cycles. Second, the host needs to
Then the hypervisor examines the cause of the interrupt.
If the interrupt is meant for a VM scheduled on the CPU core,
the hypervisor delivers this interrupt as a virtual interrupt to the target VM
next time when it is scheduled on the CPU core; otherwise the hypervisor handles
the interrupt on its own.
After a VM completes servicing an interrupt, it writes to the EOI (End of Interrupt)  register
to signal to the hardware that the interrupt in question has been handled. Because the EOI register is
a privileged resource, every EOI register write causes a VM exit.
Therefore, processing of each physical interrupt costs at least two VM exits.
This per-interrupt overhead is too expensive for network-intensive applications that
are designed to process multi-million packets per second.

\mycomment{
and handle the physical interrupt. If the physical
interrupt is meant for the guest, the host needs to deliver it
as the virtual interrupt upon the next VM entry. Otherwise,
the host handles it and schedules the next VM entry. Third,
when the guest's interrupt handler finishes, it writes to the
EOI register. Such write operation may induce the VM exit.
Since the guest is not aware of the distinction between the
physical and virtual interrupt, it signals the completion of
interrupt in the same way. After the guest handles the virtual
interrupt, its EOI update is normally emulated by the host.
Fourth, the host may need to use CPU cache and reduce the time
to handle the physical interrupt. This introduces the CPU
cache pollution.
}

%To allow a VM to avoid a VM exit due to clearing the EOI register after handling an interrupt of vector X,  KVM clears X's corresponding bit in
%the {\em EOI exit bitmap} in the VM's VMCS. As a result, at run time after this VM completes servicing an interrupt of vector X and clears the EOI register via a memory mapped
%interface (called APIC-access page), no VM exit occurs.
%Helped with the posted interrupt and EOI virtualization mechanism, a HaaS VM is now able to process any external interrupt directly without triggering any VM exit.


\mycomment{
VT-d supports the posted-interrupt capability and deliver the
external interrupts directly from the I/O devices and external
controllers without the cost of VM exits and the hypervisor
intervention. Before utilizing such feature, the system
software needs to define the posted interrupt notification
vector. The PIN signifies the incoming external interrupt from
the assigned device is subjected to the posted-interrupt
processing. The processing is achieved by updating the
posted-interrupt descriptor dynamically. When the VMCS is
actively used by the logical processor in the non-root mode,
it is prohibited to update its data structures. The PID is the
exception. Nonetheless, there is one requirement that the PID
modifications must be done using locked read-modify-write
instructions. Here is another benefit of posted-interrupt
support. When the virtual processor is scheduled on another
VCPU, the VMM can co-migrate its interrupts from the assigned
devices by setting the corresponding bits in posted-interrupt
register of PID.

The posted-interrupt support is accomplished in three general
steps. First, the VMM programs the interrupt-remapping
hardware with the mapping between the external interrupt and
virtual interrupt. Second, when the external interrupt is
delivered to the interrupt-remapping hardware, it sets the
outstanding bit and corresponding bit of virtual interrupt in
the posted-interrupt register of PID. It generates the PIN.
The IOAPIC delivers the PIN to the appropriate LAPIC. Third,
PIN notifies the logical processor that it is the
posted-interrupt event. The logical processor starts the
posted interrupt processing and delivers the virtual interrupt
without any VM exit.

The posted-interrupt processing is described in the following
steps. First, when the external interrupt is delivered to the
guest's processor, it is acknowledged by the LAPIC. LAPIC
provides the processor core the interrupt number. Second, if
the physical interrupt is equal to the PIN, the logical
processor starts the posted interrupt processing. Third, the
processor clears the outstanding notification bit from the
posted-interrupt descriptor. Fourth, the processor
acknowledges the EOI. Fifth, the processor updates the vIRR by
synchronizing it with the PIR. Sixth, the processor acquires
the next request virtual interrupt. It updates RVI by the
maximum of previous RVI and highest index of bits set in PIR,
before it clears PIR. Seventh, the processor evaluates the
pending virtual interrupt. Eighth, the processor delivers the
virtual interrupt.
}

%\figw{virtualization_overhead}{9}{Virtualization overheads in I/O operations due to VM exits in (a) para-virtualized I/O, (b) pass-through I/O without posted interrupt, and (c) optimized pass-through I/O in \sna.}
%TODO: Describe I/O challenges here
%	Why CPU utilization increases
% 	VM Exit overhead


%
%First, the direct assigned network card and disk drive under
%the VFIO framework removes the host from the forward I/O path.
%%The guest has the control over the assigned devices and avoids
%the virtualization overhead, when accessing the device control
%registers and performing the
%DMA~\cite{sdm:2018,intelvtd-manual,williamson:2016}. After the
%assigned device services the request, it delivers the
%interrupt to the guest and triggers the VM exits due to the
%external interrupts and EOI respectively. Using the VT-d
%posted-interrupt and VT-x APIC virtualization~\cite{postedinterrupt},
%the guest handles the device interrupt and updates the EOI
%without any VM exit. Thus, the host is completed removed from
%the guest I/O path for the pass-through devices. Nonetheless,
%


\mycomment{

The longer the guest stays on its CPU, the more local timer
interrupts it receives. The goal is to let the guest have its
dedicated CPUs. Our design does not only directly deliver the
timer interrupts to the guest, but also take one step further
by allowing the guest update its next timer event directly.

The local timer interrupt is delivered to the guest and
results in two scenarios. First, the timer interrupt is meant
for the guest. It induces the VM exit and the control is
transferred back to the host. The host handles the timer
interrupt and injects the virtual timer interrupt to the
guest. When the guest receives the virtual timer interrupt, it
services the timer interrupt and set up the next timer event
by updating the LAPIC timer initial count register through the
x2APIC interface. This triggers the MSR-write VM exit and the
control is transfer to the host. The host helps the guest to
set up its next timer by registering the \texttt{hrtimer}
object of guest next timer event. Second, the timer interrupt
is not meant for the guest. It induces the VM exit and
transfer the control back to the host. The host processes the
timer interrupt but does not inject the virtual timer
interrupt. Nonetheless, if the timer interrupt is not meant
for the guest, the guest should not pay the price.

The first task is to transform the local timer interrupt into
the posted interrupt, which is directly delivered to the guest
by the VT-d hardware. It requires two actions. The
timer-interrupt bit of posted-interrupt request needs to be
set, before the posted-interrupt notification is delivered to
the guest core. Since the guest is responsible for its own
timer interrupt, the guest should set the bit in the PIR.
However, such a PIR structure is embedded in the
posted-interrupt descriptor and protected by the host. The
host needs to share the PIR with the guest by isolating the
entire PID to a shared page. If the guest messes up setting
the proper bits in the PID through the EPT, it does not affect
the host normal operations. In our design, the shared PID page
is accessible by three entities: host, guest and
virtualization hardware. The second task is to allow the guest
control the timer initial count register of LAPIC timer. With
the hardware-assisted APIC virtualization, this is achieved by
updating the MSR bitmap of VM control structure. The KVM
intercept of TMICT MSR update is disabled. When the guest
configures the TMICT, the change is written to the register
directly without triggering the VM exit. The third task is to
configure the LAPIC timer chip to deliver the posted-interrupt
notification instead of the actual timer interrupt. In
summary, we reach our goal of guest having dedicated CPUs by
disabling the HLT- and timer-related VM exits.

Using the shared PID has the draw back. It induces the
spurious timer interrupts causing additional interrupt
processing in the guest. Since the guest sets the PIR
timer-interrupt bit before its next timer event, the it
induces the spurious timer interrupts. Such a fake timer
interrupt is induced in two cases. First, the guest
experiences the spurious timer interrupt when performing
I/O-bound activities with the assigned device. Let's take the
assigned network card for an example. Both the bits of timer
interrupt and network-device interrupt are set in the PIR.
Based on the Intel architecture, the timer interrupt has a
higher priority than the network interrupt does. The timer
interrupt is delivered before the network interrupt. Although
the guest should have only processed the network interrupt, it
first processes the timer and then network interrupt. Second,
upon the VM entry, the PIR is synchronized to the virtual
interrupt-request register because of the KVM implementation.
One of time points to evaluate the virtual interrupt delivery
is at the VM entry time. If the PIR timer-interrupt bit is
present during the copy, the fake virtual timer interrupt is
delivered into the guest after the VM entry. If the arrival of
virtual timer interrupt is earlier than the expected
expiration, the guest ignores it and processes the next
interrupt. Thus, the CPU overhead is reduced in comparison
with the full timer interrupt processing.

}


%The VM on an \na server is special because it is the only VM on the server and it
%interacts directly with PCIe devices and local timers.
%We call such a VM a HaaS VM.

\mycomment{
The direct device assignment makes it difficult to migrate the
guest to its destination~\cite{zhai:2008}. After the VM
migration, it is possible that the previously-assigned devices
may not be available at the destination. Even if the assigned
device is available, the internal state of device may not be
readable or still on its way to the destination. The host at
the destination has a hard time to pass-through the device
without the device-specific knowledge. Moreover, some devices
have the unique hardware information that cannot be
transferred, such as the MAC address of network interface
card. In the case of guest-controlled timer, it depends on the
VT-x availability at the destination. Our design takes the
approach of alternating the usage of pass-through and
respective virtual device with the acceptable service downtime
or number of missed time interrupts. We assume that the
devices and hardware supports are available. For the network
activity, the network traffic is switch from the assigned to
virtual network device, before the migration starts. The
network traffic is switched back after the guest starts up at
the destination. For the local timer interrupt, the direct
timer interrupt delivery is switched back to the indirect
delivery with the help of \texttt{hrtimer} object and TMICT
WRMSR VM exit is enabled, before the migration. The changes
are reverted after the guest starts up at the destination.
}

\mycomment{
In this section we describe the mechanism of migrating a guest
with direct NIC assignment. As shown in Figure
\ref{fig:nic_bonding} the guest is configured with virtio
network device backed by the Vhost driver and a pass-through
network interface card~\cite{zhai:2008}. Using SR-IOV
~\cite{dong:2008} the physical NIC is presented as virtual NIC
through virtual functions. For the purpose of simplicity, we
assume that the guest has one assigned network device. The
prototype overcomes the challenge of migrating NIC assigned
guest by the following strategy. It uses the Ethernet bonding
driver to direct the network traffic between the assigned and
virtual NIC. The migration procedure is divided into two
parts. During regular operation of guest, the assigned NIC is
used for higher network bandwidth. Before the migration, the
host uses the bonding driver and shifts the network traffic
from the assigned NIC to the virtual NIC. The source host
takes the control of the assigned NIC through hot unplug event
of the assigned NIC and starts the migration. After the guest
resumes at the destination, the destination host transfers the
control back to the guest using hot plug event of the assigned
NIC and switches the network traffic from virtual NIC to
assigned NIC.

Linux provides bonding driver to present multiple network
interfaces into a single logical interface. The modes of
bonding driver define the behavior of the bonded interfaces.
To maintain higher network performance during regular
operation of guest, we configure the bonding driver in
active-backup mode where the assigned NIC is chosen as the
active interface and the virtual NIC as a backup-slave
interface. In active-backup mode only one of the interfaces is
active at any time. When the active interface fails, one of
the slave interfaces becomes active. The bonding driver always
takes the MAC of the active interface. On failure of the
active interface, the bonding driver takes the MAC address of
the next to be slave interface. The change in MAC address is
notified by broadcasting ARP packets to avoid the network
packets loss in guest. Before the migration is initiated, on
hot unplug command, the outgoing network traffic is redirected
to the virtual NIC interface by the bonding driver and the
incoming traffic is shifted to the virtual NIC by broadcasting
ARP packets. Once the assigned NIC is hot unplugged, QEMU
issues migrate command. After the migration is completed, once
the guest resumes on the destination, the NIC device is hot
plugged.
}

\mycomment{
However, by forcing the migrated HaaS VM resumed on the destination server to continue using the virtual NIC,
the time required to hot-plug the directly accessed NIC on the destination server could be fully masked.
After \na successfully brings up the directly accessed NIC and makes it the Backup slave,  it fails the virtual NIC so as
to direct all network traffic to the directly accessed NIC.


We observed that on hot plug event of NIC device on the
destination host, the network service in guest drops until for
0.3seconds. Further, we investigate the reason for the network
packet loss in the guest. The hot plug mechanism of assigned
NIC consists of the following three steps. First, QEMU
prepares a software object that represents the pass-through
NIC. It then realizes the QEMU software object by getting a
copy of configuration space from the NIC device. Finally, it
resets the software NIC object and setup the BAR and interrupt
forwarding. The first and last step happen in QEMU main event
loop during which the guest remains paused. As a result, the
guest experiences downtime during hot plug operation. In \na,
to mitigate the downtime due to hot plug operation on the
destination host, the first two steps are executed during
migration. During the first phase of pre-copy live
migration~\cite{clark:2005,postcopy-osr}, all the memory pages are
transferred to the destination over the network. The dirty
memory pages are then transferred in multiple iterative
rounds. The VCPU and I/O state of guest are transferred in the
final phase to resume the guest on destination. Step one and
two are executed on the destination host during migration.
Consequently, the network service in guest does not get
affected as it runs on the source host. QEMU allows to setup
and realize the software object during migration. However, the
BAR and interrupts can be setup only after resuming the guest.
Hence, we eliminate the downtime caused during the setup of
software NIC object phase.
}



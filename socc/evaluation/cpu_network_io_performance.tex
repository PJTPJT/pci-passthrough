% CPU and network performance of assigned NIC

\begin{table}
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|} \hline
{\bf Configuration} & {\bf Outbound} & {\bf Inbound} & {\bf Round-Trip} \\
                    & {\bf (Gbps)}   & {\bf (Gbps)}  & {\bf Delay ($\mu$s)} \\ \hline
{\bf Bare-metal} & 37.39 & 37.52 & 12 \\ \hline
{\bf VHOST}      & 37.39 & 19.02/37.10 & 24/18\\ \hline
{\bf VFIO}       & 37.45 & 37.58 & 13 \\ \hline
{\bf OPTI}       & 37.37 & 37.52 & 13 \\ \hline
{\bf DTID}       & 37.35 & 37.50 & 12 \\ \hline
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Comparison of network throughput and round-trip
packet delay over a 40Gbps Infiniband link among the five
evaluated configurations}
\label{tab:network_performance}
\vspace{-0.1in}
\end{table}

In this experiment, we demonstrate that, with all the proposed
optimizations applied, a HaaS VM performs essentially the same
as a bare-metal server in terms of network throughput, network
packet latency and CPU utilization.

We use iPerf~\cite{iperf} to measure the network throughput of
incoming and outgoing TCP traffic on a 40Gbps Infiniband link.
iPerf~\cite{iperf} runs in uni-directional and uses 2 TCP
streams, which the packet size is 8KB. MTU is set to 1500 with
the jumbo frame enabled. As shown in
Table~\ref{tab:network_performance}, for outgoing TCP traffic,
all five evaluated configurations are able to saturate the
Infiniband link and have almost identical network throughput.
For incoming TCP traffic, all five configurations except the
VHOST configuration have roughly the same network throughput;
however, the VHOST configuration's network throughput is about
52\% of that of the other four configurations when two CPU
cores are used. It is comparable, only when three CPU cores
are used.

We use ping~\cite{ping} to measure the round-trip delays over
a 40Gbps Infiniband link and use them to compare the network
packet latency of the evaluated configuration. As shown in
Table~\ref{tab:network_performance}, when two CPU cores are
used, the round-trip delay in the VHOST configuration is
24$\mu$s, which is about twice as much as that of the other
four configurations. The extra delay arises because, when a
HaaS VM runs in the VHOST configuration, VM exists occur
whenever it access virtual I/O devices and receives interrupts
from these devices.

\begin{table}
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|} \hline
{\bf Exit Cause} & {\bf VHOST } & {\bf VFIO} & {\bf OPTI} & {\bf DTID} \\ \hline
{\bf HLT Inst}   & 4362  & 79765 & 0    & 0    \\ \hline
{\bf EPT Fault}  & 55071 & 0     & 0    & 0    \\ \hline
{\bf Interrupt}  & 15702 & 219   & 498  & 1    \\ \hline
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Comparison of number of VM exits per second when the
evaluated configurations transmit TCP traffic over a 40Gbps
Infiniband link using iPerf~\cite{iperf}.}
\label{tab:vm_exit}
\vspace{-0.1in}
\end{table}

Because the number of VM exits significantly impacts the CPU
utilization during the network I/O activities, let's first
examine the number of VM exits per second for each of the four
virtualization configurations. Table \ref{tab:vm_exit} lists
the number of VM exists per second a HaaS VM experiences the
most, while it sends out TCP traffic over an Infiniband link
in one of the four configurations. The VHOST configuration
incurs a VM exit whenever a HaaS VM executes the HLT
instruction, accesses a NIC and triggers an EPT fault, or is
interrupted.

In contrast, a HaaS VM running in the VFIO configuration does
not trigger any VM exit when accessing a NIC or receiving a
NIC interrupt. The only interrupts that cause a VM exit for
such a HaaS VM are internal interrupts, such as timer and IPI
interrupts. This is why the numbers of VM exits per second due
to EPT faults and interrupts for the VFIO configuration are
significantly lower than those of the VHOST configuration. On
the other hand, the number of VM exits per second due to HLT
instructions for the VFIO configuration is drastically larger
than that of the VHOST instruction. This behavior depends on
the amount of time that a VM idles on its CPUs. The percentage
of idling time in the non-root mode for the VFIO configuration
is higher than that in the VHOST configuration. When a HaaS VM
occupies the CPU longer, it is more likely for the VM to be
idle and issue HLT instructions.

Compared with the VFIO configuration, the OPTI configuration
further eliminates VM exits due to HLT instructions but
experiences 498 VM exits per seconds due to local interrupts.
In our test, both the hypervisor and the HaaS VM set a timer
resolution of 4 msec or 250Hz. Their expiration times are not
synchronized. Together there are about 500 hardware timer
interrupts every second. Therefore, the OPTI configuration
encounters roughly 498 VM exits per second, most of which are
attributed to timer interrupts. In the VFIO configuration, the
rate of VM exit due to the local interrupts is substantially
less than what we have observed in the OPTI configuration.
Because of the large number of VM exits due to HLT
instructions, a timer interrupt could arrive, when the
hypervisor is in control of the interrupted CPU. Such
interrupt triggers no additional VM exit.

In the DTID configuration, the only interrupt that triggers a
VM exit for a HaaS VM is IPI interrupt. because IPIs do not
occur frequently during our test, the resulting interrupt rate
becomes very low, about 1 per second.

\begin{table}[]
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
                    & \multicolumn{2}{c|}{\textbf{Host}}  & \multicolumn{2}{c|}{\textbf{Guest}} & \textbf{Total} \\ \cline{2-5}
                    & \textbf{Usr\%} & \textbf{Sys\%}    & \textbf{Usr\%} & \textbf{Sys\%}    & \textbf{\%}    \\ \hline
\textbf{Bare-metal} & 0.64            & 80.98             & NA              & NA                & 81.62          \\ \hline
\textbf{VHOST}      & 68.32           & 83.85             & 0.23            & 31.20             & 115.28         \\ \hline
\textbf{VFIO}       & 83.35           & 99.25             & 0.26            & 34.92             & 134.43         \\ \hline
\textbf{OPTI}       & 199.76          & 0.24              & 0.65            & 81.62             & 82.51          \\ \hline
\end{tabular}
\end{center}
\caption{Comparison of the total CPU utilization and its
breakdown when the evaluated configurations send out TCP
traffic over a 40Gbps Infiniband link at full speed using
iPerf~\cite{iperf}. The total is 200\% because two isolated
CPU cores are used. \textbf{Usr}: User. \textbf{Sys}: System.}
\label{tab:cpu_utilization_40gbps}
\end{table}

Next we profile the CPU utilization that a HaaS VM needs in
both the host and VM, when the VM saturates the Infiniband
link at full speed. It provides us how much CPU time that VM
consumes from both the host's and guest's point of view. We
use such information to compute and compare the CPU resource
consumption between the host and guest of each configuration.
Previously two isolated CPU cores are used to saturate the
Infiniband link. The CPU utilization is taken for the two
cores as well. The total CPU utilization is 200\%.

As shown in Table~\ref{tab:cpu_utilization_40gbps}, \emph{Host
User} and \emph{Host System} represent the total CPU time
consumed in the user mode and system mode respectively and are
measured from the host. \emph{Guest User} and \emph{Guest
System} are the total CPU time consumed in the user and system
mode respectively by the guest and are part of \emph{Host
User}. For example, the user-mode CPU utlization of guest is
measured as $0.67/200$ whereas the user-mode CPU utilization
of host is $68.32/200$. \emph{Guest User} is $0.23/200$.
Thus, the total CPU utilization due to the work done by the VM
is the sum of \emph{Guest User}, \emph{Guest System} and
\emph{Host System}.

The Bare-metal configuration consumes the least CPU resource.
The OPTI configuration comes very close to the Bare-metal
performance. Both the VHOST and VFIO configuration consume at
least 40\% more CPU resource in comparison.

Although the VFIO configuration enables direct NIC access and
interrupt delivery, surprisingly, the total CPU utilization of
the VFIO configuration is actually higher than that of the
VHOST configuration. The main cause is the large number of VM
exits due to HLT instructions in the VFIO configuration, as
shown in Table~\ref{tab:vm_exit}. When KVM processes emulates
the HLT instruction, it uses a busy waiting loop, and the CPU
cycles burned by these busy waiting loops bump up the Host
System mode's CPU utilization and eventually the total CPU
utilization of the VFIO configuration.

In the OPTI configuration, \emph{Host System} is close to 0\%,
indicating that the hypervisor is almost completely out of the
picture. Although \emph{Host User}is 199.76\%, the sum of
\emph{Guest User} and \emph{Guest System} is only 82.27\%. The
gap between the two is due to HLT instructions. After
disabling the VM exit due to the HLT instructions, it
transforms the privileged HLT instruction to the user-space
HLT instruction.  When a HaaS VM becomes idle and issues HLT
instructions, these instructions put the VM in a lower power
idle mode. From the host's point of view, the HaaS VM remains
active because it occupies the CPU.

Thus, having the dedicated CPU core and passthrough NIC
suggests that \na is successful in enabling a Haas Server to
match the network performance and the CPU utilization of
bare-metal server.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mycomment{
\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|}
\hline
& \begin{tabular}[c]{@{}l@{}}Outbound\\ (Gbps)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inbound\\ (Gbps)\end{tabular} & \begin{tabular}[c]{@{}l@{}}RTT\\ ($\mu$s)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf Bare-metal}\end{tabular}     & 37.39 & 37.52 & 12\\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf VHOST} \end{tabular} & 37.39 & 37.10 & 18\\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf VFIO}\end{tabular}  & 37.45 & 37.58 & 13\\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf OPTI} \end{tabular}    & 37.37 & 37.52 & 13\\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf DTID} \end{tabular}    & 37.35 & 37.50 & 12\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + vHOST\end{tabular}                          & $37.39 \pm 0.04$ & $19.02 \pm 0.50$ & $24 \pm 5$\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\end{tabular}                           & $37.45 \pm 0.08$ & $37.58 \pm 0.15$ & $13 \pm 2$\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\\ + OPTIMIZATION\end{tabular}          & $37.37 \pm 0.11$ & $37.52 \pm 0.15$ & $13 \pm 3$\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\\ + OPTIMIZATION\\ + DTID\end{tabular} & $37.35 \pm 0.08$ & $37.50 \pm 0.23$ & $12 \pm 5$\\ \hline
\end{tabular}
\caption{Comparison of network bandwidth and network packet latency over a
40Gbps Infiniband link for the five configurations evaluated.}
\label{tab:network_performance}
\end{table}
}

\mycomment{
\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|}
\hline
& \%User & \% System & \%Guest \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf Bare-metal}\end{tabular}     & 0.64   & 80.98 & -- \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf vHost} \end{tabular} & 68.6   & 84.36 & 68.6 \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf VFIO} \end{tabular}  & 90.48  & 85.08 & 90.48 \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf OPTI}\end{tabular}    & 199.76 & 0.24  & 199.76 \\ \hline \hline
\begin{tabular}[c]{@{}l@{}}In OPTI Guest\end{tabular} & 0.66   & 81.7  & -- \\ \hline
\begin{tabular}[c]{@{}l@{}}In DTID Guest\end{tabular} & 0.68   & 82.9  & -- \\ \hline
\end{tabular}
\caption{Comparison of CPU Utilization. When the guest
generates the TCP outgoing traffic, we measure the CPU
utilization in the host. We also measure the CPU utilization
in the OPTI and DTID guest. The total CPU utilization is 200\%
for two working cores. \%User is the \%CPU time consumed in
the user mode, \%System is the \%CPU time consumed in the
kernel mode and \%Guest is the \%CPU time consumed by the
guest. \%Idle is not shown.}
\label{tab:cpu_utilization_40gbps}
\end{table}
}

\mycomment{
Although the outgoing TCP throughput is about 37.4Gbps for the
guest using the vHost as the backend driver or assigned NIC,
the guest using the assigned NIC has higher CPU utilization.
The VFIO configuration consumes $175.56/200$ of total CPU
utilization, while the vHost configuration consumes
$152.96/200$ of total CPU utilization. This is due to the
higher number of HLT instruction issued by the guest using the
assigned NIC. The NIC assignment by VT-d allows the guest to
handle the network interrupts directly without a VM exit. At
the same time, it keeps the guest on its CPU for longer time.
The guest issues the HLT instruction more frequently and waits
for its network I/O, when it is idle. The vCPUs burn CPU
cycles when they poll for sometime before executing the HLT
instruction. The vCPU polling mechanism keeps the CPU busy
leading to higher CPU utilization.

To reduce the CPU utilization in host, we disable the HLT
exits by modifying the VMCS structure in KVM. To avoid other
system processes to compete with guest, the vCPUs are pinned
on isolated CPUs. As shown in
\ref{tab:cpu_utilization_40gbps}, we notice that disabling HLT
exits along with dedicating cores to guest, reduces the CPU
utilization in system mode to $0.24/200$ and increases the CPU
Utilization in guest mode to $199.76/200$. It indicates that
the guest occupies the CPUs for most of the time.
}

\mycomment{
\begin{table}[tbp]
\begin{tabular}{lllll}
\hline
& \begin{tabular}[c]{@{}l@{}}Guest\\ + vHost\end{tabular} & \begin{tabular}[c]{@{}l@{}}Guest\\ + VFIO\end{tabular} & \begin{tabular}[c]{@{}l@{}}OPTI\\ Guest\end{tabular} & \begin{tabular}[c]{@{}l@{}}DTID\\ Guest\end{tabular}\\ \hline
HLT                & 4362  & 79765 & 0    & 0    \\ \hline
EPT Misconfig.     & 55071 & 0     & 0    & 0    \\ \hline
External Interrupt & 15702 & 219   & 498  & 1    \\ \hline
%Preemption Timer   & 406   & 271   & 601  & 0    \\ \hline
%IO Instruction     & 18    & 19    & 18   & 19   \\ \hline
%MSR Read           & 2     & 2     & 2    & 2    \\ \hline
%MSR Write          & 2248  & 3919  & 4495 & 2499 \\ \hline
%Pause Instruction  & 1266  & 0     & 0    & 0    \\ \hline
%Pending Interrupt  & 273   & 0     & 0    & 0    \\ \hline
\end{tabular}
\caption{Comparison of VM Exits per Second Among Busy Guests.
The VM exits are recorded when the guest generates the TCP
outgoing traffic.}
\label{tab:vm_exit}
\end{table}
}

\mycomment{
we show the most relevant VM exits
that has the significant impact on the CPU utilization. With
the NIC assignment, the number of HLT exits per second
increases from 4362 to 79765. Both the number of VM exits per
second due to the EPT misconfiguration and external interrupts
goes down. After disabling the HLT exit, we observes the
increases number of VM exits frequency due to the timer
interrupts. This is due to the time-slice expiration from both
the host and guest. In our experiment, the host and guest has
the time-slice of 4ms which triggers 250 timer interrupts per
second. We examine the VM-exit qualification and find 498
numbers of VM exits per second are due to the timer
interrupts. This matches what we have expected with the
sampling error. In contrast, if the HLT exiting is not
disabled, some of the host timer interrupts are hidden by the
time when the host is emulating HLT instruction. Furthermore,
the timer interrupts are directly delivered to the guest
without causing any VM exit.
}

\mycomment {
\begin{table}
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|} \hline
 & Host & Host  & Guest & Guest & Total \\
 & User & System & User & System & \\ \hline
Bare-metal    & 0.64   & 80.98 & NA   & NA    & 81.62  \\ \hline
VHOST         & 68.32  & 83.85 & 0.23 & 31.20 & 115.28 \\ \hline
VFIO          & 83.35  & 99.25 & 0.26 & 34.92 & 134.43 \\ \hline
OPTI          & 199.76 & 0.24  & 0.65 & 81.62 & 82.51  \\ \hline
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Comparison of the total CPU utilization and its
breakdown when the evaluated configurations send out TCP
traffic over a 40Gbps Infiniband link at full speed using
iPerf. The total is 200\% because two CPU cores are used.}
\label{tab:cpu_utilization_40gbps}
\vspace{-0.1in}
\end{table}
}

\mycomment{
\begin{table}[]
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
                    & \multicolumn{2}{c|}{\textbf{Host}}  & \multicolumn{2}{c|}{\textbf{Guest}} & \textbf{Total} \\ \cline{2-5}
                    & \textbf{User\%} & \textbf{System\%} & \textbf{User\%} & \textbf{System\%} & \textbf{\%}    \\ \hline
\textbf{Bare-metal} & 0.64            & 80.98             & NA              & NA                & 81.62          \\ \hline
\textbf{VHOST}      & 68.32           & 83.85             & 0.23            & 31.20             & 115.28         \\ \hline
\textbf{VFIO}       & 83.35           & 99.25             & 0.26            & 34.92             & 134.43         \\ \hline
\textbf{OPTI}       & 199.76          & 0.24              & 0.65            & 81.62             & 82.51          \\ \hline
\end{tabular}
\end{center}
\caption{Comparison of the total CPU utilization and its
breakdown when the evaluated configurations send out TCP
traffic over a 40Gbps Infiniband link at full speed using
iPerf. The total is 200\% because two CPU cores are used.}
\label{tab:cpu_utilization_40gbps}
\end{table}
}

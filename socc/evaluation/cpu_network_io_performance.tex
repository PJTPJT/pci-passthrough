% CPU and network performance of assigned NIC

\begin{table*}[t]
\renewcommand{\arraystretch}{1.2}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
%\multirow{2}{*}{}   & \multicolumn{2}{c|}{\textbf{Host}}  & \multicolumn{2}{c|}{\textbf{Guest}} & \multirow{2}{*}{\textbf{Total CPU\%}} & \multirow{2}{*}{\textbf{No. CPUs}} & \multicolumn{3}{c|}{\textbf{Network}}                                                                                                                                                                    \\ \cline{2-5} \cline{8-10}
                                          & \multicolumn{2}{c|}{\textbf{Host}}  & \multicolumn{2}{c|}{\textbf{Guest}} & \multirow{2}{*}{\textbf{Total CPU\%}} & \multirow{2}{*}{\textbf{Active CPUs}} & \multicolumn{3}{c|}{\textbf{Network}}                                                                                                                                                                    \\ \cline{2-5} \cline{8-10}
\diagbox{\textbf{Configuration}}{\textbf{Performance}} & \textbf{User\%} & \textbf{System\%} & \textbf{User\%} & \textbf{System\%} &                                       &                                    & \textbf{\begin{tabular}[c]{@{}c@{}}Outbound\\ (Gbps)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Inbound\\ (Gbps)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Round-Trip Delay\\ ($\boldsymbol{\mu}$s)\end{tabular}} \\ \hline
\textbf{Bare-metal} & 0.68            & 90.59             & --              & --                & 91.27                                 & 2                                  & 37.39                                                              & 37.60                                                             & 11                                                              \\ \hline
\textbf{VHOST}      & 79.32           & 172.74            & 0.31            & 35.19             & 208.24                                & 3                                  & 37.61                                                              & 34.36                                                             & 24                                                              \\ \hline
\textbf{VFIO}       & 102.45          & 91.90             & 0.36            & 44.66             & 136.92                                & 2                                  & 37.59                                                              & 37.60                                                             & 12                                                              \\ \hline
\textbf{OPTI}       & 199.58          & 0.42              & 0.85            & 99.97             & 101.24                                & 2                                  & 37.55                                                              & 37.58                                                             & 12                                                              \\ \hline
\textbf{DTID}       & 199.58$^*$      & 0.42$^*$          & 0.89            & 90.55             & 91.86                                 & 2                                  & 37.59                                                              & 37.60                                                             & 12                                                              \\ \hline
\textbf{DID}        & 199.58$^*$      & 0.42$^*$          & 0.86            & 90.74             & 92.02                                 & 2                                  & 37.56                                                              & 37.59                                                             & 13                                                              \\ \hline
\end{tabular}%
}
\caption{Comparison of CPU utilization, network throughput and
round-trip packet delay over 40Gbps Infiniband link among the
six evaluated configurations. $^*$: Since the LAPIC timers are
passthroughed in DTID and DID, the host cannot measure the CPU
utilization for the VM. We expect the host user\% and system\%
of DTID and DID is comparable to OPTI and thus, reuse the OPTI
values.}
\label{tab:cpu_network_io}
\end{table*}

\begin{table*}[t]
\renewcommand{\arraystretch}{1.2}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\diagbox{\textbf{Configuration}}{\textbf{Exit Reason}} & \textbf{EPT Fault} & \textbf{Interrupt} & \textbf{HLT Instruction} & \textbf{PAUSE Instruction} & \textbf{Preemption Timer} & \textbf{MSR Write} & \textbf{Total} \\ \hline
\textbf{VHOST} & 13652              & 98                 & 10454                    & 104                        & 145                       & 1065               & 25518          \\ \hline
\textbf{VFIO}  & 0                  & 126                & 36672                    & 19                         & 151                       & 1421               & 38389          \\ \hline
\textbf{OPTI}  & 0                  & 547                & 0                        & 0                          & 0                         & 1446               & 1993           \\ \hline
\textbf{DTID}  & 0                  & 3                  & 0                        & 0                          & 0                         & 1147               & 1150           \\ \hline
\textbf{DID}   & 0                  & 3                  & 0                        & 0                          & 0                         & 0                  & 3              \\ \hline
%\diagbox{\textbf{Configuration}}{\textbf{Exit Reason}} & \textbf{EPT Fault} & \textbf{Interrupt} & \textbf{HLT} & \textbf{MSR Write} & \textbf{PAUSE} & \textbf{Preemption Timer} & \textbf{CPUID} & \textbf{MSR Read} & \textbf{Total} \\ \hline
%\textbf{VHOST} & 1638238            & 11798                       & 1254475                  & 127812             & 12478                      & 17382                     & 0              & 120               & 3062303        \\ \hline
%\textbf{VFIO}  & 5                  & 15083                       & 4400606                  & 170565             & 2338                       & 18108                     & 0              & 120               & 4606825        \\ \hline
%\textbf{OPTI}  & 8                  & 65650                       & 0                        & 173460             & 1                          & 0                         & 18             & 120               & 239257         \\ \hline
%\textbf{DTID}  & 12                 & 343                         & 0                        & 137662             & 1                          & 0                         & 36             & 119               & 138173         \\ \hline
%\textbf{DID}   & 8                  & 331                         & 0                        & 0                  & 0                          & 0                         & 0              & 120               & 459            \\ \hline
\end{tabular}%
}
\caption{Comparison of number of VM exits per second per VCPU
when the evaluated configurations transmit TCP traffic over a
40Gbps Infiniband link for 60 seconds. Physical interrupts
include both the local and external interrupts.}
\label{tab:vm_exit}
\end{table*}

In this experiment, with all the proposed optimizations
applied, a HaaS VM performed essentially the same as a
bare-metal server in terms of network throughput, network
packet latency and CPU utilization.

We used iPerf~\cite{iperf} to measure the network throughput
of incoming and outgoing TCP traffic on a 40Gbps Infiniband
link. iPerf~\cite{iperf} ran in uni-directional and used 2 TCP
streams, which the packet size was 8KB. MTU was set to 1500
with the jumbo frame enabled.

As shown in Table~\ref{tab:cpu_network_io}, for outgoing TCP
traffic, all six evaluated configurations were able to
saturate the Infiniband link and had almost identical network
throughput. For incoming TCP traffic, all six configurations
except the VHOST configuration had roughly the same network
throughput. VHOST needed three CPU cores to reach 34.36 Gbps.
If VHOST only used two CPU cores, its throughput was about
52\% of that of the other five configurations.

We used the Ping~\cite{ping} tool to measure the round-trip
delays over a 40Gbps Infiniband link. The packet size was 56
bytes. The network latency of each configuration was shown in
Table~\ref{tab:cpu_network_io}. The round-trip delay in the
VHOST configuration was 24$\mu$s. It was approximately twice
as much as that of the other five configurations. The extra
delay arised because of VM exits. When a HaaS VM ran in the
VHOST configuration, VM exits occur whenever it accessed or
received interrrupts from the virtual NIC.

% VM exits
% - Access NIC and receive NIC interrupts: EPT and external
%   interrupts.
% - HLT: different idled time for each virtualization
%   configuration.
% - Local interrupt.
% - Masked local interrupt, when the host controlled the
%   interrupted CPU.

Because the number of VM exits significantly impacted the CPU
utilization during the network I/O activities, let's first
examine the number of VM exits per second per VCPU for each
virtualization configurations. We profiled the VM exits for
the 2 active VCPUs by perf~\cite{perf}, when the HaaS VM
transmitted the TCP traffic over an Infiniband link. In
Table~\ref{tab:vm_exit}, the VHOST configuration suffered the
highest VM-exit overhead among all configurations. It incurred
a VM exit whenever a HaaS VM executed the HLT instruction,
accessed a NIC and triggered an EPT fault, was interrupted or
had an expired VMX timer.

In contrast, a HaaS VM running in the VFIO configuration did
not trigger any VM exit when accessing a NIC or receiving a
NIC interrupt. The only interrupts that caused a VM exit were
internal interrupts, such as timer and IPI interrupts. On the
other hand, the rate of VM exits due to HLT instructions for
the VFIO configuration was drastically larger than that of the
VHOST instruction. This behavior depended on the amount of
time that a VM was idled on its CPUs. The idling time in the
non-root mode for the VFIO configuration was longer than that
in the VHOST configuration. When a HaaS VM occupied the CPU
longer and waited for its next network I/O, it was idled often
and issued the HLT instructions.

Compared with the VFIO configuration, the OPTI configuration
further eliminated VM exits due to HLT instructions but
experienced 547 VM exits per second per VCPU due to the local
interrupts. In our test, both the hypervisor and the HaaS VM
set a timer resolution of 4ms or 250Hz. Their expiration time
was not synchronized. Together there were at most 500 hardware
timer interrupts every second. In addition, iPerf~\cite{iperf}
threads set up its down timer and there were also rescheduling
IPIs that woke up iPerf~\cite{iperf} threads. Therefore, the
OPTI configuration encountered roughly 547 VM exits per second
per VCPU due to local interrupts. Most of which were
attributed to timer interrupts.

In the VFIO configuration, the rate of VM exit due to the
local interrupts was substantially less than what we have
observed in the OPTI configuration. Because of the large
number of HLT-related VM exits, a timer interrupt could
arrive, when the hypervisor was in control of the interrupted
CPU. Such interrupt did not contribute an additional VM exit.

In the DTID configuration, the only interrupt that triggered a
VM exit for a HaaS VM was IPI interrupt. The HaaS VM
experienced the VM exits due to IPI, when it recevied the
reschduling or function call IPIs from the \na 
or issued IPIs to another VCPU by writing to the interrupt
command register.

Next we profiled the CPU utilization that a HaaS VM needed in
both the host and VM, when the VM saturated the Infiniband
link at full speed. It provided us how much CPU time that the
VM consumed from both the host's and guest's point of view. We
used such information to compute and compare the CPU resource
consumption between the host and guest of each configuration.
Previously two isolated CPU cores were used to saturate the
Infiniband link. The CPU utilization was taken for the two
cores as well. The total CPU utilization was 200\%.

As shown in Table~\ref{tab:cpu_network_io}, \emph{Host
User} and \emph{Host System} represented the total CPU time
consumed in the user mode and system mode respectively and
were measured from the host. \emph{Guest User} and \emph{Guest
System} were the total CPU time consumed in the user and
system mode respectively by the guest and are part of
\emph{Host User}. For example, the user-mode CPU utlization of
guest was measured as $0.67/200$ whereas the user-mode CPU
utilization of host was $68.32/200$. Thus, \emph{Guest User}
is $0.23/200$. The total CPU utilization due to the work done
by the VM was the sum of \emph{Guest User}, \emph{Guest
System} and \emph{Host System}.

The Bare-metal configuration consumed the least CPU resource.
The OPTI configuration came very close to the Bare-metal
performance. Both the VHOST and VFIO configuration consumed at
least 40\% of additional CPU resource in comparison.

Although the VFIO configuration enabled direct NIC access and
interrupt delivery, surprisingly, the total CPU utilization of
the VFIO configuration was actually higher than that of the
VHOST configuration. The main cause was the large number of VM
exits due to HLT instructions as shown in
Table~\ref{tab:vm_exit}. When KVM emulated the HLT
instruction, it used a busy waiting loop, which
increased the CPU utilization in \emph{Host System}.

In the OPTI configuration, \emph{Host System} was close to
0\%, indicating that the hypervisor was almost completely out
of the picture. Although \emph{Host User}is 199.76\%, the sum
of \emph{Guest User} and \emph{Guest System} was only 82.27\%.
The gap between the two was due to that the VM was waiting for
its next network I/O. After disabling the VM exit due to the
HLT instructions, it transformed the privileged HLT
instruction to the user-space HLT instruction. When a HaaS VM
became idle and issued HLT instruction, it put the guest CPU
in a lower power state but did not preempt the VM. From the
host's point of view, the HaaS VM remains active because it
occupies the CPU.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mycomment{
\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|}
\hline
& \begin{tabular}[c]{@{}l@{}}Outbound\\ (Gbps)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inbound\\ (Gbps)\end{tabular} & \begin{tabular}[c]{@{}l@{}}RTT\\ ($\mu$s)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf Bare-metal}\end{tabular}     & 37.39 & 37.52 & 12\\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf VHOST} \end{tabular} & 37.39 & 37.10 & 18\\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf VFIO}\end{tabular}  & 37.45 & 37.58 & 13\\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf OPTI} \end{tabular}    & 37.37 & 37.52 & 13\\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf DTID} \end{tabular}    & 37.35 & 37.50 & 12\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + vHOST\end{tabular}                          & $37.39 \pm 0.04$ & $19.02 \pm 0.50$ & $24 \pm 5$\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\end{tabular}                           & $37.45 \pm 0.08$ & $37.58 \pm 0.15$ & $13 \pm 2$\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\\ + OPTIMIZATION\end{tabular}          & $37.37 \pm 0.11$ & $37.52 \pm 0.15$ & $13 \pm 3$\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\\ + OPTIMIZATION\\ + DTID\end{tabular} & $37.35 \pm 0.08$ & $37.50 \pm 0.23$ & $12 \pm 5$\\ \hline
\end{tabular}
\caption{Comparison of network bandwidth and network packet latency over a
40Gbps Infiniband link for the five configurations evaluated.}
\label{tab:network_performance}
\end{table}
}

\mycomment{
\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|}
\hline
& \%User & \% System & \%Guest \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf Bare-metal}\end{tabular}     & 0.64   & 80.98 & -- \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf vHost} \end{tabular} & 68.6   & 84.36 & 68.6 \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf VFIO} \end{tabular}  & 90.48  & 85.08 & 90.48 \\ \hline
\begin{tabular}[c]{@{}l@{}} {\bf OPTI}\end{tabular}    & 199.76 & 0.24  & 199.76 \\ \hline \hline
\begin{tabular}[c]{@{}l@{}}In OPTI Guest\end{tabular} & 0.66   & 81.7  & -- \\ \hline
\begin{tabular}[c]{@{}l@{}}In DTID Guest\end{tabular} & 0.68   & 82.9  & -- \\ \hline
\end{tabular}
\caption{Comparison of CPU Utilization. When the guest
generates the TCP outgoing traffic, we measure the CPU
utilization in the host. We also measure the CPU utilization
in the OPTI and DTID guest. The total CPU utilization is 200\%
for two working cores. \%User is the \%CPU time consumed in
the user mode, \%System is the \%CPU time consumed in the
kernel mode and \%Guest is the \%CPU time consumed by the
guest. \%Idle is not shown.}
\label{tab:cpu_utilization_40gbps}
\end{table}
}

\mycomment{
Although the outgoing TCP throughput is about 37.4Gbps for the
guest using the vHost as the backend driver or assigned NIC,
the guest using the assigned NIC has higher CPU utilization.
The VFIO configuration consumes $175.56/200$ of total CPU
utilization, while the vHost configuration consumes
$152.96/200$ of total CPU utilization. This is due to the
higher number of HLT instruction issued by the guest using the
assigned NIC. The NIC assignment by VT-d allows the guest to
handle the network interrupts directly without a VM exit. At
the same time, it keeps the guest on its CPU for longer time.
The guest issues the HLT instruction more frequently and waits
for its network I/O, when it is idle. The vCPUs burn CPU
cycles when they poll for sometime before executing the HLT
instruction. The vCPU polling mechanism keeps the CPU busy
leading to higher CPU utilization.

To reduce the CPU utilization in host, we disable the HLT
exits by modifying the VMCS structure in KVM. To avoid other
system processes to compete with guest, the vCPUs are pinned
on isolated CPUs. As shown in
\ref{tab:cpu_utilization_40gbps}, we notice that disabling HLT
exits along with dedicating cores to guest, reduces the CPU
utilization in system mode to $0.24/200$ and increases the CPU
Utilization in guest mode to $199.76/200$. It indicates that
the guest occupies the CPUs for most of the time.
}

\mycomment{
\begin{table}[tbp]
\begin{tabular}{lllll}
\hline
& \begin{tabular}[c]{@{}l@{}}Guest\\ + vHost\end{tabular} & \begin{tabular}[c]{@{}l@{}}Guest\\ + VFIO\end{tabular} & \begin{tabular}[c]{@{}l@{}}OPTI\\ Guest\end{tabular} & \begin{tabular}[c]{@{}l@{}}DTID\\ Guest\end{tabular}\\ \hline
HLT                & 4362  & 79765 & 0    & 0    \\ \hline
EPT Misconfig.     & 55071 & 0     & 0    & 0    \\ \hline
External Interrupt & 15702 & 219   & 498  & 1    \\ \hline
%Preemption Timer   & 406   & 271   & 601  & 0    \\ \hline
%IO Instruction     & 18    & 19    & 18   & 19   \\ \hline
%MSR Read           & 2     & 2     & 2    & 2    \\ \hline
%MSR Write          & 2248  & 3919  & 4495 & 2499 \\ \hline
%Pause Instruction  & 1266  & 0     & 0    & 0    \\ \hline
%Pending Interrupt  & 273   & 0     & 0    & 0    \\ \hline
\end{tabular}
\caption{Comparison of VM Exits per Second Among Busy Guests.
The VM exits are recorded when the guest generates the TCP
outgoing traffic.}
\label{tab:vm_exit}
\end{table}
}

\mycomment{
we show the most relevant VM exits
that has the significant impact on the CPU utilization. With
the NIC assignment, the number of HLT exits per second
increases from 4362 to 79765. Both the number of VM exits per
second due to the EPT misconfiguration and external interrupts
goes down. After disabling the HLT exit, we observes the
increases number of VM exits frequency due to the timer
interrupts. This is due to the time-slice expiration from both
the host and guest. In our experiment, the host and guest has
the time-slice of 4ms which triggers 250 timer interrupts per
second. We examine the VM-exit qualification and find 498
numbers of VM exits per second are due to the timer
interrupts. This matches what we have expected with the
sampling error. In contrast, if the HLT exiting is not
disabled, some of the host timer interrupts are hidden by the
time when the host is emulating HLT instruction. Furthermore,
the timer interrupts are directly delivered to the guest
without causing any VM exit.
}

\mycomment {
\begin{table}
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|} \hline
 & Host & Host  & Guest & Guest & Total \\
 & User & System & User & System & \\ \hline
Bare-metal    & 0.64   & 80.98 & NA   & NA    & 81.62  \\ \hline
VHOST         & 68.32  & 83.85 & 0.23 & 31.20 & 115.28 \\ \hline
VFIO          & 83.35  & 99.25 & 0.26 & 34.92 & 134.43 \\ \hline
OPTI          & 199.76 & 0.24  & 0.65 & 81.62 & 82.51  \\ \hline
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Comparison of the total CPU utilization and its
breakdown when the evaluated configurations send out TCP
traffic over a 40Gbps Infiniband link at full speed using
iPerf. The total is 200\% because two CPU cores are used.}
\label{tab:cpu_utilization_40gbps}
\vspace{-0.1in}
\end{table}
}

\mycomment{
\begin{table}[]
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
                    & \multicolumn{2}{c|}{\textbf{Host}}  & \multicolumn{2}{c|}{\textbf{Guest}} & \textbf{Total} \\ \cline{2-5}
                    & \textbf{User\%} & \textbf{System\%} & \textbf{User\%} & \textbf{System\%} & \textbf{\%}    \\ \hline
\textbf{Bare-metal} & 0.64            & 80.98             & NA              & NA                & 81.62          \\ \hline
\textbf{VHOST}      & 68.32           & 83.85             & 0.23            & 31.20             & 115.28         \\ \hline
\textbf{VFIO}       & 83.35           & 99.25             & 0.26            & 34.92             & 134.43         \\ \hline
\textbf{OPTI}       & 199.76          & 0.24              & 0.65            & 81.62             & 82.51          \\ \hline
\end{tabular}
\end{center}
\caption{Comparison of the total CPU utilization and its
breakdown when the evaluated configurations send out TCP
traffic over a 40Gbps Infiniband link at full speed using
iPerf. The total is 200\% because two CPU cores are used.}
\label{tab:cpu_utilization_40gbps}
\end{table}
}

\mycomment{
\begin{table}
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|} \hline
{\bf Configuration} & {\bf Outbound} & {\bf Inbound} & {\bf Round-Trip} \\
                    & {\bf (Gbps)}   & {\bf (Gbps)}  & {\bf Delay ($\boldsymbol{\mu}$s)} \\ \hline
{\bf Bare-metal} & 37.39 & 37.52 & 12 \\ \hline
{\bf VHOST}      & 37.39 & 19.02/37.10 & 24/18\\ \hline
{\bf VFIO}       & 37.45 & 37.58 & 13 \\ \hline
{\bf OPTI}       & 37.37 & 37.52 & 13 \\ \hline
{\bf DTID}       & 37.35 & 37.50 & 12 \\ \hline
{\bf DID}        & 37.61 & 37.58 & 13 \\ \hline
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Comparison of network throughput and round-trip
packet delay over a 40Gbps Infiniband link among the six
evaluated configurations}
\label{tab:network_performance}
\vspace{-0.1in}
\end{table}
}

\mycomment{
\begin{table}[]
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
                    & \multicolumn{2}{c|}{\textbf{Host}}  & \multicolumn{2}{c|}{\textbf{Guest}} & \textbf{Total} \\ \cline{2-5}
                    & \textbf{Usr\%} & \textbf{Sys\%}    & \textbf{Usr\%} & \textbf{Sys\%}    & \textbf{\%}    \\ \hline
\textbf{Bare-metal} & 0.64            & 80.98             & NA              & NA                & 81.62          \\ \hline
\textbf{VHOST}      & 68.32           & 83.85             & 0.23            & 31.20             & 115.28         \\ \hline
\textbf{VFIO}       & 83.35           & 99.25             & 0.26            & 34.92             & 134.43         \\ \hline
\textbf{OPTI}       & 199.76          & 0.24              & 0.65            & 81.62             & 82.51          \\ \hline
\end{tabular}
\end{center}
\caption{Comparison of the total CPU utilization and its
breakdown when the evaluated configurations send out TCP
traffic over a 40Gbps Infiniband link at full speed using
iPerf~\cite{iperf}. The total is 200\% because two isolated
CPU cores are used. \textbf{Usr}: User. \textbf{Sys}: System.}
\label{tab:cpu_utilization_40gbps}
\end{table}
}

\mycomment{
\begin{table}
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|} \hline
{\bf Exit Cause} & {\bf VHOST } & {\bf VFIO} & {\bf OPTI} & {\bf DTID} \\ \hline
{\bf HLT Inst}   & 4362  & 79765 & 0    & 0    \\ \hline
{\bf EPT Fault}  & 55071 & 0     & 0    & 0    \\ \hline
{\bf Interrupt}  & 15702 & 219   & 498  & 1    \\ \hline
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Comparison of number of VM exits per second when the
evaluated configurations transmit TCP traffic over a 40Gbps
Infiniband link using iPerf~\cite{iperf}.}
\label{tab:vm_exit}
\vspace{-0.1in}
\end{table}
}


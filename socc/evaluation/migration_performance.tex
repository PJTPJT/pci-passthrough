% Migration performance.

%\figw{seamless_migration_haas}{9}{Performance of iPerf TCP
%benchmark during live migration of HaaS VM using fail-over-mac
%and hot-plug optimizations.}

To support the seamless live migration, the Linux ethernet
bonding driver was used to assist the shift of network traffic
between the virtual function (VF) or passthrough NIC and
Virtio network device in a HaaS VM. Usually the VM used the
passthrough NIC to achieve the baremetal network performance.
Before the live migration, the VM shifted the network traffic
from the VF to the Virtio NIC and unplugged the VF to satisfy
the QEMU-migration requirement. After the VM was migrated to
the destination and the VF was hotplugged, the network traffic
was shifted back to the VF. Importanly, the NIC hotplug halted
the VM and stopped the network traffic. To minimize such
network downtime, we made the VF available to the VM as soon
as it could. We optimized the NIC-hotplug mechanism in QEMU
to overlap the hotplug steps with
guest-state transfer so that the destination was ready to
configure the VF as soon as VM resumed.

In this experiment, the number of UDP packets lost were measured
during VF-to-VIRTIO switch-over and VF hotunplug at source, and 
VF hotplug and VIRTIO-to-VF switchover at destination . The VM
was configured with a bonding driver in active-backup mode
where the VFIO and Virtio NIC served as active and backup
slave, respectively. The bonding driver was configured with a
\texttt{fail\_over\_mac} parameter, which ensured that the MAC
address of the bond was always the same as the active slave.
The VM received 10K UDP packets per second, with each payload
containing a unique 8-byte sequence number. 
The {\tt tcpdump} tool~\cite{tcpdump} was used to
measure the number of lost packets during various stages. 

As seen from Table~\ref{tab:udp_packet_lost}, very few packets
were dropped during the switch-over operations at the source and destination.
Before applying our hotplug optimization, the network downtime was
around 300ms. Once our optimization was enabled, no measurable network
downtime was observed other than as the few  dropped packets mentioned above. 
As described in Section~\ref{sec:nic_migration}, the 
optimization steps at the destination were executed at the destination 
in parallel with the guest-state transfer. 
%The total migration time and downtime remained unaffected with
\begin{table}[t]
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Operation}    & \multicolumn{1}{l|}{\textbf{\# of Lost UDP}} \\ \hline
\textbf{VF to VIRTIO at source} & 25                                           \\ \hline
\textbf{Unplug VF at source}    & 0                                            \\ \hline
\textbf{Plug VF at destination}      & 0                                            \\ \hline
\textbf{VIRTIO to VF at destination} & 17                                           \\ \hline
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Small number of packets are lost during various stages of 
live migration when redirecting 
network traffic between Virtual Function (VF) and VIRTIO NICs.
%\textbf{VF}: Virtual Function on Passthrough NIC. \textbf{VIRTIO}: Virtio interface.
}
\vspace{-0.1in}
\label{tab:udp_packet_lost}
\end{table}

We also did not observe any missed timer interrupts when
disabling and re-enabling DTID during migration, since the
switch-over operation was in the order of few tens of
microseconds, the guest and hypervisor transition to/from
virtual interrupt delivery as soon as DTID was
disabled/enabled during migration.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\figw{seamless_migration_default}{8}{Place holder for seamless
%migration. Top: Guest sends the outgoing traffic. Bottom:
%guest receives the incoming traffic.}
%\figw{seamless_migration_haas}{8}{Place holder for seamless
%migration. Top: Guest sends the outgoing traffic. Bottom:
%guest receives the incoming traffic.}

%\figw{seamless_migration_default}{9}{Performance of iPerf TCP benchmark during live migration of HaaS VM without fail-over-mac and hot-plug optimizations.}

%\begin{table}[]
%\begin{tabular}{|l|l|l|l|}
%\hline
% & \begin{tabular}[c]{@{}l@{}}Unplug\\ (ms)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Plug\\ (ms)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Optimized Plug\\ (ms)\end{tabular} \\ \hline
%UDP Sender   & 4.6 & 300 & 0 \\ \hline
%UDP Receiver & 1.8 & 300 & 0 \\ \hline
%\end{tabular}
%\caption{Network downtime experienced by iPerf UDP traffic during VM migration due to Unplug, Plug, and Optimized Plug operations of the HaaS VM's VFIO network device.}
%\label{tab:downtime}
%\end{table}

%when switching
%from the VFIO (pass-through) network device to Virtio network device
%at the source machine,
%the network downtime (in the ``Unplug'' column)
%is observed to be 40ms  for the UDP sender case and 1.8ms for the  UDP receiver case.
%When switching back from the Virtio to  VFIO network device at the destination
%machine, the network downtime (in the ``Plug'' column)
%is measured to be 300ms for both the UDP sender and receiver cases.
%After we mask the overhead of VFIO NIC
%initialization at the destination machine,
%the network downtime (in the ``Optimized Plug'' column) reduces to zero for both sender and receiver cases.

%the timer interrupts received by the
%guest per seconds matches the expected frequency of timer
%interrupt received by the unmodified guest. Since KVM delivers
%the virtual interrupts with or without DTID, the guest can
%still receives its interrupt during the transition. The longer
%the transition takes, the later the migration starts. The host
%communicates with the guest by the TCP transmission. We expect
%most of the transition time is due to the packet transmission
%and processing.
%TODO: The average transition time is -- $\mu$s.

%{\em without using the \texttt{fail\_over\_mac} option},
%As shown in Figure~\ref{fig:seamless_migration_haas},
%when the switch from VFIO to Virtio is performed
%using \texttt{fail\_over\_mac} option,
%the downtime due to hot unplug reduces to 80ms.
%During live migration, the iPerf benchmark continues
%to send traffic over the Virtio network interface
%with the network bandwidth of 940Mbps.
%The VM experiences another network downtime of
%0.1 seconds during the last phase of migration
%when the guest is paused on source server
%to transfer the CPU and I/O state to destination server
%for guest resumption. This downtime is typical for pre-copy live migration.

%\na sets the VFIO network interface as the active slave and
%switches the network traffic from Virtio network interface
%back to the VFIO network interface.
%In Figure~\ref{fig:seamless_migration_default}, by default,
%the hot-plug operation introduces an additional downtime
%of 300 ms.
%To reduce the downtime introduced by
%the hot-plug event, it is broken down to three steps.
%As shown in Figure~\ref{fig:seamless_migration_haas},
%the step (a) of creating software NIC object
%and step (b) of extracting the values from the
%configuration space are executed before the guest
%is resumed at the destination. The last step (c)
%of resetting the software NIC object to set up the BAR and
%interrupt forwarding is executed after
%the live migration completes.
%Using the above  approach, as shown in Figure~\ref{fig:seamless_migration_haas},
%the downtime due to hot-plug operation reduces to zero

%Table~\ref{tab:downtime} shows that

%We also demonstrate the average
%number of missed local timer interrupts in guest,
%when we disable the DTID.

%Before migration, \na hot-unplugs the assigned
%NIC. The bonding driver detects hot-unplug event as failure
%of pass-through NIC and chooses
%Virtio NIC as the active slave.
%\texttt{fail\_over\_mac} switches the network
%traffic to the Virtio NIC by broadcasting
%gratuitous ARP packets notifying the change in MAC address of the bond.

%In Figure~\ref{fig:seamless_migration_haas}, we measure the
%bandwidth achieved by a HaaS VM when running the iPerf TCP
%benchmark over a 1Gbps network. First we note that, the iPerf
%maintains a steady throughput of 940Mbps bandwidth, except
%during brief periods that correspond to the hot-unplug and
%hot-plug of the VFIO NIC and during execution state transfer.
%Hot-unplug and execution state transfer each correspond to
%less than 100ms network disruption.

%To investigate the network disruption further, we also
%measured the number of packets lost when when sending iPerf
%UDP traffic at 15000 packets per second from the HaaS VM.
%During hot-unplug operation, iPerf reports around 600 packets
%lost, which corresponds to around 40ms network disruption.
%During hot-plug operation, without hot-plug optimization, 4000
%lost packets are reported, corresponding to 266ms network
%disruption time.  With our hot-plug optimization enabled, zero
%lost packets are reported by iPerf, showing that
%pre-configuring the VFIO NIC at destination before the VM
%resumes is effective.

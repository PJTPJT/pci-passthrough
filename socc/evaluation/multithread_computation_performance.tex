% Multi-threaded Computation Performance

\figw{parsec_barchart}{8.5}{
Slowdown of PARSEC benchmark programs for VANILLA and DID guests
compared to bare-metal.
%\textbf{VANILLA}: unmodified guest has the configuration without our optimizations.
}

\figw{fpspeed_barchart}{8.5}{
Slowdown of floating point computation benchmarks in 
SPEC CPU 2017~\cite{bucek:2018} for VANILLA and 
DID guests compared to bare-metal. 
The x-axis refers to the IDs for the following benchmarks:
%\textbf{VANILLA}: unmodified guest has the configuration
%without our optimizations. 
\textbf{603}: explosion modeling.
\textbf{607}: relativity. \textbf{619}: fluid dynamics.
\textbf{621}: weather forecasting. \textbf{627}: atmosphere
modeling. \textbf{628}: wide-scale ocean modeling.
\textbf{638}: image manipulation. \textbf{644}: molecular
dynamics. \textbf{649}: electromagnetic. \textbf{654}:
regional ocean modeling.}

\figw{intrate_barchart}{8.5}{
Slowdown of integer computation benchmarks in 
SPEC CPU 2017~\cite{bucek:2018} for VANILLA and 
DID guests compared to bare-metal. 
The x-axis refers to the IDs for the following benchmarks:
%\textbf{VANILLA}: unmodified guest has the configuration
%without our optimizations. 
{500}: perl scripts.
{502}: GCC. 
{505}: route planning.
{520}: OMNeT simulation. 
{523}: XML conversion.
{525}: video compression. 
{531}: chess.
{541}: GO. 
{548}: sudoku. 
{57}: data compression.}

To evaluate how well a HaaS VM performed for the multi-tasked
CPU-bound applications, we measured the percentage of
performance slowdown by running SPEC CPU
2017~\cite{bucek:2018} and PARSEC~\cite{bienia:2008}
benchmark. In our experiment, the HaaS VM had 8 dedicated
cores and 27GB RAM whereas \na had the two cores, including
physical core 0, and the remaining 5GB RAM. Moreover, the
topology of L1/L2/L3 CPU cache was exposed to the VM. The
bare-metal was the baseline and had the same configuration as
the VM. We compared the percent slowdown between the VMs of
DID and vanilla configuration, which did not use our
optimizations except the dedicated cores and cache
information.

Two popular thread models were considered:
Pthreads~\cite{lewis:1998} and OpenMP~\cite{dagum:1998}.
Pthreads~\cite{lewis:1998} provided the finer-grained control
over the thread management, while openMP~\cite{dagum:1998} was
the industry standard and easier to scale than
Pthreads~\cite{lewis:1998}. As shown in
Figure~\ref{fig:parsec_barchart}, most cases had the
percentage slowdown less than 5\% except canneal and
streamcluster. DID reduced the slowdown even further.
Especially for Dedup, DID trimmed 11.45\% of slowdown to
3.85\%. Although the performance of DID in Vips seemed to be
worse than Vanilla, the absolute values
differed less than 0.1 seconds.
For Canneal and Streamcluster,
even after tracking and eliminating all VM exits triggered by EPT violations
and I/O instructions from certain virtual devices
(virtual floppy and CD-ROM), the performance of 
Canneal and Stremcluster did not significantly improve.
% After analyzing the VM Exit reasons, we eliminated
%the two expensive VM exits: (1) I/O instructions and (2) EPT
%violations. To eliminate VM exits triggered by I/O instructions, 
%we removed the virtual devices for floppy disk and CD-ROM, 
%which accounted for about 20 I/O instructions per second. 
%To eliminate VM exits triggered by EPT violation, the HaaS agent in 
%the VM touched almost all guest pages at initiaization time, 
%which forced the \na to populate the corresponding EPT entries ahead of time. 
%These two optimizations reduced the rate of VM exits to around 11 VM exits per
%second. We re-ran the two benchmark programs and observed it
%had only marginal performance improvement. 
We suspect that the performance difference for these two cases 
might be due to VTx-related architecture issues; we are
further investigating the uderlying causes.



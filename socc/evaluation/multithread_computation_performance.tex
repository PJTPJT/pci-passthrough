% Multi-threaded Computation Performance

\figw{parsec_barchart}{8.5}{Percentage of CPU performance
slowdown compared to the bare-metal performance when running
the PARSEC benchmark programs. \textbf{VANILLA}: unmodified
guest has the configuration without our optimizations.}

\figw{fpspeed_barchart}{8.5}{Percentage of CPU performance
slowdown for the floating-point computation compared to the
bare-metal performance. The x-axis is the IDs for the
benchmark programs of SPEC CPU 2017~\cite{bucek:2018}.
\textbf{VANILLA}: unmodified guest has the configuration
without our optimizations. \textbf{603}: explosion modeling.
\textbf{607}: relativity. \textbf{619}: fluid dynamics.
\textbf{621}: weather forecasting. \textbf{627}: atmosphere
modeling. \textbf{628}: wide-scale ocean modeling.
\textbf{638}: image manipulation. \textbf{644}: molecular
dynamics. \textbf{649}: electromagnetic. \textbf{654}:
regional ocean modeling.}

\figw{intrate_barchart}{8.5}{Percentage of CPU performance
slowdown for the integer computation compared to the
bare-metal performance. The x-axis is the IDs for the
benchmark programs of SPEC CPU 2017~\cite{bucek:2018}.
\textbf{VANILLA}: unmodified guest has the configuration
without our optimizations. \textbf{500}: perl scripts.
\textbf{502}: GCC. \textbf{505}: route planning.
\textbf{520}: OMNeT simulation. \textbf{523}: XML conversion.
\textbf{525}: video compression. \textbf{531}: chess.
\textbf{541}: GO. \textbf{548}: sudoku. \textbf{57}: data
compression.}

To evaluate how well a HaaS VM performed for the multi-tasked
CPU-bound applications, we measured the percentage of
performance slowdown by running SPEC CPU
2017~\cite{bucek:2018} and PARSEC~\cite{bienia:2008}
benchmark. In our experiment, the HaaS VM had 8 dedicated
cores and 27GB RAM whereas \na had the two cores, including
physical core 0, and the remaining 5GB RAM. Moreover, the
topology of L1/L2/L3 CPU cache was exposed to the VM. The
bare-metal was the baseline and had the same configuration as
the VM. We compared the percent slowdown between the VMs of
DID and vanilla configuration, which did not use our
optimizations except the dedicated cores and cache
information.

Two popular thread models were considered:
Pthreads~\cite{lewis:1998} and OpenMP~\cite{dagum:1998}.
Pthreads~\cite{lewis:1998} provided the finer-grained control
over the thread management, while openMP~\cite{dagum:1998} was
the industry standard and easier to scale than
Pthreads~\cite{lewis:1998}. As shown in
Figure~\ref{fig:parsec_barchart}, most cases had the
percentage slowdown less than 5\% except canneal and
streamcluster. DID reduced the slowdown even further.
Especially for Dedup, DID trimmed 11.45\% of slowdown to
3.85\%. Although the performance of DID in Vips seemed to be
worse than the one of vanilla, it actually fell within the
0.1-second of error margin and was comparable.

Canneal and streamcluster were the two cases that caught our
attention. After analyzing the VM-exit reasons, we eliminated
the two expensive VM exits: (1) I/O instructions and (2) EPT
violations. For (1), the virtual floppy disk and CD-ROM issued
about 20 I/O instructions per second. We removed them. For
(2), VM touched almost all of its pages, which made the \na to
populate as many EPTs as it could. After applying (1) and (2)
to DID, the rate of VM exits was around 11 VM-exits per
seconds. We re-ran the two benchmark programs and observed it
had only marginal performnace improvement. We suspected that
the degradation would be VT-x related architecture issues.
This is under our current investigation.



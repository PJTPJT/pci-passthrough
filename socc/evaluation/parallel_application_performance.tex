% Multi-threaded Computation Performance

\figw{parsec_barchart}{8.5}{Slowdown of PARSEC benchmark
programs for VANILLA and DID guests compared to bare-metal.}

\figw{fpspeed_barchart}{8.5}{Slowdown of floating point
computation benchmarks in SPEC CPU 2017~\cite{bucek:2018} for
VANILLA and DID guests compared to bare-metal.}

\figw{intrate_barchart}{8.5}{Throughput reduction of integer
computation benchmarks in SPEC CPU 2017~\cite{bucek:2018} for
VANILLA and DID guests compared to bare-metal.}

To evaluate how well a HaaS VM performed for concurrent
CPU-bound applications, we measured the percentage of slowdown
or throughput reduction. Two popular thread models were
considered: Pthreads~\cite{lewis:1998} and
OpenMP~\cite{dagum:1998}. Pthreads~\cite{lewis:1998} provided
the finer-grained control over the thread management, while
OpenMP~\cite{dagum:1998} was the industry standard and easier
to scale than Pthreads~\cite{lewis:1998}. To measure the
slowdown, we used SPECspeed 2017 Floating
Point~\cite{bucek:2018} and PARSEC~\cite{lewis:1998}. To
measure the throughput reduction, we used SPECrate 2017
Integer~\cite{bucek:2018}.

In our experiment, the HaaS VM had 8 dedicated cores and 27GB
RAM whereas \na had the two cores, including physical core 0,
and the remaining 5GB RAM. Moreover, the topology of L1/L2/L3
CPU cache was exposed to the VM. The bare-metal was the
baseline and had the same configuration as the VM. We compared
the percent slowdown between the VMs of DID and vanilla
configuration, which did not use our optimizations except the
dedicated cores and cache information. To To measure the
slowdown, we used 8 threads in each in SPEC
CPU~\cite{bucek:2018} benchmark programs. To measure the
throughput reduction, we used 8 processes in each SPEC
CPU~\cite{bucek:2018} and PARSEC~\cite{lewis:1998} benchmark
programs.

For the cases of OpenMP~\cite{dagum:1998} in
Figure~\ref{fig:fpspeed_barchart}, the maximum performance
slowdown was 3\%. DID improved the performance further and did
not perform worse than the vanilla VM for the wide-scale ocean
modeling. For the cases of Pthreads~\cite{lewis:1998} in
Figure~\ref{fig:parsec_barchart}, most cases had the
percentage slowdown less than 5\% except Canneal and
Streamcluster. DID reduced the slowdown even further.

Especially for Dedup, DID trimmed 11.45\% of slowdown to
3.85\%. Although the performance of DID in Vips seemed to be
worse than Vanilla, the absolute values differed less than 0.1
seconds. For Canneal and Streamcluster, even after tracking
and eliminating all VM exits triggered by EPT violations and
I/O instructions from certain virtual devices (virtual floppy
and CD-ROM), the performance of Canneal and Streamcluster did
not significantly improve. We suspect that the performance
difference for these two cases might be due to VTx-related
architecture issues; we are further investigating the
underlying causes.

% After analyzing the VM Exit reasons, we eliminated
%the two expensive VM exits: (1) I/O instructions and (2) EPT
%violations. To eliminate VM exits triggered by I/O
%instructions, we removed the virtual devices for floppy disk
%and CD-ROM, which accounted for about 20 I/O instructions per
%second. To eliminate VM exits triggered by EPT violation, the
%HaaS agent in the VM touched almost all guest pages at
%initialization time, which forced the \na to populate the
%corresponding EPT entries ahead of time. These two
%optimizations reduced the rate of VM exits to around 11 VM
%exits per second. We re-ran the two benchmark programs and
%observed it had only marginal performance improvement. 

%As shown in Figure~\ref{fig:intrate_barchart}, DID improved
%the CPU throughput in all cases which had less than 10\% of
%throughput reductions. For 

% DID efficiency.

\figw{cyclictest_in_kernel}{8.5}{The cumulative distribution
function of timer interrupt latency measured for the
Bare-metal, DTID, DID, and unmodified-guest (VANILLA)
configuration which does not use our optimizations.}

\figw{ipi_latency}{8.5}{The cumulative distribution function
of IPI latency measured for the Bare-metal, DTID, DID, and
unmodified-guest (VANILLA) configuration which does not use our
optimizations.}

\begin{table}
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
&
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{B/W}\\ \textbf{(Gbps)}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{CPU}\\ \textbf{Core}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{NIC INTR}\\ \textbf{(per sec)}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{TMR INTR}\\ \textbf{(per sec)}\end{tabular}} \\ \hline
\multirow{2}{*}{\textbf{Bare-Metal}} & \multirow{2}{*}{37.39} & 0 & 118592 & 256 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\multirow{2}{*}{\textbf{DTID}} & \multirow{2}{*}{37.35} & 0 & 110856 & 255 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\end{tabular}
\end{center}
\vspace{-0.05in}
\caption{Comparison of the number of network and timer
interrupts per second experienced between a Bare-metal server
and a DTID HaaS VM, when they send out TCP traffic over a
40Gbps Infiniband link at full speed using iPerf.
\textbf{B/W}: Bandwidth. \textbf{TMR}: Timer. \textbf{INTR}:
Interrupt.}
\label{tab:network_interrupts}
\vspace{-0.06in}
\end{table}

To evaluate the direct local interrupt delivery, we measured
both the timer interrupt and IPI latency. The timer interrupt
latency was the timing difference between the expected and
actual wakeup time. We ran the experiment with the sleep time
of 200$\mu$s for 10 million iterations. For the IPI, our own
IPI vector was registered in the kernel. When such an IPI was
sent or received, the timestamps were recorded. The latency
was the difference between the tranmission and receipt time.
In our experiment, IPI was sent from the core A to B with the
interval of 200$\mu$s for 10 million iterations.

As shown in Figure~\ref{fig:cyclictest_in_kernel}, the median
timer interrupt latency for the Bare-metal, DID, DTID and
VANILLA configuration, which did not use our optimizations
except the dedicated cores, were 1.51, 2.03, 2.04 and 9.20
$\mu$s, respectively. Both the a DTID and DID HaaS VM came
close to the bare-metal OS with the additional 0.52$\mu$s
latency and reduced the median timer interrupt latency by
$78\%$ over a VANILLA HaaS VM. For the IPI latency in
Figure~\ref{fig:ipi_latency}, the median IPI latency for
Bare-metal, DID, DTID and VANILLA configuration were 1.80,
2.89, 3.33 and 9.35, respectively. DID HaaS VM had 1$mu$s
extra latency than Bare-metal and reduced the median latency
by $69\%$ over a VANILLA HaaS VM. Because a DTID or DID HaaS
VM still experienced VM exits due to reasons such as
privileged instructions and required to set the correct bit in
the PID page, its median timer interrupt latency is 0.52$\mu$s
more than the bare-metal OS. Reducing this difference further
by identifying the underlying VM exit reasons is part of our
future work.

Since both the NIC and timer-interrupt delivery used the
posted-interrupt mechanism, it was possible that the timer
interrupts was triggered by the network interrupts and arrived
earlier than the expected time. Then, the early timer
interrupt was serviced and the timer was updated. Nonetheless,
the more NIC interrupts occured, the more early timer
interrupts needed to be handled. When the DTID HaaS VM
performed the network I/O over the 40Gbps Infiniband link,
spurious timer interrupts did not only reduce the network
bandwidth performanc, but also kept CPU busy handling
additional timer interrupts. The filtering mechansim was
implemented to ignore early timer interrupts. Its
effectiveness was determined by measuring the frequency of NIC
and timer interrupts for the DTID haas VM, when it transmitted
the TCP traffic over a 40Gbps Infiniband link by
iPerf~\cite{iperf}.

In our experiment, the CPU core 0 was configured to process
NIC interrupts and 250Hz local timer interrupts, whereas the
CPU core 1 ran the TCP benchmark and processes 250Hz local
timer interrupts. As shown in
Table~\ref{tab:network_interrupts}, the timer interrupt rate
of CPU core 0 of the DTID configuration was almost the same as
that of the Bare-metal configuration. It demonstrated that
\sna's spurious timer interrupt filtering mechanism worked
correctly. Although the DTID configuration incurred extra
filter processing overhead for every NIC interrupt, this
overhead had negligible impact on its network bandwidth
performance. The NIC interrupt rate of CPU core 0 of the DTID
configuration was slightly lower than that of the Bare-metal
configuration, because some of NIC interrupts were processed
as part of timer interrupt processing. The timer interrupt
rate of CPU core 1 was higher than that of CPU core 0 in both
configurations because CPU core 1 runs the iPerf benchmark,
which set up aperiodic timers in addition to the system's
periodic 250Hz timers.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mycomment{
\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|l|}
\hline
& IB & CPU & \begin{tabular}[c]{@{}l@{}}NIC\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}TMR\\ INTR\end{tabular} \\ \hline
\multirow{2}{*}{HOST} & \multirow{2}{*}{37.39} & 0 & 118592 & 256 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\multirow{2}{*}{OPTI Guest} & \multirow{2}{*}{37.37} & 0 & 123024 & 256 \\ \cline{3-5}
&  & 1 & 0 & 347 \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}DTID Guest \end{tabular}} & \multirow{2}{*}{37.35} & 0 & 110856 & 255 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\end{tabular}
\caption{Comparison of Network Interrupts Between Host and
Guest. The interrupts are reported as the number of interrupts
per second. For the DTID guest, the numbers of spurious timer
interrupts per second for core 0 and 1 are 100114 and 3933
respectively. OPTI guest uses the assigned NIC with the CPU
optimization. IB: 40Gbps Infiniband. NIC: network interface
card. INTR: interrupt. TMR: timer.}
\label{tab:network_interrupts}
\end{table}
}

\mycomment{
We measure the expected frequency of timer and network
interrupts, when running the iPerf benchmark over the 40Gbps
Infiniband. Both the host and guest uses two cores. One core
handles the network interrupts, while the other core runs the
iPerf benchmark. In Table~\ref{tab:network_interrupts}, the
host and guest both receive the expected frequency of timer
and network interrupts and saturate the 40Gbps link.

Nonetheless, DTID guest needs to handle the spurious timer
interrupt before processing the network interrupts. This is
the addition CPU overhead. Since the PIR timer-interrupt is
almost always set, for each network interrupt, there is a
spurious timer interrupt. The frequency of network interrupt
and spurious timer interrupts are 110856 and 100114
respectively. The DTID algorithm simply ignores all the
spurious timer interrupts. The DTID guest is able to match the
bare-metal network bandwidth and latency, while having 1.2\%
additional CPU overhead to handle the spurious timer
interrupts as shown in Figure~\ref{fig:network_bandwidth},
Figure~\ref{fig:network_latency}, and
Table~\ref{tab:cpu_utilization_40gbps}.

Moreover, we observe that the spurious interrupt occurs on the
core running the iPerf benchmark, but there is no network
interrupts. Since the spurious timer interrupt also happens
after the VM entry, the  analysis suggests it is due to
other type of VM exits such as IO instructions, MSR writes or
IPIs.

% TODO: atopsar and micro-benchmark
Furthermore, We measure the overhead of handling spurious
interrupt. It takes --$\mu$s, while the typical handling of
timer interrupt is --$\mu$s. The data suggests our algorithm
works efficiently to deliver the timer interrupt and ignore
the spurious timer interrupts.

% DTID scalability parallel processing Kernbench
We are able to scale the DTID algorithm to all the 9 cores,
while the host has 1 dedicated core. Each vCPU receives the
expected number of timer interrupts. When the DTID guest uses
the periodic timer of 250Hz, each vCPU receives around 250
interrupts per second. To see how well the DTID guest performs
the parallel processing, we run Kernbench~\cite{kernbench} or
PARSEC~\cite{bienia:2008} benchmark. The result shows
~\ref{fig:kernbench}....
}

\mycomment{
\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|l|l|}
\hline
& IB & CPU & \begin{tabular}[c]{@{}l@{}}NIC\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}SPU\\ TMR\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}TMR\\ INTR\end{tabular} \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}DTID Guest \end{tabular}} & \multirow{2}{*}{37.35} & 0 & 110856 & 100114 & 255 \\ \cline{3-6}
&  & 1 & 0 & 3933 & 348 \\ \hline
\end{tabular}
\caption{Analysis of Spurious Timer Interrupts. The interrupts
are reported as the number of interrupts per second. DTID
guest uses the assigned NIC with the CPU optimization and DTID
enabled. IB: 40Gbps Infiniband. NIC: network interface card,
INTR: interrupt, SPU TMR INTR: spurious timer interrupt.}
\label{tab:spurious_timer_interrupt}
\end{table}
}

\mycomment{
\begin{table}
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|} \hline
& {\bf B/W} & {\bf CPU} & {\bf NIC INTR} & {\bf Tmr INTR} \\
& {\bf (Gbps)} & {\bf Core} & {\bf (per sec)} & {\bf (per sec)}  \\ \hline
\multirow{2}{*}{\bf Bare-metal} & \multirow{2}{*}{37.39} & 0 & 118592 & 256 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
%\multirow{2}{*}{\bf OPTI} & \multirow{2}{*}{37.37} & 0 & 123024 & 256 \\ \cline{3-5}
%&  & 1 & 0 & 347 \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}} {\bf DTID} \end{tabular}} & \multirow{2}{*}{37.35} & 0 & 110856 & 255 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\end{tabular}
\end{center}
\vspace{-0.05in}
\caption{Comparison of the number of network and timer
interrupts per second experienced between a Bare-metal server
and a DTID HaaS VM, when they send out TCP traffic over a
40Gbps Infiniband link at full speed using iPerf.
\textbf{B/W}: Bandwidth. \textbf{Tmr}: Timer. \textbf{INTR}:
Interrupt.}
\label{tab:network_interrupts}
\vspace{-0.06in}
\end{table}
}

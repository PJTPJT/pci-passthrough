% DID efficiency.
% - Describe what we have done.
% - Include tables and figures.
% - Evaluate the performance and see if it match our goal.

\figw{cyclictest}{10}{Cyclictest Performance. The distrubution
of interrupt latency are estimated for (a) host (b) guest and
(c) DTID guest. We apply the optimization on (b) and (c). We
also enabled DTID in (c).}

\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|l|}
\hline
& IB & CORE & \begin{tabular}[c]{@{}l@{}}NIC\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}TMR\\ INTR\end{tabular} \\ \hline
\multirow{2}{*}{HOST} & \multirow{2}{*}{37.39} & 0 & 118592 & 256 \\ \cline{3-5} 
&  & 1 & 0 & 348 \\ \hline
\multirow{2}{*}{GUEST} & \multirow{2}{*}{37.37} & 0 & 123024 & 256 \\ \cline{3-5} 
&  & 1 & 0 & 347 \\ \hline
\end{tabular}
\caption{Network Interrupts Comparison Between Host and Guest.
The interrupts are reported as the average number of
interrupts per second. We apply the CPU optimization to the
guest. IB: 40Gbps infiniband. NIC: network interface card,
INTR: interrupt, TMR: timer.}
\label{tab:network_interrupts}
\end{table}


\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|l|l|}
\hline
& IB & CORE & \begin{tabular}[c]{@{}l@{}}NIC\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}SPU\\ TMR\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}TMR\\ INTR\end{tabular} \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}GUEST\\ + DTID\end{tabular}} & \multirow{2}{*}{37.35} & 0 & 110856 & 100114 & 255 \\ \cline{3-6} 
&  & 1 & 0 & 3933 & 348 \\ \hline
\end{tabular}
\caption{Analysis of Spurious Timer Interrupts. The interrupts
are reported as the average number of interrupts per second.
We apply the optimization and DTID to the guest. The spurious
interrupts are induced when the guest receives the network
interrupts. When there is a spurious interrupt, the DTID guest
ignore it and is able to saturate the bandwidth. IB: 40Gbps
infiniband. NIC: network interface card, INTR: interrupt, SPU
TMR INTR: spurious timer interrupt.}
\label{tab:spurious_timer_interrupt}
\end{table}

In this experiment, we demonstrate that the guest improve its
timer interrupt latency using our DTID mechanism. Typically,
the host virtualizes the timer and the time interrupt by its
high-resolution timer subsystem and the delivery of virtual
timer interrupts. DTID reduces the overheads by directly
delivering the timer interrupts and allowing the guest to
configure TMICT directly. We measure the latency using the
cylictest in (a) host, (b) guest and (c) DTID guest. For (b)
and (c), we apply our CPU optimization. (c) has DTID enabled
whereas (b) has DTID disabled. We also show the additional
cost to handle the spurious timer interrupts in the DTID
guest.

In Figure~\ref{fig:cyclictest}, we observe that the median of
interrupt latency for the host, guest and DTID are --, -- and
--ns respectively. Using the DTID algorithm, we improve the
timer interrupt latency by --\%.

In Table~\ref{tab:network_interrupt}, both the host and guest
have two cores. All the network interrupts are handled on the
core 0, while the iperf benchmark runs on the core 1. The
guest has its own dedicated core once we apply the CPU
optimization. The host and guest receive the expected number
of interrupts per second. In
Table~\ref{tab:spurious_timer_interrupt}, the guest with DTID
enabled matches the baremetal network bandwidth performance ,
while handling both the expected timer interrupts and spurious
timer interrupts. The number of network and spurious timer
interrupts are -- and -- per second respectively. For every
network interrupt, the guest needs to handle a spurious timer
interrupt first.

% TODO: atopsar and micro-benchmark
We measure the overhead of handling spurious interrupt. It
takes --$\mu$s, while the typical handling of timer interrupt
is --$\mu$s. The data suggests our algorithm works efficiently
to deliver the timer interrupt and ignore the spurious timer
interrupts.

% DTID scalability
% parallel processing
% Kernbench
We are able to scale the DTID algorithm to all the 9 cores,
while the host has 1 dedicated core. Each vCPU receives the
expected number of timer interrupts. When the DTID guest uses
the periodic timer of 250Hz, each vCPU receives around 250
interrupts per second. To see how well the DTID guest performs
the parallel processing, we run Kernbench~\cite{kernbench} or
PARSEC~\cite{bienia:2008} benchmark. The result shows ....

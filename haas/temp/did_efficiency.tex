% DID efficiency.
% - Describe what we have done.
% - Include tables and figures.
% - Evaluate the performance and see if it match our goal.

\figw{cyclictest}{9}{The cumulative distribution function of timer interrupt latency measured using the Cyclictest benchmark
for Bare-metal , VHOST, OPTI, and DTID configurations.}
%\figw{kernbench}{9}{Comparison of Kernbench execution time with bare-metal}

\mycomment{
\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|l|}
\hline
& IB & CPU & \begin{tabular}[c]{@{}l@{}}NIC\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}TMR\\ INTR\end{tabular} \\ \hline
\multirow{2}{*}{HOST} & \multirow{2}{*}{37.39} & 0 & 118592 & 256 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\multirow{2}{*}{OPTI Guest} & \multirow{2}{*}{37.37} & 0 & 123024 & 256 \\ \cline{3-5}
&  & 1 & 0 & 347 \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}DTID Guest \end{tabular}} & \multirow{2}{*}{37.35} & 0 & 110856 & 255 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\end{tabular}
\caption{Comparison of Network Interrupts Between Host and
Guest. The interrupts are reported as the number of interrupts
per second. For the DTID guest, the numbers of spurious timer
interrupts per second for core 0 and 1 are 100114 and 3933
respectively. OPTI guest uses the assigned NIC with the CPU
optimization. IB: 40Gbps infiniband. NIC: network interface
card. INTR: interrupt. TMR: timer.}
\label{tab:network_interrupts}
\end{table}
}

\begin{table}
\renewcommand{\arraystretch}{1.2}
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|} \hline
& {\bf Net BW} & {\bf CPU} & {\bf NIC INTRs} & {\bf Timer INTRs} \\
& {\bf (Gpbs)} & {\bf Core} & {\bf per sec} & {\bf per sec}  \\ \hline 
\multirow{2}{*}{\bf Bare-metal} & \multirow{2}{*}{37.39} & 0 & 118592 & 256 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
%\multirow{2}{*}{\bf OPTI} & \multirow{2}{*}{37.37} & 0 & 123024 & 256 \\ \cline{3-5}
%&  & 1 & 0 & 347 \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}} {\bf DTID} \end{tabular}} & \multirow{2}{*}{37.35} & 0 & 110856 & 255 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\end{tabular}
\end{center}
\vspace{-0.05in}
\caption{Comparison of the number of network and timer interrupts per second experienced
between a Bare-metal server and a DTID HaaS VM, when they send out 
TCP traffic over a 40Gbps Infiniband link at full speed using iperf}
\label{tab:network_interrupts}
\vspace{-0.06in}
\end{table}


%\begin{table}[tbp]
%\begin{tabular}{|l|l|l|l|l|l|}
%\hline
%& IB & CPU & \begin{tabular}[c]{@{}l@{}}NIC\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}SPU\\ TMR\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}TMR\\ INTR\end{tabular} \\ \hline
%\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}DTID Guest \end{tabular}} & \multirow{2}{*}{37.35} & 0 & 110856 & 100114 & 255 \\ \cline{3-6}
%&  & 1 & 0 & 3933 & 348 \\ \hline
%\end{tabular}
%\caption{Analysis of Spurious Timer Interrupts. The interrupts
%are reported as the number of interrupts per second. DTID
%guest uses the assigned NIC with the CPU optimization and DTID
%enabled. IB: 40Gbps infiniband. NIC: network interface card,
%INTR: interrupt, SPU TMR INTR: spurious timer interrupt.}
%\label{tab:spurious_timer_interrupt}
%\end{table}

To evaluate the usefulness of direct timer interrupt delivery, we 
measured the timer interrupt latency using cyclictest~\cite{cyclictest} benchmark, which 
measures the interrupt latency by comparing the time taken when a sleep 
call is made and the corresponding wakeup time. 
We ran the cyclictest program with the sleep time of 200$\mu$s for 10 million
iterations. 
%The main thread of cyclictest runs on the core
%which is not being evaluated, whereas the test thread runs on
%the experiment core. 
We used the top 99\% of the measurements to compute a cumulative distribution function (CDF)
for the timer interrupt latency. As shown in Figure~\ref{fig:cyclictest}, the 
median timer interrupt latency for the Bare-metal, DTID, OPTI and
VHOST configuration are 1.2, 2.46, 8.06 and 13.93 $\mu$s, respectively.
That is, the a DTID HaaS VM reduces the median timer interrupt latency by
more than five times over a VHOST HaaS VM.
Because a DTID HaaS VM still experiences VM exits due to reasons such as privileged instructions,
its median timer interrupt latency is 1.26$\mu$s more than the bare-metal OS. 
Reducing this difference further by identifying the underlying VM exit reasons 
is part of our future work.

%Using the DTID and our CPU optimization, we improve the timer
%interrupt latency by 11.47$\mu$s.

To determine if a DTID HaaS VM indeed performs like a bare-metal server,
we measured the number of network and timer interrupts per second that a DTID HaaS VM and a bare-metal server
experiences when they send out TCP traffic over a 40Gbps infiniband link using iperf.
In this test, CPU core 0 is configured to process NIC interrupts and 250Hz timer interrupts, whereas CPU core 1 is only for 250Hz timer interrupts.
As shown in Table~\ref{tab:network_interrupts},
the timer interrupt rate of CPU core 0 of the DTID configuration is almost the same as that of the Bare-metal configuration, 
demonstrating that \sna's spurious timer interrupt filtering mechnaim works correctly. 
Even though the DTID configuration may incur additional filter processing overhead for every NIC interrupt,
this overhead has negligible impact on its network bandwidth performance, which is comparable to that of the Bare-metal 
configuration.   
The NIC interrupt rate of CPU core 0 of the DTID configuration is slightly lower than that of the Bare-metal configuration, 
because some of NIC interrupts are processed as part of timer interrupt processing.
The timer interrupt rate of CPU core 1 is higher than that of CPU core 0 in both configurations because iperf sets up aperiodic
timers in addition to the system's periodic timers. 




\mycomment{
We measure the expected frequency of timer and network
interrupts, when running the iperf benchmark over the 40Gbps
infiniband. Both the host and guest uses two cores. One core
handles the network interrupts, while the other core runs the
iperf benchmark. In Table~\ref{tab:network_interrupts}, the
host and guest both receive the expected frequency of timer
and network interrupts and saturate the 40Gbps link.

Nonetheless, DTID guest needs to handle the spurious timer
interrupt before processing the network interrupts. This is
the addition CPU overhead. Since the PIR timer-interrupt is
almost always set, for each network interrupt, there is a
spurious timer interrupt. The frequency of network interrupt
and spurious timer interrupts are 110856 and 100114
respectively. The DTID algorithm simply ignores all the
spurious timer interrupts. The DTID guest is able to match the
bare-metal network bandwidth and latency, while having 1.2\%
additional CPU overhead to handle the spurious timer
interrupts as shown in Figure~\ref{fig:network_bandwidth},
Figure~\ref{fig:network_latency}, and
Table~\ref{tab:cpu_utilization_40gbps}.

Moreover, we observe that the spurious interrupt occurs on the
core running the iperf benchmark, but there is no network
interrupts. Since the spurious timer interrupt also happens
after the VM entry, the perf analysis suggests it is due to
other type of VM exits such as IO instructions, MSR writes or
IPIs.

% TODO: atopsar and micro-benchmark
Furthermore, We measure the overhead of handling spurious
interrupt. It takes --$\mu$s, while the typical handling of
timer interrupt is --$\mu$s. The data suggests our algorithm
works efficiently to deliver the timer interrupt and ignore
the spurious timer interrupts.

% DTID scalability parallel processing Kernbench
We are able to scale the DTID algorithm to all the 9 cores,
while the host has 1 dedicated core. Each vCPU receives the
expected number of timer interrupts. When the DTID guest uses
the periodic timer of 250Hz, each vCPU receives around 250
interrupts per second. To see how well the DTID guest performs
the parallel processing, we run Kernbench~\cite{kernbench} or
PARSEC~\cite{bienia:2008} benchmark. The result shows
~\ref{fig:kernbench}....
}

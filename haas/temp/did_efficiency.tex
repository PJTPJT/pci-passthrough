% DID efficiency.
% - Describe what we have done.
% - Include tables and figures.
% - Evaluate the performance and see if it match our goal.

\figw{cyclictest}{9}{Cumulative Probability Distribution of
Timer-Interrupt Latency.}
%\figw{kernbench}{9}{Comparison of Kernbench execution time with baremetal}

%Kernbench performance
\begin{table}[tbp]
\begin{tabular}{|l|l|}
\hline
& \begin{tabular}[c]{@{}l@{}}Execution Time\\ (seconds)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Baremetal\end{tabular}     & 88.4   \\ \hline
\begin{tabular}[c]{@{}l@{}}Guest\end{tabular} 	      & 95.32  \\ \hline
\begin{tabular}[c]{@{}l@{}}OPTI Guest\end{tabular}    & 95.08  \\ \hline
\begin{tabular}[c]{@{}l@{}}DTID Guest\end{tabular}    & 94.82  \\ \hline
\end{tabular}
\caption{Comparison of Kernbench performance}
\label{tab:kernbench_performance}
\end{table}

\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|l|}
\hline
& IB & CPU & \begin{tabular}[c]{@{}l@{}}NIC\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}TMR\\ INTR\end{tabular} \\ \hline
\multirow{2}{*}{HOST} & \multirow{2}{*}{37.39} & 0 & 118592 & 256 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\multirow{2}{*}{OPTI Guest} & \multirow{2}{*}{37.37} & 0 & 123024 & 256 \\ \cline{3-5}
&  & 1 & 0 & 347 \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}DTID Guest \end{tabular}} & \multirow{2}{*}{37.35} & 0 & 110856 & 255 \\ \cline{3-5}
&  & 1 & 0 & 348 \\ \hline
\end{tabular}
\caption{Comparison of Network Interrupts Between Host and
Guest. The interrupts are reported as the number of interrupts
per second. For the DTID guest, the numbers of spurious timer
interrupts per second for core 0 and 1 are 100114 and 3933
respectively. OPTI guest uses the assigned NIC with the CPU
optimization. IB: 40Gbps infiniband. NIC: network interface
card. INTR: interrupt. TMR: timer.}
\label{tab:network_interrupts}
\end{table}

%\begin{table}[tbp]
%\begin{tabular}{|l|l|l|l|l|l|}
%\hline
%& IB & CPU & \begin{tabular}[c]{@{}l@{}}NIC\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}SPU\\ TMR\\ INTR\end{tabular} & \begin{tabular}[c]{@{}l@{}}TMR\\ INTR\end{tabular} \\ \hline
%\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}DTID Guest \end{tabular}} & \multirow{2}{*}{37.35} & 0 & 110856 & 100114 & 255 \\ \cline{3-6}
%&  & 1 & 0 & 3933 & 348 \\ \hline
%\end{tabular}
%\caption{Analysis of Spurious Timer Interrupts. The interrupts
%are reported as the number of interrupts per second. DTID
%guest uses the assigned NIC with the CPU optimization and DTID
%enabled. IB: 40Gbps infiniband. NIC: network interface card,
%INTR: interrupt, SPU TMR INTR: spurious timer interrupt.}
%\label{tab:spurious_timer_interrupt}
%\end{table}

In this experiment, we demonstrate that the guest improve its
timer interrupt latency using the DTID mechanism. Typically,
the host virtualizes the timer and the time interrupt by its
high-resolution timer subsystem and the delivery of virtual
timer interrupts. DTID reduces the overheads by directly
delivering the timer interrupts and allowing the guest to
configure TMICT directly. We measure the timer-interrupt
latency using the cyclictest~\cite{cyclictest} in (a) host,
(b) guest and (c) DTID guest. For (b) and (c), we also apply
our CPU optimization. (c) has DTID enabled whereas (b) has
DTID disabled. We also show the additional cost to handle the
spurious timer interrupts in the DTID guest.

To estimate the cumulative probability distribution, we
collect the timer-interrupt latency by
cyclictest~\cite{cyclictest}. It measures the latency by
comparing the time taken the sleep and wakeup time. We run the
cyclictest with the sleep time of 200$\mu$s for $10^7$
iterations. The main thread of cyclictest runs on the core
which is not being evaluated, whereas the test thread runs on
the experiment core. We use the top 99\% of data to estimate
the CDF. In Figure~\ref{fig:cyclictest}, we observe that the
median of interrupt latency for the host, DTID guest and
unmodified guest are 1.2, 2.46 and 13.93 $\mu$s respectively.
Using the DTID and our CPU optimization, we improve the timer
interrupt latency by 11.47$\mu$s.

We measure the expected frequency of timer and network
interrupts, when running the iperf benchmark over the 40Gbps
infiniband. Both the host and guest uses two cores. One core
handles the network interrupts, while the other core runs the
iperf benchmark. In Table~\ref{tab:network_interrupts}, the
host and guest both receive the expected frequency of timer
and network interrupts and saturate the 40Gbps link.

Nonetheless, DTID guest needs to handle the spurious timer
interrupt before processing the network interrupts. This is
the addition CPU overhead. Since the PIR timer-interrupt is
almost always set, for each network interrupt, there is a
spurious timer interrupt. The frequency of network interrupt
and spurious timer interrupts are 110856 and 100114
respectively. The DTID algorithm simply ignores all the
spurious timer interrupts. The DTID guest is able to match the
baremetal network bandwidth and latency, while having 1.2\%
additional CPU overhead to handle the spurious timer
interrupts as shown in Figure~\ref{fig:network_bandwidth},
Figure~\ref{fig:network_latency}, and
Table~\ref{tab:cpu_utilization_40gbps}.

Moreover, we observe that the spurious interrupt occurs on the
core running the iperf benchmark, but there is no network
interrupts. Since the spurious timer interrupt also happens
after the VM entry, the perf analysis suggests it is due to
other type of VM exits such as IO instructions, MSR writes or
IPIs.

Furthermore, We measure the overhead of handling spurious
interrupt. It takes 28.26ns, while the typical handling of
timer interrupt is 366.69ns. The data suggests our algorithm
works efficiently to deliver the timer interrupt and ignore
the spurious timer interrupts.

We are able to scale the DTID algorithm to all the 9 cores,
while the host has 1 dedicated core. Each vCPU receives the
expected number of timer interrupts. When the DTID guest uses
the periodic timer of 250Hz, each vCPU receives around 250
interrupts per second. Furthermore, we measure the CPU
throughput using kernbench. The kernbench compiles the Linux
4.10.1 in the ramdisk. It minimize the virtualization overhead
due to the disk I/O activities. In comparison to the
unmodified guest, the CPU optimization along improves the CPU
throughput by 240ms. The CPU optimization plus DTID improves
the CPU throughput by 500ms. Nonetheless, the guests of
different configuration still have the additional 6-second
overhead than the baremetal. Our analysis of VM exits
indicates the number of VM exits per second due to CPUID is
around 3357 for all guest configurations.

% CPU and network performance of assigned NIC
% - Describe what we have done.
% - Include tables and figures.
% - Evaluate the performance and see if it match our goal.

% For some reason, putting the figures next to each other
% gives me the compilation error. We temporarily comment it
% out.
%\figw{iperf}{10}{Place holder for iperf performance}
%\figw{cpu_util_iperf}{10}{Place holder for iperf CPU utiliztion}

\begin{table}[tbp]
\begin{tabular}{lllll}
\hline
& \begin{tabular}[c]{@{}l@{}}Guest\\ + vHost\end{tabular} & \begin{tabular}[c]{@{}l@{}}Guest\\ + VFIO\end{tabular} & \begin{tabular}[c]{@{}l@{}}OPTI\\ Guest\end{tabular} & \begin{tabular}[c]{@{}l@{}}DTID\\ Guest\end{tabular}\\ \hline
HLT                & 4362  & 79765 & 0    & 0    \\ \hline
EPT Misconfig.     & 55071 & 0     & 0    & 0    \\ \hline
External Interrupt & 15702 & 219   & 498  & 1    \\ \hline
%Preemption Timer   & 406   & 271   & 601  & 0    \\ \hline
%IO Instruction     & 18    & 19    & 18   & 19   \\ \hline
%MSR Read           & 2     & 2     & 2    & 2    \\ \hline
%MSR Write          & 2248  & 3919  & 4495 & 2499 \\ \hline
%Pause Instruction  & 1266  & 0     & 0    & 0    \\ \hline
%Pending Interrupt  & 273   & 0     & 0    & 0    \\ \hline
%Total              & 79438 & 84195 & 5614 & 2521 \\ \hline
\end{tabular}
\caption{Comparison of VM Exits per Second Among Busy Guests.
The VM exits are recorded when the guest generates the TCP
outgoing traffic.}
\label{tab:vm_exit}
\end{table}

%\begin{table}[tbp]
%\begin{tabular}{|l|l|l|l|}
%\hline
%& \begin{tabular}[c]{@{}l@{}}Outbound\\ (Gbps)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inbound\\ (Gbps)\end{tabular} & \begin{tabular}[c]{@{}l@{}}RTT\\ ($\mu$s)\end{tabular} \\ \hline
%\begin{tabular}[c]{@{}l@{}}Baremetal\end{tabular}     & 37.39 & 37.52 & 12\\ \hline
%\begin{tabular}[c]{@{}l@{}}Guest + vHOST\end{tabular} & 37.39 & 19.02 & 24\\ \hline
%\begin{tabular}[c]{@{}l@{}}Guest + VFIO\end{tabular}  & 37.45 & 37.58 & 13\\ \hline
%\begin{tabular}[c]{@{}l@{}}OPTI Guest\end{tabular}    & 37.37 & 37.52 & 13\\ \hline
%\begin{tabular}[c]{@{}l@{}}DTID Guest\end{tabular}    & 37.35 & 37.50 & 12\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + vHOST\end{tabular}                          & $37.39 \pm 0.04$ & $19.02 \pm 0.50$ & $24 \pm 5$\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\end{tabular}                           & $37.45 \pm 0.08$ & $37.58 \pm 0.15$ & $13 \pm 2$\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\\ + OPTIMIZATION\end{tabular}          & $37.37 \pm 0.11$ & $37.52 \pm 0.15$ & $13 \pm 3$\\ \hline
%%\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\\ + OPTIMIZATION\\ + DTID\end{tabular} & $37.35 \pm 0.08$ & $37.50 \pm 0.23$ & $12 \pm 5$\\ \hline
%\end{tabular}
%\caption{Comparison of Network Bandwidth and Latency Over the
%40Gbps Infiniband.}
%\label{tab:network_performance}
%\end{table}

% TODO: Put them together as the sub-figures.
% TODO: May need to redo the figure and make the label clear.
\figw{network_latency}{8}{Comparison of Network Latency over 40Gbps Link}
\figw{network_bandwidth}{8}{Comparison of Network Bandwith over 40Gbps Link}

\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|}
\hline
& \%User & \%System & \%Guest \\ \hline
\begin{tabular}[c]{@{}l@{}}Baremetal\end{tabular}     & 0.64   & 80.98 & -- \\ \hline
\begin{tabular}[c]{@{}l@{}}Guest + vHOST\end{tabular} & 68.6   & 84.36 & 68.6 \\ \hline
\begin{tabular}[c]{@{}l@{}}Guest + VFIO\end{tabular}  & 90.48  & 85.08 & 90.48 \\ \hline
\begin{tabular}[c]{@{}l@{}}OPTI Guest\end{tabular}    & 199.76 & 0.24  & 199.76 \\ \hline \hline
\begin{tabular}[c]{@{}l@{}}In OPTI Guest\end{tabular} & 0.66   & 81.7  & -- \\ \hline
\begin{tabular}[c]{@{}l@{}}In DTID Guest\end{tabular} & 0.68   & 82.9  & -- \\ \hline
\end{tabular}
\caption{Comparison of CPU Utilization. When the guest
generates the TCP outgoing traffic, we measure the CPU
utilization in the host. We also measure the CPU utilization
in the OPTI and DTID guest. The total CPU utilization is 200\%
for two working cores. \%User is the \%CPU time consumed in
the user mode, \%System is the \%CPU time consumed in the
kernel mode and \%Guest is the \%CPU time consumed by the
guest. \%Idle is not shown.}
\label{tab:cpu_utilization_40gbps}
\end{table}

In this experiment, we demonstrate that the guest using our
optimization achieves near bare-metal performance with minimum
CPU utilization and minimal hypervisor involvement. We use the
macro benchmark and measure the performance in the following
three metrics: network latency, bandwidth and CPU utilization.

The network latency is measured by the round-trip delay using
ping~\cite{ping}. For the gigabit link, the RTT for the host
and guest using VFIO configuration is about 160$\mu$s, whereas
the RTT of guest using vHost as the backend driver is
173$\mu$s. For the 40Gbps infiniband, the guest using the VFIO
out performs the guest using the vHost configuration by 50\%.
As shown in Figure~\ref{fig:network_latency}, the RTT for the
host and guest using VFIO configuration is about 13$\mu$s,
whereas the RTT of guest using vHost as the backend driver is
24$\mu$s.

The network bandwidth is measure by the incoming and outgoing
TCP traffic using iperf~\cite{iperf}. The bandwidth
performance for the gigabit link is not shown here. All the
configurations are able to saturate the 1Gbps NIC and 40 Gbps
infiniband for the outgoing TCP throughput. For the incoming
traffic, the bandwidth performance of 40Gbps infiniband is
differed. The guests using the VFIO configuration saturate the
bandwidth and out perform the guest using the vHost
configuration by 48\% as shown in
Figure~\ref{fig:network_bandwidth}.

Although the outgoing TCP throughput is about 37.4Gbps for the
guest using the vHost as the backend driver or assigned NIC,
the guest using the assigned NIC has higher CPU utilization.
The VFIO configuration consumes $175.56/200$ of total CPU
utilization, while the vHost configuration consumes
$152.96/200$ of total CPU utilization. This is due to the
higher number of HLT instruction issued by the guest using the
assigned NIC. The NIC assignment by VT-d allows the guest to
handle the network interrupts directly without a VM exit. At
the same time, it keeps the guest on its CPU for longer time.
The guest issues the HLT instruction more frequently and waits
for its network I/O, when it is idle. The vCPUs burn CPU
cycles when they poll for sometime before executing the HLT
instruction. The vCPU polling mechanism keeps the CPU busy
leading to higher CPU utilization.

To reduce the CPU utilization in host, we disable the HLT
exits by modifying the VMCS structure in KVM. To avoid other
system processes to compete with guest, the vCPUs are pinned
on isolated CPUs. As shown in
\ref{tab:cpu_utilization_40gbps}, we notice that disabling HLT
exits along with dedicating cores to guest, reduces the CPU
utilization in system mode to $0.24/200$ and increases the CPU
Utilization in guest mode to $199.76/200$. It indicates that
the guest occupies the CPUs for most of the time.

In Table \ref{tab:vm_exit}, we show the most relevant VM exits
that has the significant impact on the CPU utilization. With
the NIC assignment, the number of HLT exits per second
increases from 4362 to 79765. Both the number of VM exits per
second due to the EPT misconfiguration and external interrupts
goes down. After disabling the HLT exit, we observes the
increases number of VM exits frequency due to the timer
interrupts. This is due to the time-slice expiration from both
the host and guest. In our experiment, the host and guest has
the time-slice of 4ms which triggers 250 timer interrupts per
second. We examine the VM-exit qualification and find 498
numbers of VM exits per second are due to the timer
interrupts. This matches what we have expected with the
sampling error. In contrast, if the HLT exiting is not
disabled, some of the host timer interrupts are hidden by the
time when the host is emulating HLT instruction. Furthermore,
the timer interrupts are directly delivered to the guest
without causing any VM exit.

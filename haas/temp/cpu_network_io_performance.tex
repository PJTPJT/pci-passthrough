% CPU and network performance of assigned NIC
% - Describe what we have done.
% - Include tables and figures.
% - Evaluate the performance and see if it match our goal.

% For some reason, putting the figures next to each other
% gives me the compilation error. We temporarily comment it
% out.
%\figw{iperf}{10}{Place holder for iperf performance}
%\figw{cpu_util_iperf}{10}{Place holder for iperf CPU utiliztion}

\begin{table}
\begin{tabular}{|p{4cm}|p{1.8cm}|p{1.8cm}|} \hline
 & \multicolumn{2}{|c|}{\textbf{Number of VM Exits}} \\ \cline{2-3} 
\textbf{VM Exit Reasons}& \textbf{Before \linebreak opti} & \textbf{After \linebreak opti}\\ %\hline
\textbf{HLT} & &\\ \hline
\textbf{MSR\_WRITE} & &\\ \hline
\textbf{EXTERNAL\_INTERRUPT} & &\\ \hline
\textbf{PREEMPTION\_TIMER} & &\\ \hline
\textbf{IO\_INSTRUCTION} & &\\ \hline
\textbf{EXCEPTION\_NMI} & &\\ \hline
\textbf{PAUSE\_INSTRUCTION} & &\\ \hline
\textbf{EPT\_VIOLATION} & &\\ \hline
\textbf{MSR\_READ} & &\\ \hline
\textbf{CPUID} & &\\ \hline
\textbf{TOTAL} &  &\\ \hline
\end{tabular}
\vspace{6pt}
\caption{Place holder for VM Exit Reasons}
\label{tab:vm_exit_reasons}
\end{table}

\begin{table}
\begin{tabular}{|l|p{2cm}|p{2cm}|}
\hline
\textbf{} & \textbf{GIGABIT ($\mu$s)} & \textbf{INFINIBAND ($\mu$s)} \\ \hline
\textbf{BAREMETAL} & & \\ \hline
\textbf{vHOST} & & \\ \hline
\textbf{VFIO} & & \\ \hline
\textbf{VFIO + OPTI} & & \\
\hline
\end{tabular}
\caption{Network Latency}
\label{tab:network_latency}
\end{table}

\begin{table}
\begin{tabular}{|l|p{2cm}|p{2cm}|}
\hline
\textbf{} & \textbf{GIGABIT (Mbps)} & \textbf{INFINIBAND (Gbps)} \\ \hline
\textbf{BAREMETAL} & & \\ \hline
\textbf{vHOST} & & \\ \hline
\textbf{VFIO} & & \\ \hline
\textbf{VFIO + OPTI} & & \\
\hline
\end{tabular}
\caption{Network BandWidth}
\label{tab:network_bandwidth}
\end{table}

\begin{table}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{} & \textbf{\%GUEST} & \textbf{\%USER} & \textbf{\%SYS} \\ \hline
\textbf{BAREMETAL} & 0 & & \\ \hline
\textbf{vHOST} & & & \\ \hline
\textbf{VFIO} & & & \\ \hline
\textbf{VFIO + OPTI} & & & \\ \hline
\textbf{OPTI GUEST} & 0 & & \\
\hline
\end{tabular}
\caption{CPU Utilization while saturating the gigabit link}
\label{tab:cpu_utilization_1gbps}
\end{table}

\begin{table}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{} & \textbf{\%GUEST} & \textbf{\%USER} & \textbf{\%SYS} \\ \hline
\textbf{BAREMETAL} & 0 & & \\ \hline
\textbf{vHOST} & & & \\ \hline
\textbf{VFIO} & & & \\ \hline
\textbf{VFIO + OPTI} & & & \\ \hline
\textbf{OPTI GUEST} & 0 & & \\
\hline
\end{tabular}
\caption{CPU Utilization while saturating the infiniband}
\label{tab:cpu_utilization_40gbps}
\end{table}

In this experiment, we demonstrate that the \na achieves near
bare-metal performance with minimum CPU utilization and
minimal hypervisor involvement in the following three metrics.
\begin{itemize}
  \item Network latency.
  \item Network bandwidth.
  \item CPU utilization.
\end{itemize}

We measure the performance using macro benchmark in (a) Host
(b) VM with virtio-net interface and (c) VFIO interface.
Additionally, we also show that we eliminate the VM exits due
to HLT instruction.

The network latency is measured by the round-trip delay using
the ping tool. In Table~ref{tab:network\_latency}, we observe
that the network latency in host for 1Gbps and 40Gbps NIC is
-- and --$\mu$s. With virtio network front-end device and
vhost back-end driver, the network latency in guest for 1Gbps
and 40Gbps is -- and --$\mu$s. The network latency in guest
using the assigned 1Gbps and 40Gbps NIC is -- and --$\mu$s.
(c) is out performed (b) by --\%, while matching the network
latency in host.

We measure the TCP throughput in (a)-(c) using iperf benchmark
\cite{}. In Table~\ref{tab:network_bandwidth}, we observe that
the network bandwidth in host for a 1Gbps and 40Gbps NIC is
--Mbps and --Gbps respectively. With virtio network front-end
device and vhost back-end driver, the network bandwidth in
guest for 1Gbps and 40Gbps is --Mbps and --Gbps. The guest
achieves near bare metal performance using configuration (b)
and (c).  However, the CPU utilization differs between (b) and
(c). With virtio network device backed with vhost driver, the
CPU utilization in system mode is --\% and --\% in guest mode.
With NIC passthrough we observe that the CPU utilization is in
system mode is --\% and --\% in guest mode. The CPU overhead
with passthrough NIC is high compared to virtio network backed
by vhost driver. The vCPUs in (c) execute HLT instruction by
--\% more frequently than the vCPUs in (b) when the guest is
idle. The vCPUs burn CPU cycles when they poll for sometime
before executing the HLT instruction. The vCPU polling
mechanism keeps the CPU busy leading to higher CPU
utilization.

As explained before, to reduce the CPU utilization in host, we
disable the HLT exits by modifying the VMCS structure in KVM.
To avoid other system processes to compete with guest, the
VCPUs are pinned on isolated CPUs. As shown in \ref{}, we
notice that disabling HLT exits along with dedicating cores to
guest, reduces the CPU utilization in system mode and
increases the CPU Utilization in guest mode indicating that
the guest occupies the CPUs for most of the time. In \ref{} we
show that the number of VM exits due to HLT instruction is
zero.

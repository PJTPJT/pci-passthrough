% CPU and network performance of assigned NIC
% - Describe what we have done.
% - Include tables and figures.
% - Evaluate the performance and see if it match our goal.

% For some reason, putting the figures next to each other
% gives me the compilation error. We temporarily comment it
% out.
%\figw{iperf}{10}{Place holder for iperf performance}
%\figw{cpu_util_iperf}{10}{Place holder for iperf CPU utiliztion}

\begin{table}[tbp]
\begin{tabular}{|l|l|l|}
\hline
& \begin{tabular}[c]{@{}l@{}}BEFORE\\ OPTI\end{tabular} & \begin{tabular}[c]{@{}l@{}}AFTER\\ OPTI\end{tabular} \\ \hline
HLT                & 4785929 & 0 \\ \hline
EXTERNAL INTERRUPT & 13114   & 29881 \\ \hline
IO INSTRUCTION     & 1110    & 1073 \\ \hline
MSR READ           & 120     & 120 \\ \hline
MSR WRITE          & 235121  & 269695 \\ \hline
PAUSE INSTRUCTION  & 23      & 0 \\ \hline
PREEMPTION TIMER   & 16260   & 36063 \\ \hline
TOTAL              & 5051677 & 336832 \\ \hline
\end{tabular}
\caption{Comparison of VM Exit Before and After the CPU
Optimization. The VM exits are recorded when the guest with
the passthrough NIC generates the TCP outgoing traffic.}
\label{tab:vm_exit}
\end{table}

\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|}
\hline
& \begin{tabular}[c]{@{}l@{}}OUTBOUND\\ TRAFFIC\\ (Gbps)\end{tabular} & \begin{tabular}[c]{@{}l@{}}INBOUND\\ TRAFFIC\\ (Gbps)\end{tabular} & \begin{tabular}[c]{@{}l@{}}RTT\\ ($\mu$s)\end{tabular} \\ \hline
BAREMETAL   & $37.39 \pm 0.08$ & $37.52 \pm 0.10$ & $12 \pm 3$\\ \hline
vHOST GUEST & $37.39 \pm 0.04$ & $19.02 \pm 0.50$ & $24 \pm 5$\\ \hline
VFIO  GUEST & $37.45 \pm 0.08$ & $37.58 \pm 0.15$ & $13 \pm 2$\\ \hline
OPTI  GUEST & $37.37 \pm 0.11$ & $37.52 \pm 0.15$ & $13 \pm 3$\\ \hline
DTID  GUEST & $37.35 \pm 0.08$ & & $12 \pm 5$\\ \hline
\end{tabular}
\caption{Comparison of Network Bandwith and Latency.}
\label{tab:network_performance}
\end{table}

\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|}
\hline
& \%USER & \%SYSTEM & \%GUEST \\ \hline
BAREMETAL     & 0.64   & 80.98 & -- \\ \hline
vHOST GUEST   & 68.6   & 84.36 & 68.6 \\ \hline
VFIO GUEST    & 90.48  & 85.08 & 90.48 \\ \hline
OPTI GUEST    & 199.76 & 0.24  & 199.76 \\ \hline
IN OPTI GUEST & 0.66   & 81.2  & -- \\ \hline
IN DITD GUEST & 0.68   & 82.9  & -- \\ \hline
\end{tabular}
\caption{Comparison of CPU Utilization. When the guest
generates the TCP outgoing traffic, we measure the CPU
utilization of 2 cores in the host. We also measure the CPU
utilization in both the OPTI and DTID guest. \%USER is the
\%CPU time consumed in the user mode, \%SYSTEM is the \%CPU
time consumed in the kernel mode and \%GUEST is the \%CPU time
consumed by the guest.}
\label{tab:cpu_utilization_40gbps}
\end{table}

In this experiment, we demonstrate that the guest using our
optimization achieves near bare-metal performance with minimum
CPU utilization and minimal hypervisor involvement. We use the
macro benchmark and measure the performance in the following
three metrics: network latency, bandwidth and CPU utilization.

The network latency is measured by the round-trip delay using
ping~\cite{ping}. The RTT measured from the 1Gbps link is not
shown there. The RTT for the host and guest using VFIO
configuration is about 160 $\mu$s, whereas the RTT of guest
using vHost as the backend driver is 173 $\mu$s. Similar trend
is detected when measuring the RTT using the 40Gbps
infiniband. In Table~\ref{tab:network_performance}, the guest
using the VFIO out performs the guest using the vHost
configuration by 50\%.

The network bandwidth is measure by the incoming and outgoing
TCP traffic using iperf~\ref{iperf}. The bandwidth performance
for the gigabit link is not shown here. All the configurations
are able to saturate the 1Gbps NIC and 40 Gbps infiniband for
the outgoing traffic. For the incoming traffic, the bandwidth
performance of 40Gbps infiniband is differed. The guests using
the VFIO configuration saturate the bandwidth and out perform
the guest using the vHost configuration by 48\% as shown in
Table~\ref{tab:network_performance}.

Althought the outgoing TCP throughput is about 37.4Gbps for
the guest using the vHost as the backend driver or assigned
NIC, the guest using the assigned NIC has higher CPU
utilization. The VFIO configuration consumes $175.56/200$ of
total CPU utilization, while the vHost configuration consumes
$152.96/200$ of total CPU utlization. This is due to the
higher number of HLT instruction issued by the guest using the
assigned NIC. The NIC assignment by VT-d allows the guest to
handle the network interrupts directly without a VM exit. At
the same time, it keeps the guest on its CPU for longer time.
The guest issues the HLT instruction more frequently and waits
for its network I/O, when it is idle. The vCPUs burn CPU
cycles when they poll for sometime before executing the HLT
instruction. The vCPU polling mechanism keeps the CPU busy
leading to higher CPU utilization.

As explained before, to reduce the CPU utilization in host, we
disable the HLT exits by modifying the VMCS structure in KVM.
To avoid other system processes to compete with guest, the
VCPUs are pinned on isolated CPUs. As shown in
\ref{tab:cpu_utilization_40gbps}, we notice that disabling HLT
exits along with dedicating cores to guest, reduces the CPU
utilization in system mode and increases the CPU Utilization
in guest mode indicating that the guest occupies the CPUs for
most of the time. In \ref{tab:vm_exit} we show that the number
of VM exits due to HLT instruction is zero.

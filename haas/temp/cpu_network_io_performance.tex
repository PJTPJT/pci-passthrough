% CPU and network performance of assigned NIC
% - Describe what we have done.
% - Include tables and figures.
% - Evaluate the performance and see if it match our goal.

% For some reason, putting the figures next to each other
% gives me the compilation error. We temporarily comment it
% out.
%\figw{iperf}{10}{Place holder for iperf performance}
%\figw{cpu_util_iperf}{10}{Place holder for iperf CPU utiliztion}


\begin{table}[tbp]
\begin{tabular}{|l|l|l|}
\hline
& \begin{tabular}[c]{@{}l@{}}BEFORE\\ OPTI\end{tabular} & \begin{tabular}[c]{@{}l@{}}AFTER\\ OPTI\end{tabular} \\ \hline
HLT & 4785929 & 0 \\ \hline
EXTERNAL INTERRUPT & 13114 & 29881 \\ \hline
IO INSTRUCTION & 1110 & 1073 \\ \hline
MSR READ & 120 & 120 \\ \hline
MSR WRITE & 235121 & 269695 \\ \hline
PAUSE INSTRUCTION & 23 & 0 \\ \hline
PREEMPTION TIMER & 16260 & 36063 \\ \hline
TOTAL & 5051677 & 336832 \\ \hline
\end{tabular}
\caption{Comparison of VM Exit Before and After the CPU
Optimization. The VM exits are recorded for 60 seconds when
the guest with the passthrough NIC generates the TCP outgoing
traffic. The HLT exiting is eliminated.}
\label{tab:vm_exit}
\end{table}


\begin{table}[tbp]
\begin{tabular}{|l|l|l|}
\hline
& \begin{tabular}[c]{@{}l@{}}OUTGOING\\ TRAFFIC\end{tabular} & \begin{tabular}[c]{@{}l@{}}INCOMING\\ TRAFFIC\end{tabular} \\ \hline
BAREMETAL & $37.39 \pm 0.08$ & $37.52 \pm 0.10$ \\ \hline
\begin{tabular}[c]{@{}l@{}}GUEST\\ + vHOST\end{tabular} & $37.39 \pm 0.04$ & $19.02 \pm 0.50$ \\ \hline
\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\end{tabular} & $37.45 \pm 0.08$ & $37.58 \pm 0.15$ \\ \hline
\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\\ + OPTIMIZATION\end{tabular} & $37.37 \pm 0.11$ & $37.52 \pm 0.15$ \\ \hline
\end{tabular}
\caption{Comparison of Network Bandwith for the Outgoing and
Incoming TCP Traffic. We measured the network bandwidth for
the (a) baremetal, (b) guest using the Virtio frontend backed
by the host vHost driver, (c) guest using the assigned NIC and
(d) guest using the assigned NIC with CPU optimization.}
\label{tab:network_bandwidth}
\end{table}

\begin{table}[tbp]
\begin{tabular}{|l|l|}
\hline
& RTT ($\mu$s) \\ \hline
BAREMETAL & $12 \pm 3$ \\ \hline
\begin{tabular}[c]{@{}l@{}}GUEST\\ + vHOST\end{tabular} & $24 \pm 5$ \\ \hline
\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\end{tabular} & $13 \pm 2$ \\ \hline
\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\\ + OPTIMIZATION\end{tabular} & $13 \pm 5$ \\ \hline
\end{tabular}
\caption{Comparison of Network Latency. We measured the
network latency for the (a) baremetal, (b) guest using the
Virtio frontend backed by the host vHost driver, (c) guest
using the assigned NIC and (d) guest using the assigned NIC
with CPU optimization.}
\label{tab:network_latency}
\end{table}

\begin{table}[tbp]
\begin{tabular}{|l|l|l|l|}
\hline
& \%USER & \%SYSTEM & \%GUEST \\ \hline
BAREMETAL & 0.64 & 80.98 & -- \\ \hline
\begin{tabular}[c]{@{}l@{}}GUEST\\ + vHOST\end{tabular} & 68.6 & 84.36 & 68.6 \\ \hline
\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\end{tabular} & 90.48 & 85.08 & 90.48 \\ \hline
\begin{tabular}[c]{@{}l@{}}GUEST\\ + VFIO\\ + OPTIMIZATION\end{tabular} & 199.76 & 0.24 & 199.76 \\ \hline
IN GUEST & 0.66 & 81.62 & -- \\ \hline
\end{tabular}
\caption{Comparison of CPU Utilization. When the guest
generates the TCP outgoing traffic, we measured the CPU
utilization of 2 cores in the host with the following
configuration: (a) baremetal, (b) guest using the Virtio
frontend backed by the host vHost driver, (c) guest using the
assigned NIC and (d) guest using the assigned NIC with CPU
optimization.  We also measure the CPU utilization in the
guest using (d). \%USER is the \%CPU time consumed in the user
mode, \%SYSTEM is the \%CPU time consumed in the kernel mode
and \%GUEST is the \%CPU time consumed by the guest.}
\label{tab:cpu_utilization_40gbps}
\end{table}

In this experiment, we demonstrate that the \na achieves near
bare-metal performance with minimum CPU utilization and
minimal hypervisor involvement in the following three metrics.
\begin{itemize}
  \item Network latency.
  \item Network bandwidth.
  \item CPU utilization.
\end{itemize}

We measure the performance using macro benchmark in (a) Host
(b) VM with virtio-net interface and (c) VFIO interface.
Additionally, we also show that we eliminate the VM exits due
to HLT instruction.

The network latency is measured by the round-trip delay using
the ping tool. In Table~\ref{tab:network_latency}, we observe
that the network latency in host for 1Gbps and 40Gbps NIC is
-- and --$\mu$s. With virtio network front-end device and
vhost back-end driver, the network latency in guest for 1Gbps
and 40Gbps is -- and --$\mu$s. The network latency in guest
using the assigned 1Gbps and 40Gbps NIC is -- and --$\mu$s.
(c) is out performed (b) by --\%, while matching the network
latency in host.

We measure the TCP throughput in (a)-(c) using iperf benchmark
\cite{iperf}. In Table~\ref{tab:network_bandwidth}, we observe that
the network bandwidth in host for a 1Gbps and 40Gbps NIC is
--Mbps and --Gbps respectively. With virtio network front-end
device and vhost back-end driver, the network bandwidth in
guest for 1Gbps and 40Gbps is --Mbps and --Gbps. The guest
achieves near bare metal performance using configuration (b)
and (c).  However, the CPU utilization differs between (b) and
(c). With virtio network device backed with vhost driver, the
CPU utilization in system mode is --\% and --\% in guest mode.
With NIC passthrough we observe that the CPU utilization is in
system mode is --\% and --\% in guest mode. The CPU overhead
with passthrough NIC is high compared to virtio network backed
by vhost driver. The vCPUs in (c) execute HLT instruction by
--\% more frequently than the vCPUs in (b) when the guest is
idle. The vCPUs burn CPU cycles when they poll for sometime
before executing the HLT instruction. The vCPU polling
mechanism keeps the CPU busy leading to higher CPU
utilization.

As explained before, to reduce the CPU utilization in host, we
disable the HLT exits by modifying the VMCS structure in KVM.
To avoid other system processes to compete with guest, the
VCPUs are pinned on isolated CPUs. As shown in \ref{}, we
notice that disabling HLT exits along with dedicating cores to
guest, reduces the CPU utilization in system mode and
increases the CPU Utilization in guest mode indicating that
the guest occupies the CPUs for most of the time. In \ref{} we
show that the number of VM exits due to HLT instruction is
zero.

\section{Virtualization Support for HaaS}
% Virtualization support for HaaS include the following items
% CPU idleness detection and processing
% Direct NIC assignment
%   - VFIO
%   - CPU optimization for the baremetal network performance
% Direct interrupt delivery
%   - Posted-interrupt mechanism
%   - Direct timer-interrupt delivery
%     - periodic vs aperiodic timer interrupt
%     - one-shot vs periodic hardware timer interrupt
%     - guest-level access to the PIR page
%     - Eliminate the hardware lock, when the guest accesses
%       the PIR bit
%     - Spurious timer interrupts
% Seamless baremetal virtual machine migration
%   - NIC bonding with the hot plug/unplug operation migration
%   - DTID during the migration

\figw{architecture}{9}{Architecture of \na for single-VM virtualization (SVMV). 
The hypervisor runs on a single CPU core and provides
manageability services such as live migration and VM monitoring.
The HaaS VM runs on all the remaining CPU cores with direct access to I/O devices 
and local timers, including direct delivery of device and timer interrupts.}

%The design goal of \na is to eliminate the hypervisor from
%the guest I/O path while achieving bare-metal performance in
%the guest. To achieve this \na considers direct device
%assignment to the guest using Intel's VT-d support. First, the
%guest meets the bare-metal network and disk I/O performance
%with direct device assignment. We further apply optimizations
%to reduce the CPU utilization by hypervisor. Second, the timer
%interrupts are transformed into the posted interrupts, which
%are directly delivered to the guest by the logical processor
%without causing any VM exits. Finally, we present the
%migration of virtual machine with directly assigned devices.

To enhance the manageability of physical servers offered in a bare-metal cloud service, \na installs 
a thin hypervisor on each physical server  to create a single VM (called the HaaS VM), on which 
a \na user then installs a preferred OS and applications. 
Figure~\ref{fig:architecture} shows the high-level architecture of \na.
This  hypervisor is primarily meant to provide management functionalities, such as 
physical machine migration and application performance monitoring, rather than virtualization 
of server resources, and therefore is largely not involved in the I/O operations in which a HaaS VM participates.
%This section describes in detail the main design issues of this hypervisor, 
%which is based on Linux/KVM/QEMU, and our corresponding solutions.

I/O operations and interrupt processing using traditional VMs incur higher overheads than bare-metal execution. 
I/O operations issued by a VM typically trap into the hypervisor via VM exits for emulation.
Likewise, external device interrupts and local timer interrupts 
to the CPU running a VM result in VM exits for emulating virtual interrupt delivery.
Each VM exit is expensive, since it requires saving the VM's execution context upon exit,
emulation of the exit reason in hypervisor mode, 
and finally restoration of the VM's context before re-entry into guest mode.

The main technical challenge of providing the guest OS with the illusion of running on 
a bare-metal server is that its interactions with I/O devices, such as network 
interface card (NIC) and disk controller, must be direct without going through any intermediary.
\na allows the sole VM on the physical machine to directly interact with PCIe I/O devices and 
timer hardware by leveraging Intel VT-d~\cite{intelvtd-paper} and 
Linux Virtual Function I/O (VFIO)~\cite{vfio} mechanisms. 
To match bare-metal I/O performance, \na implements mechanisms 
for reducing the hypervisor's CPU, I/O, and memory 
footprint during runtime to minimize interference with guest operations.
In addition \na also enables direct delivery of both device interrupts
and timer interrupts to a HaaS VM by leveraging
the posted interrupt mechanism in Intel VT-d.
Direct interrupt delivery greatly reduces the interrupt processing latency and is
particularly useful for real-time applications running on bare-metal servers. 
When the VM must be live migrated, \na temporarily switches the VM to use
para-virtual I/O and virtualized timer, completes the migration
and, at the destination, switches the VM back to direct I/O device and direct 
timer access, all with minimal disruption of VM's workload.

In this section, we first  provide background on direct device access using 
Intel VT-d and Linux KVM/QEMU~\cite{kvm}, followed by the description of design challenges 
and our solutions for achieving bare-metal performance while supporting live migration.

\subsection{Background: Intel VT-d and VFIO}

%\subsection{Direct Interactions with PCI Devices}
Intel VT-d~\cite{intelvtd-paper} provides processor-level support for direct 
and safe access to hardware I/O devices by VMs running in non-root mode.
Virtual function I/O (VFIO)~\cite{vfio} is a Linux software framework that enables user-space
device drivers to interact with PCIe devices directly without involving the Linux kernel. 
In general, there are four types of interactions between an OS and its PCIe I/O devices:
\begin{enumerate} 
\parskip 0mm
\itemsep 0mm
%\setlength\itemsep{-0.04in}
\item At the system start-up time, the OS probes and enumerates the PCIe devices existing in the system, and assigns a device number, configuration space address range, and interrupt to each discovered device.

\item The OS reads and writes a device's configuration register space and the memory regions specified in its base address registers (BAR), including setting up DMA operations.

\item A DMA engine moves data blocks between main memory and PCIe devices according to the DMA commands set up by the OS.

\item A device interrupts the OS for its attention to certain hardware events, such as new packet arrival or transmission completion.

\end{enumerate}   
Except the probe/enumeration operations, 
\na enables a guest OS to interact directly (without VM exits) with NICs and disk controllers
for the other three types of device interactions. 

\subsubsection{Direct DMA Operations Using VT-d and VFIO} 
Typically, a PCIe device driver communicates with its device using programmed I/O (PIO) or
memory-mapped I/O (MMIO) operations against the memory areas associated with the device. 
Essentially, VFIO makes the configuration register space and the memory regions of
each PCIe device accessible to user-space processes.
%VFIO retrieves a PCIe device's device-specifc information such as BARs from its
%configuration register space, and maps them into distinct regions in a special file. 
%By reading and writing specific regions of this special file, a user-space program 
%is able to directly interact a particular PCIe device. 
%VFIO provides user-space programs a PCIe device read/write API, and converts these API calls
%into file read/write operations against the special PCIe device file.
%
%The userspace driver uses
%the device file descriptor and offset to access each region
%and retrieve the device information. The VFIO decomposes the
%physical device to a software interface. Such software
%interface is turned into the assigned device by QEMU.
%Essentially, the device read and write handler in the QEMU
%memory API is forwarded to the VFIO read and write handler.
%Not all accesses to a PCIe device's associated memory regions by user-space processes are direct, 
%because some parts of a PCIe configuration register space are privileged, such as 
%message signalled interrupts (MSI), BARs, and ROM,  and accessing them 
%requires KVM's or QEMU's emulation. 
%
%Nonetheless, accessing some parts of PCI configuration
%PCI configuration space is
%not handled as memory regions in QEMU. Some of the accesses to
%the PCI configuration space is passthroughed directly, while
%others, such as MSI, BARs, and ROM, need to be emulated.
%
Thus a user-space process can use DMA operations to move data directly between a PCIe device and 
a region of its virtual address space.
The address of the source or destination buffer of a DMA operation resides in 
the {\em device address space} of the PCIe device involved in the DMA operation.
Intel's VT-d architecture provides an IOMMU~\cite{ben:2006}, that maps a 
PCIe operation's {\em device address} into a {\em physical address}, 
which is used to access main memory. 
\mycomment{
When a user-space program sets up a DMA operation, it only knows the virtual address of the associated buffer and thus places
this address in the DMA command, essentially treating this virtual address as a device address. 
Given such a DMA command, VFIO consults with the page table associated with this user-space program to extract the physical address corresponding
to the DMA buffer's virtual address, and creates an IOMMU entry that links the buffer's virtual (device) address with its associated physical address. 
When this DMA command is executed at run time, its buffer's device address is correctly translated to its corresponding physical address.
 }
  
The key field of each IOMMU entry includes a PCIe device number and a device address.
Therefore, a machine's IOMMU may map the same device address into different physical addresses when the device address is associated with different PCIe devices.
By controlling which user-space programs can access which PCIe devices, VFIO is able to leverage IOMMU to effectively prevent a user-space program 
from using DMA operations to corrupt the physical memory areas owned by other user-space programs.

To KVM, a VM runs as part of a user-space process  called QEMU~\cite{qemu},
specifically, as guest-mode threads within QEMU's virtual address space.
QEMU uses VFIO to configure a VM to directly access the device address space of 
its assigned PCIe devices without emulation via KVM or QEMU.
In contrast, in para-virtual Vhost~\cite{vhost-net} I/O architecture, 
each incoming or outgoing I/O operation (network or block I/O) must go through 
a special hypervisor-level thread (called the vhost worker thread), 
which emulates a virtio~\cite{russell:2008} device in the kernel and 
therefore keeps QEMU out of the data plane.
However, QEMU is still responsible for such control plane processing as setting up, 
configuring, and negotiating features for an in-kernel virtio device.  



\mycomment{
Second, the VFIO programs the IOMMU to transfer the data
between the userspace driver and device in a hardware
protected manner. The VT-d IOMMU provides the device isolation
using the per-device IOVA and paging structure. The virtual
virtual address requested by the device is translated to the
physical address through the set of paging structures by
IOMMU. In the case of VM, the address space of assigned device
is embedded within the guest address space. The IOMMU is
programmed to translate such IOVA to the host physical address
which is mapped to the guest address space. Such translation
is both realized and protected by the IOMMU.

Third, the VFIO has a mechanism to describe and register the
device interrupt to signal its userspace driver. When the VM
accesses the device configuration space, it is trapped through
QEMU. QEMU configures the IRQs by the VFIO interrupt ioctls
and sets up the event notifiers between the kernel, QEMU and
guest. When the kernel signals the IRQ to QEMU, QEMU injects
it into the VM. The interrupt signaling is further speeded up
by moving QEMU out of the way. KVM supports both ioeventfd and
irqfd. ioeventfd registers PIO and MMIO regions to trigger an
event notification, when written by the VM. irqfd allows to
inject a specific interrupt to the VM by KVM. Once ioeventfd
and irqfd are coupled together, the interrupt pathway remains
in the host kernel without exiting to the userspace QEMU.
Using the VT-d, the KVM and QEMU is completed removed from the
signaling path way. It enables the direct interrupt delivery
from the assigned device to its VM without a VM exit.
}


\subsubsection{Posted Interrupts}

{\bf Conventional Interrupt Delivery:}
\mycomment{
Our design uses the hardware-assisted posted interrupt
mechanism and achieves the direct interrupt delivery of
assigned devices and local timers without the intervention of
hypervisor. It reduces the hypervisor CPU utilization and
dedicates the CPU time to the guest. Under the normal
circumstances, the guest can not handle physical interrupts
without the hypervisor. 
}
In the conventional architecture, when an interrupt is delivered to a CPU core,
it triggers a VM exit if the CPU core is not in the root mode and control is transferred to the hypervisor. 
%Saving and loading the execution context between the root and
%non-root mode waste the CPU cycles. Second, the host needs to
Then the hypervisor examines the cause of the interrupt. 
If the interrupt is meant for a VM scheduled on the CPU core,
the hypervisor delivers this interrupt as a virtual interrupt to the target VM
next time when it is scheduled on the CPU core; otherwise the hypervisor handles
the interrupt on its own.
After a VM completes servicing an interrupt, it writes to the EOI (End of Interrupt)  register 
to signal to the hardware that the interrupt in question has been handled. Because the EOI register is 
a privileged resource, every EOI register write causes a VM exit.
Therefore, processing of each physical interrupt costs at least two VM exits.
This per-interrupt overhead is too expensive for network-intensive applications that 
are designed to process multi-million packets per second. 

\mycomment{
and handle the physical interrupt. If the physical
interrupt is meant for the guest, the host needs to deliver it
as the virtual interrupt upon the next VM entry. Otherwise,
the host handles it and schedules the next VM entry. Third,
when the guest's interrupt handler finishes, it writes to the
EOI register. Such write operation may induce the VM exit.
Since the guest is not aware of the distinction between the
physical and virtual interrupt, it signals the completion of
interrupt in the same way. After the guest handles the virtual
interrupt, its EOI update is normally emulated by the host.
Fourth, the host may need to use CPU cache and reduce the time
to handle the physical interrupt. This introduces the CPU
cache pollution.
}

%In the X86 architecture, 
A VM may experience two types of interrupts: {\em external} and {\em local} interrupts.
External interrupts originate from external I/O devices, such as network card or disk controller.
When these I/O devices generate a hardware interrupt, this signal first goes to an IOAPIC, which, through an {\em Interrupt Redirection Table}, 
converts the hardware interrupt into an interrupt message that contains a vector number and is sent to a particular CPU core.
There is a local APIC associated with each CPU core to field interrupts sent to the CPU core.
In the Intel VT-d architecture, all interrupts sent to any local APIC are intercepted by  an {\em Interrupt Remapping Table} in the IOMMU unit, 
which provides a similar functionality to that of an Interrupt Redirection Table
to those external interrupts that do not come from an IOAPIC, e.g., message signaled interrupts from PCI devices.

{\bf Posted Interrupt Delivery:}
The Posted Interrupt mechanism~\cite{intelvtd-paper,intelvtd-manual} allows a CPU core that is running in the non-root mode to receive and handle interrupts with a specific vector
(Post Interrupt Notification or PIN vector) directly without involving the hypervisor.
When a CPU core handles a PIN interrupt, it examines a bitmap data structure associated with the CPU core called {\em Virtual Interrupt Request Register} (vIRR),
which is copied from another privileged data structure also associated with the CPU core called {\em Posted Interrupt Requests} (PIR) as a side effect of a PIN interrupt delivery,  
to determine the vectors of the interrupts behind the PIN interrupt, and processes these interrupts one by one according to their vectors and priorities.
The PIR bitmap is part of a per-VCPU data structure called {\em Posted Interrupt Descriptor} (PID), which in addition contains an Outstanding Notification (ON) flag, which, when set, indicates that there is a PIN interrupt pending.  
The address of the PID and the PIN vector associated with a CPU core are both contained in the virtual machine control structure (VMCS) associated with the VCPU running on the CPU core.

%\subsubsection{Direct External Interrupt Delivery}
To deliver an external interrupt directly to a VM, KVM sets up an Interrupt Remapping Table (IRT) entry for that external interrupt as follows.
First , it sets the IRT entry's IM bit to 1, which means that any interrupt matching this entry is to be delivered via the posted interrupt mechanism.  
Then, it sets the IRT entry's PID address field to the PID address associated with this entry's target CPU core. 
When an external interrupt arrives at the IOMMU and matches an IRT entry, the 
hardware first locates the PIR bitmap in the target CPU core's PID, then sets the bit in the PIR bitmap corresponding to the external interrupt's associated vector,
converts this interrupt into an interrupt labelled with the PIN vector, and finally delivers this PIN interrupt to the target CPU core's local APIC.
Because the external interrupts ``pretend'' to be a PIN interrupt, the target CPU core processes them directly without causing any VM exit.

When a VM completes the service of an interrupt, it needs to clear the EOI register to indicate to the 
associated local APIC that it is done with the interrupt. Because the EOI register is a privileged resource, accessing the EOI register
would normally requires a VM exit, which would add to the interrupt processing overhead. 
Hence, by default, when configuring a VM, KVM disables all EOI-triggered VM exits by setting the corresponding fields in the VM's VMCS.
%To allow a VM to avoid a VM exit due to clearing the EOI register after handling an interrupt of vector X,  KVM clears X's corresponding bit in 
%the {\em EOI exit bitmap} in the VM's VMCS. As a result, at run time after this VM completes servicing an interrupt of vector X and clears the EOI register via a memory mapped 
%intreface (called APIC-access page), no VM exit occurs.   
%Helped with the posted interrupt and EOI virtualization mechanism, a HaaS VM is now able to process any external interrupt directly without triggering any VM exit.

 
\mycomment{
VT-d supports the posted-interrupt capability and deliver the
external interrupts directly from the I/O devices and external
controllers without the cost of VM exits and the hypervisor
intervention. Before utilizing such feature, the system
software needs to define the posted interrupt notification
vector. The PIN signifies the incoming external interrupt from
the assigned device is subjected to the posted-interrupt
processing. The processing is achieved by updating the
posted-interrupt descriptor dynamically. When the VMCS is
actively used by the logical processor in the non-root mode,
it is prohibited to update its data structures. The PID is the
exception. Nonetheless, there is one requirement that the PID
modifications must be done using locked read-modify-write
instructions. Here is another benefit of posted-interrupt
support. When the virtual processor is scheduled on another
VCPU, the VMM can co-migrate its interrupts from the assigned
devices by setting the corresponding bits in posted-interrupt
register of PID.

The posted-interrupt support is accomplished in three general
steps. First, the VMM programs the interrupt-remapping
hardware with the mapping between the external interrupt and
virtual interrupt. Second, when the external interrupt is
delivered to the interrupt-remapping hardware, it sets the
outstanding bit and corresponding bit of virtual interrupt in
the posted-interrupt register of PID. It generates the PIN.
The IOAPIC delivers the PIN to the appropriate LAPIC. Third,
PIN notifies the logical processor that it is the
posted-interrupt event. The logical processor starts the
posted interrupt processing and delivers the virtual interrupt
without any VM exit.

The posted-interrupt processing is described in the following
steps. First, when the external interrupt is delivered to the
guest's processor, it is acknowledged by the LAPIC. LAPIC
provides the processor core the interrupt number. Second, if
the physical interrupt is equal to the PIN, the logical
processor starts the posted interrupt processing. Third, the
processor clears the outstanding notification bit from the
posted-interrupt descriptor. Fourth, the processor
acknowledges the EOI. Fifth, the processor updates the vIRR by
synchronizing it with the PIR. Sixth, the processor acquires
the next request virtual interrupt. It updates RVI by the
maximum of previous RVI and highest index of bits set in PIR,
before it clears PIR. Seventh, the processor evaluates the
pending virtual interrupt. Eighth, the processor delivers the
virtual interrupt.
}


\subsection{Achieving Bare-metal I/O Performance}
%\figw{virtualization_overhead}{9}{Virtualization overheads in I/O operations due to VM exits in (a) para-virtualized I/O, (b) passthrough I/O without posted interrupt, and (c) optimized passthrough I/O in \sna.}
%TODO: Describe I/O challenges here
%	Why CPU utilization increases
% 	VM Exit overhead
While using the VT-d and posted-interrupt mechanisms removes 
the hypervisor of from I/O data plane path of the VM, these alone are
not sufficient to reduce hypervisor overheads. 
To ensure that a VM using VT-d and VFIO can achieve bare-metal 
I/O performance and with least overheads, \na implements 
the following overhead reduction mechanisms

{\bf Dedicated CPUs:}
First, the hypervisor is assigned one dedicated physical CPU core
(CPU 0 in our implementation)
where it executes all of VM management operations. 
The HaaS VM is assigned all the remaining physical 
CPU cores by pinning the guest VCPUs to 
dedicated physical cores one-to-one~\cite{amit:2015}. 
This prevents the hypervisor's threads 
from competing with the VM's VCPU threads and 
also reduces the need to re-route interrupts
from the VM's assigned devices.
To avoid unnecessary VM exits on the HaaS VM's CPUs,
all external interrupts which are not directly handled
by the HaaS VM, are delivered to the hypervisor's physical 
CPU by the IOAPIC.

{\bf Disabling  HLT-triggered VM exits:}
The second optimization relates to reducing hypervisor's CPU utilization
under the following scenario.
When handling frequent I/O operations, 
a guest VCPU  may become idle for brief intervals, and 
run a special idle thread.
On X86, this idle thread consists of a loop of {\tt HLT} instructions, 
each of which is {\em intended} to place the CPU in the C1 energy-saving mode 
until an external interrupt occurs.
However, since {\tt HLT} is a privileged instruction, it triggers a 
VM exit when executed by the guest.
To emulate the {\tt HLT} instruction, currently, the KVM hypervisor
polls (busy waits) for external events for a short time 
(adaptively between 0-200ns). If no events occur during this 
polling period, the guest VCPU is blocked and the physical CPU 
is placed in the low-power state.

The problem we observe is that, when the VM sends or receives network packets
at a high rate, the hypervisor-level CPU utilization increases
dramatically. This is because frequent {\tt HLT}-triggered VM exits
result in frequent invocation of the event polling loop in the hypervisor. 
Often, the physical CPU re-enters the guest mode directly 
from the polling loop as soon as the next network packet 
or interrupt arrives for the VM, without ever entering the low-power mode.

Although peak network throughput is not reduced when 
using VFIO (see Table~\ref{tab:network_performance}), 
both VM exits and polling loop cause significant 
CPU utilization by the hypervisor 
(see Table~\ref{tab:cpu_utilization_40gbps}).
Ironically, this overhead occurs even when using passthrough I/O and 
posted interrupts which were meant to reduce hypervisor overheads.
Eliminating this event polling alone is insufficient because {\tt HLT}-triggered VM exits 
are also expensive by themselves; the guest VCPU thread is blocked and
must make a trip through the hypervisor's CPU scheduler before 
re-entry into guest mode.

Since \na is designed for a single VM setting, {\em we choose to
disable {\tt HLT}-triggred exits altogether}
by modifying the corresponding execution control fields in the VM's VMCS.
As a result, whenever a CPU becomes idle, no {\tt HLT}-triggered
VM exits occur, the CPU lowers its clock frequency while in guest mode, 
the total energy consumption is reduced, and importantly, 
hypervisor code is not invoked in I/O data path. 

Another interesting side effect of disabling {\tt HLT}-triggered VM exits
is that the HaaS VM is observed to receive more interrupts
from its direct-assigned device, possibly due to 
faster EOI response and hence less interrupt coalescing by 
the hardware, similar to the behavior on a bare-metal OS.

%
%First, the direct assigned network card and disk drive under
%the VFIO framework removes the host from the forward I/O path.
%%The guest has the control over the assigned devices and avoids
%the virtualization overhead, when accessing the device control
%registers and performing the
%DMA~\cite{sdm:2018,intelvtd-manual,williamson:2016}. After the
%assigned device services the request, it delivers the
%interrupt to the guest and triggers the VM exits due to the
%external interrupts and EOI respectively. Using the VT-d
%posted-interrupt and VT-x APIC virtualization~\cite{postedinterrupt},
%the guest handles the device interrupt and updates the EOI
%without any VM exit. Thus, the host is completed removed from
%the guest I/O path for the passthrough devices. Nonetheless,
%


\subsection{Direct Local Interrupt Delivery} 
\label{subsubsec:shared_pid_dtid}
\figw{dtid}{9}{Direct delivery of timer interrupts in \sna. 
%TODO: mention disabling WRMSR VM Exits.
The hypervisor configures each guest LAPIC to transform the local timer 
interrupt into a PIN interrupt and allows guest OS to directly update the PID page.}

Unlike an external interrupt, a local interrupt is delivered to a CPU core's local APIC without going through the IOMMU's Interrupt Remapping Table.
Therefore, such local interrupts cannot be delivered to a VM via the posted interrupt mechanism.
In this section, we describe how to achieve direct delivery of two types of local interrupts, {\em timer interrupt} and {\em inter-processor interrupt} (IPI).

{\bf Timer Interrupts:} To enable a timer interrupt to be delivered directly to a HaaS VM running on a CPU core,
\na first leverages the {\em local vector table} in the local APIC to turn each timer interrupt into a PIN interrupt, and 
then programmatically sets a particular bit in the CPU core's PIR bitmap to indicate to the target VM that underlying the PIN interrupt is a timer interrupt.

If the PIR bit associated with a timer interrupt is set way ahead of the interrupt's next expiration time, 
the interrupt's target VM may receive spurious timer interrupts. 
For example, suppose the next expiration time of a timer interrupt is $T_1$, and a NIC interrupt is delivered via the posted interrupt mechanism to the target VM at $T_2$, where ${T_2} < {T_1}$.
When the target  VM processes the PIN interrupt triggered by the NIC interrupt, it finds in the vIRR bitmap that two bits are turned on, one for the timer interrupt and the other for the NIC
interrupt, {\em even though the timer interrupt is spurious} because the real local timer has not yet expired.

To solve this spurious timer interrupt problem, a HaaS VM's guest OS is modified to keep track of the next expiration time of every timer interrupt, ignore an ostensible timer interrupt when  
the current time is substantially smaller than the next expiration time, 
and set the timer interrupt bit in the PIR because the PIR bitmap is cleared 
after a PIN interrupt is delivered.
The last step is required to ensure that a timer interrupt that is considered spurious and thus ignored because of an external interrupt,  still has a chance to be delivered directly when the timer truly expires.  

To avoid VM exit when a HaaS VM modifies the PIR of the CPU core it runs on, 
\na allocates a separate page to house the PIR and makes the page accessible to the VM.
With this set-up, a HaaS VM can write to the PIR of the CPU core it runs on without triggering any VM exit.

After a timer interrupt handler services a timer interrupt, it may need to configure the next timer expiry by updating the initial counter (TMICT) register of the local APIC timer, 
which would normally cause a VM exit. To avoid this VM exit, \na leverages VT-d's hardware-assisted APIC virtualization by properly configuring the MSR bitmap in the associated VMCS. 
As a result, the KVM's  intercept of any TMICT MSR update is disabled. When a VM writes to the TMICT, the change is written to the associated register directly without triggering any VM exit.     

Finally, from a security viewpoint, control over the timer interrupt is considered important for a hypervisor or OS to maintain control over all physical CPUs.
In \na, although the HaaS VM can directly control the timer hardware on its assigned CPUs, the hypervisor can still regain 
control over the CPUs when needed, such as before live migration. 
The hypervisor always controls the CPU0 and its local timer. 
To regain control over other physical CPUs assigned to the VM, the hypervisor 
simply disables direct timer access for the VM by reconfiguring the IOMMU and VMCS to 
disable PIN and trigger VM exits for timer interrupts; the hypervisor then delivers
emulated virtual interrupts to the guest.

Combining all the above techniques, \na is able to deliver local timer interrupts 
directly to a HaaS VM and have them properly serviced, 
all without causing any VM exits and while maintaining control over all physical CPUs.


{\bf Inter-processor Interrupts (IPI):} 
Just as with direct delivery of timer interrupts, the posted interrupt mechanism can be used for direct delivery of inter-core IPIs.
Here we describe the design of direct IPI delivery mechanism which is currently under development in our prototype.
When a source CPU core sends an IPI to a destination CPU core, the source CPU core configures the Interrupt Command Register (ICR) in its local APIC, 
and, after the low double-word of the ICR is written to, triggers an interrupt message to be sent to the destination CPU core's local APIC through an inter-APIC system bus.
To enable an IPI to be delivered directly to the HaaS VM, \na configures the source CPU core's ICR so that the resulting interrupt message carries a PIN vector,
and then sets a certain bit in the PIR of the destination CPU core to indicate that the interrupt behind the PIN interrupt is an IPI.

 



\mycomment{

The longer the guest stays on its CPU, the more local timer
interrupts it receives. The goal is to let the guest have its
dedicated CPUs. Our design does not only directly deliver the
timer interrupts to the guest, but also take one step further
by allowing the guest update its next timer event directly.

The local timer interrupt is delivered to the guest and
results in two scenarios. First, the timer interrupt is meant
for the guest. It induces the VM exit and the control is
transferred back to the host. The host handles the timer
interrupt and injects the virtual timer interrupt to the
guest. When the guest receives the virtual timer interrupt, it
services the timer interrupt and set up the next timer event
by updating the LAPIC timer initial count register through the
x2APIC interface. This triggers the MSR-write VM exit and the
control is transfer to the host. The host helps the guest to
set up its next timer by registering the \texttt{hrtimer}
object of guest next timer event. Second, the timer interrupt
is not meant for the guest. It induces the VM exit and
transfer the control back to the host. The host processes the
timer interrupt but does not inject the virtual timer
interrupt. Nonetheless, if the timer interrupt is not meant
for the guest, the guest should not pay the price.

The first task is to transform the local timer interrupt into
the posted interrupt, which is directly delivered to the guest
by the VT-d hardware. It requires two actions. The
timer-interrupt bit of posted-interrupt request needs to be
set, before the posted-interrupt notification is delivered to
the guest core. Since the guest is responsible for its own
timer interrupt, the guest should set the bit in the PIR.
However, such a PIR structure is embedded in the
posted-interrupt descriptor and protected by the host. The
host needs to share the PIR with the guest by isolating the
entire PID to a shared page. If the guest messes up setting
the proper bits in the PID through the EPT, it does not affect
the host normal operations. In our design, the shared PID page
is accessible by three entities: host, guest and
virtualization hardware. The second task is to allow the guest
control the timer initial count register of LAPIC timer. With
the hardware-assisted APIC virtualization, this is achieved by
updating the MSR bitmap of VM control structure. The KVM
intercept of TMICT MSR update is disabled. When the guest
configures the TMICT, the change is written to the register
directly without triggering the VM exit. The third task is to
configure the LAPIC timer chip to deliver the posted-interrupt
notification instead of the actual timer interrupt. In
summary, we reach our goal of guest having dedicated CPUs by
disabling the HLT- and timer-related VM exits.

Using the shared PID has the draw back. It induces the
spurious timer interrupts causing additional interrupt
processing in the guest. Since the guest sets the PIR
timer-interrupt bit before its next timer event, the it
induces the spurious timer interrupts. Such a fake timer
interrupt is induced in two cases. First, the guest
experiences the spurious timer interrupt when performing
I/O-bound activities with the assigned device. Let's take the
assigned network card for an example. Both the bits of timer
interrupt and network-device interrupt are set in the PIR.
Based on the Intel architecture, the timer interrupt has a
higher priority than the network interrupt does. The timer
interrupt is delivered before the network interrupt. Although
the guest should have only processed the network interrupt, it
first processes the timer and then network interrupt. Second,
upon the VM entry, the PIR is synced to the virtual
interrupt-request register because of the KVM implementation.
One of time points to evaluate the virtual interrupt delivery
is at the VM entry time. If the PIR timer-interrupt bit is
present during the copy, the fake virtual timer interrupt is
delivered into the guest after the VM entry. If the arrival of
virtual timer interrupt is earlier than the expected
expiration, the guest ignores it and processes the next
interrupt. Thus, the CPU overhead is reduced in comparison
with the full timer interrupt processing. 

}
% Currently, 3.4 is under the review.
\subsection{Seamless Device State Migration}

%The VM on an \na server is special because it is the only VM on the server and it 
%interacts directly with PCIe devices and local timers.
%We call such a VM a HaaS VM.
Because the states associated with directly accessible devices complicate the migration process~\cite{zhai:2008}, 
\na augments KVM's VM migration capability with additional mechanisms to support seamless migration of HaaS VMs. 
More specifically, \na adopts the following unified strategy to hide the states of directly accessed devices from KVM's VM migration logic:
In the normal mode, a HaaS VM directly accesses PCIe devices and timers; immediately before and during when the HaaS VM is migrated,  the VM 
accesses PCIe devices and timers indirectly; after the migration, the migrated HaaS VM accesses PCIe devices and timers directly again.

Moreover, \na assumes that the source and destination physical servers involved in a migration have an identical
set of hardware devices with which a HaaS VM interacts directly.
This way, the driver code directly interacting with these devices has a chance to properly work on 
both the source and destination server.
In this section, we focus on two types of directly accessed devices: NIC and timer.

\mycomment{
The direct device assignment makes it difficult to migrate the
guest to its destination~\cite{zhai:2008}. After the VM
migration, it is possible that the previously-assigned devices
may not be available at the destination. Even if the assigned
device is available, the internal state of device may not be
readable or still on its way to the destination. The host at
the destination has a hard time to passthrough the device
without the device-specific knowledge. Moreover, some devices
have the unique hardware information that cannot be
transferred, such as the MAC address of network interface
card. In the case of guest-controlled timer, it depends on the
VT-x availability at the destination. Our design takes the
approach of alternating the usage of passthrough and
respective virtual device with the acceptable service downtime
or number of missed time interrupts. We assume that the
devices and hardware supports are available. For the network
activity, the network traffic is switch from the assigned to
virtual network device, before the migration starts. The
network traffic is switched back after the guest starts up at
the destination. For the local timer interrupt, the direct
timer interrupt delivery is switched back to the indirect
delivery with the help of \texttt{hrtimer} object and TMICT
WRMSR VM exit is enabled, before the migration. The changes
are reverted after the guest starts up at the destination.
}

\subsubsection{NIC State Migration}
\figw{nic_bonding}{9}{Bonding of passthrough and para-virtual NICS in a HaaS VM to support live migration in \na.}

We assume each HaaS physical server is equipped with an SR-IOV Ethernet NIC~\cite{dong:2008},
which provides one physical function and multiple virtual functions, each with its own MAC address.
As shown in Figure~\ref{fig:nic_bonding}, 
\na sets up a directly accessed passthrough NIC using one of the virtual functions
and an indirectly accessed para-virtual NIC using another virtual function.
A NIC bonding driver~\cite{bond-dri} in the guest OS teams up the passthrough and para-virtual NICs in an active-backup mode.
The HaaS VM running on every \na physical server sends and receives network packets through such a bonded interface. 

During normal execution, the passthrough NIC is the Active slave and the para-virtual NIC is the Backup slave,
so that a HaaS VM can make full use of the underlying physical NIC's capability.
Before a HaaS VM is to be migrated, \na hot-unplugs the passthrough NIC so as to fail the current Active slave.
Upon detecting the Active slave's failure, the bonding driver immediately switches network traffic to the Backup slave.
% to be the Active slave.
From this point on, all network traffic goes through the para-virtual NIC. 
KVM captures the state of the para-virtual NIC, which is easier to migrate than the state of a passthrough NIC.
Then \na kicks off a VM migration transaction for the HaaS VM, which includes the transfer its NIC-related state.
%During the migration, \na . 
After the migration transaction is completed, the HaaS VM is resumed on the destination server and still continues to use the para-virtual NIC.
In the mean time, \na hot-plugs the passthrough NIC on the destination  server 
%to make it the Backup slave, and then hot-unplugs the virtual NIC so that the current Backup slave or 
so that the passthrough NIC becomes the new Active slave.


\mycomment{
In this section we describe the mechanism of migrating a guest
with direct NIC assignment. As shown in Figure
\ref{fig:nic_bonding} the guest is configured with virtio
network device backed by the vhost driver and a passthrough
network interface card~\cite{zhai:2008}. Using SR-IOV
~\cite{dong:2008} the physical NIC is presented as virtual NIC
through virtual functions. For the purpose of simplicity, we
assume that the guest has one assigned network device. The
prototype overcomes the challenge of migrating NIC assigned
guest by the following strategy. It uses the Ethernet bonding
driver to direct the network traffic between the assigned and
virtual NIC. The migration procedure is divided into two
parts. During regular operation of guest, the assigned NIC is
used for higher network bandwidth. Before the migration, the
host uses the bonding driver and shifts the network traffic
from the assigned NIC to the virtual NIC. The source host
takes the control of the assigned NIC through hot unplug event
of the assigned NIC and starts the migration. After the guest
resumes at the destination, the destination host transfers the
control back to the guest using hot plug event of the assigned
NIC and switches the network traffic from virtual NIC to
assigned NIC.

Linux provides bonding driver to present multiple network
interfaces into a single logical interface. The modes of
bonding driver define the behavior of the bonded interfaces.
To maintain higher network performance during regular
operation of guest, we configure the bonding driver in
active-backup mode where the assigned NIC is chosen as the
active interface and the virtual NIC as a backup-slave
interface. In active-backup mode only one of the interfaces is
active at any time. When the active interface fails, one of
the slave interfaces becomes active. The bonding driver always
takes the MAC of the active interface. On failure of the
active interface, the bonding driver takes the MAC address of
the next to be slave interface. The change in MAC address is
notified by broadcasting ARP packets to avoid the network
packets loss in guest. Before the migration is initiated, on
hot unplug command, the outgoing network traffic is redirected
to the virtual NIC interface by the bonding driver and the
incoming traffic is shifted to the virtual NIC by broadcasting
ARP packets. Once the assigned NIC is hot unplugged, QEMU
issues migrate command. After the migration is completed, once
the guest resumes on the destination, the NIC device is hot
plugged.
}

The most important performance metric for VM migration is the service disruption time.
The additional service disruption time that \na introduces is attributed to the transition from
the passthrough NIC to the para-virtual NIC on the source server, and the transition from
the para-virtual NIC to the passthrough NIC on the destination server.
Measurements on a earlier \na prototype suggested that the transition from the 
para-virtual to the passthrough NIC introduces non-trivial service disruption time (about 300ms), 
when hot-plugging the passthrough NIC on the destination server. 

A deeper analysis shows that the hot-plug operation consists of three steps:
(1) QEMU prepares a software object to represent the passthrough
NIC, (2) then It populates this software object with parameter values extracted 
from the PCIe configuration space of the NIC, (3) and finally it
resets the software NIC object to set up the BAR and interrupt forwarding
information. The first and third steps must take place in QEMU's main event
loop when the entire HaaS VM requesting the hot-plug operation must be paused.

To minimize this service disruption time due to hot-plug, 
\na performs the first and second steps of the hot-plugging the passthrough NIC
on the destination server while the HaaS VM is being migrated and before it is resumed, 
and performs the third step after the HaaS VM is migrated and resumed. 
%This brings the service disruption time due to hot-plug to TODO second.


\mycomment{
However, by forcing the migrated HaaS VM resumed on the destination server to continue using the virtual NIC,
the time required to hot-plug the directly accessed NIC on the destination server could be fully masked. 
After \na successfully brings up the directly accessed NIC and makes it the Backup slave,  it fails the virtual NIC so as 
to direct all network traffic to the directly accessed NIC.  


We observed that on hot plug event of NIC device on the
destination host, the network service in guest drops until for
0.3seconds. Further, we investigate the reason for the network
packet loss in the guest. The hot plug mechanism of assigned
NIC consists of the following three steps. First, QEMU
prepares a software object that represents the passthrough
NIC. It then realizes the QEMU software object by getting a
copy of configuration space from the NIC device. Finally, it
resets the software NIC object and setup the BAR and interrupt
forwarding. The first and last step happen in QEMU main event
loop during which the guest remains paused. As a result, the
guest experiences downtime during hot plug operation. In \na,
to mitigate the downtime due to hot plug operation on the
destination host, the first two steps are executed during
migration. During the first phase of pre-copy live
migration~\cite{clark:2005,postcopy-osr}, all the memory pages are
transferred to the destination over the network. The dirty
memory pages are then transferred in multiple iterative
rounds. The VCPU and I/O state of guest are transferred in the
final phase to resume the guest on destination. Step one and
two are executed on the destination host during migration.
Consequently, the network service in guest does not get
affected as it runs on the source host. QEMU allows to setup
and realize the software object during migration. However, the
BAR and interrupts can be setup only after resuming the guest.
Hence, we eliminate the downtime caused during the setup of
software NIC object phase.
}



\subsubsection{Timer State Migration}

To leverage the posted interrupt mechanism to directly deliver local APIC timer interrupts,
\na allows a HaaS VM running on a CPU core to directly access the following device state:
(1) setting the timer interrupt bit in the PIR associated with the CPU core's local APIC ,  (2) 
using a WRMSR to modify the initial counter register of the local APIC timer, and (3) 
computing the next timer expiration time from conversions between clock cycles and nano-seconds, the multiplication
and shift factor of the calibrated timer, etc., which are provided by the hypervisor.

During the normal run time, timer interrupts are delivered to a HaaS VM directly. 
Before a HaaS VM is to be migrated, \na notifies the HaaS VM to stop the 
direct timer interrupt delivery (DTID) mechanism, unmaps the PIR page, 
enables the TMICT WRMSR VM exit, configures the local vector table in the local APIC to 
fire timer interrupts as they are rather than as posted-interrupt notification interrupts. 
Upon receiving this notification, the HaaS VM uses a different set of 
multiplication and shift parameters to compute the next timer value, 
and convey the resulting value to the hypervisor via the \texttt{hrtimer} object
when control is transferred to the hypervisor upon a TMICT WRMSR VM exit. 


After a HaaS VM is successfully migrated and resumed, both the hypervisor and the VM restart 
the DTID mechanism, by running 
the aforementioned steps in reverse order.


\mycomment{   
The direct timer interrupt delivery depends the following
factors. First, the host shares the posted-interrupt
descriptor with the guest. When the guest updates the
timer-interrupt bit of posted-interrupt request through EPT,
it does not trigger the EPT violations and the local timer
interrupt is delivered as the posted interrupt. Second, the
guest directly configures the timer initial count register
without a WRMSR VM exit. This is achieved by updating the MSR
bitmap of VM control structure. Third, the guest needs to
correctly compute its next timer events with the
host-calibrated LAPIC timer. To compute the next timer event
from the nano-seconds to the clock cycles, the multiplication
and shift factor of the calibrated timer are required. The
host needs to convey such information to the guest. The guest
configures the TMICT with the correct timer event in clock
cycles.

Before the migration starts, both host and guest need to tear
down the DTID. In the host, it
} 

\mycomment{
It does not only support the direct PCI device
assignment to the userspace processes of VM, but also the
platform devices. Why do we need to allow the userspace
programs to gain the control of physical devices? For the
field of high performance computing, the I/O performances has
a great impact on the overall system performance. The
performance congestion comes, When the rate of data being read
is slower than the rate of data being consumed. Or it happens,
when the rate of data being written is slower than the rate of
data being computed and produced.

The VFIO needs to fulfill the three requirements of device
assignment to a userspace process. First, the userspace driver
can access to the device resources such as I/O ports. Second,
the userspace driver can perform the DMA securely. This is
provided by the IOMMU protection mechanism. Third, the device
interrupts is delivered to the device owner in the userspace.
The way how the VFIO fulfills the three requirements and
applies them with QEMU is briefly described below.
}

\mycomment{
The hardware-assisted direct device assignment helps the guest
achieve the baremetal I/O performance without the additional
virtualization overhead using the VT-x and VT-d support.
After proper VMCS and EPT configuration, the guest gains
the control of assigned device
by MMIO/PIO without the help of hypervisor. The VT-x APIC
virtualization permits the guest to write to the
end-of-interrupt register without a VM exit. With the VT-d
support, the guest performs DMA with the enhanced security and
eliminates the VM exit overhead due to the device interrupts by using 
the posted-interrupt mechanism. In addition to the hardware
support, VFIO provides the software framework for the
userspace device drivers. It works with VT-d and QEMU and sets
up the direct PCI device assignment. Although it is expected
to move the hypervisor out of the guest I/O path, the
hypervisor still induces high CPU utilization due to the
HLT emulation. This greatly deviates our goal of guest having
its own dedicated cores. Nonetheless, our CPU optimization
strategies remedy such a problem.
}


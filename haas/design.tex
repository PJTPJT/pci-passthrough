\section{Virtualization support for HaaS}
% Virtualization support for HaaS include the following items
% CPU idleness detection and processing
% Direct NIC assignment
%   - VFIO
%   - CPU optimization for the baremetal network performance
% Direct interrupt delivery
%   - Posted-interrupt mechanism
%   - Direct timer-interrupt delivery
%     - periodic vs aperiodic timer interrupt
%     - one-shot vs periodic hardware timer interrupt
%     - guest-level access to the PIR page
%     - Eliminate the hardware lock, when the guest accesses
%       the PIR bit
%     - Spurious timer interrupts
% Seamless baremetal virtual machine migration
%   - NIC bonding with the hot plug/unplug operation migration
%   - DTID during the migration

\figw{architecture}{10}{Architecture of \na}
\figw{virtualization_overhead}{10}{Virtualization overhead}

The design goal of \na is to eliminate the hypervisor from
the guest I/O path while achieving bare-metal performance in
the guest. To achieve this \na considers direct device
assignment to the guest using Intel's VT-d support. First, the
guest meets the bare-metal network and disk I/O performance
with direct device assignment. We further apply optimizations
to reduce the CPU utilization by hypervisor. Second, the timer
interrupts are transformed into the posted interrupts, which
are directly delivered to the guest by the logical processor
without causing any VM exits. Finally, we present the
migration of virtual machine with directly assigned devices.

\subsection{CPU Idleness and Processing}
One of our design goals is to let the guest stay on its CPU as
long as it can. We encounter two different types of
virtualization overheads. First, the idle guest issues the
privileged HLT instruction. Such an instruction induces the VM
exit and transfers the control to the KVM which starts to poll
on the CPU until the event arrival. This incurs the high CPU
utilization due to the HLT-related VM exit. It is eliminated
by updating the primary processor-based VM execution control
and disabling the VM exit due to the HLT instruction. It
allows the idle guest to stay on its CPU without polling
and results in the CPU clock frequency remains at minimal.
Thus, disabling HLT-induced VM exit helps to reduce the CPU
power consumption of idle guest. Second, the local timer
interrupt fires and causes the VM exit, when the guest's time
quantum is expired. The longer the guest stays on its CPU, the
higher number of physical timer interrupt it receives. To
support our objective, our work utilizes the posted-interrupt
mechanism and directly deliver the interrupt into the guest
without triggering any VM exit. This feature is discussed in
the~\nameref{subsubsec:shared_pid_dtid}.

\subsection{Direct PCI Device Assignment}
The hardware-assisted direct device assignment helps the guest
achieve the baremetal I/O performance without the additional
virtualization overhead using the VT-x and VT-d support.
After proper VMCS and EPT configuration, the guest gains
the control of assigned device
by MMIO/PIO without the help of hypervisor. The VT-x APIC
virtualization permits the guest to write to the
end-of-interrupt register without a VM exit. With the VT-d
support, the guest performs DMA with the enhanced security and
eliminates the VM exit overhead due to the device interrupts by using 
the posted-interrupt mechanism. In addition to the hardware
support, VFIO provides the software framework for the
userspace device drivers. It works with VT-d and QEMU and sets
up the direct PCI device assignment. Although it is expected
to move the hypervisor out of the guest I/O path, the
hypervisor still induces high CPU utilization due to the
HLT emulation. This greatly deviates our goal of guest having
its own dedicated cores. Nonetheless, our CPU optimization
strategies remedy such a problem.

\subsubsection{VFIO}
Virtual function I/O is a secure framework for the userspace
device drivers. It does not only support the direct PCI device
assignment to the userspace processes of VM, but also the
platform devices. Why do we need to allow the userspace
programs to gain the control of physical devices? For the
field of high performance computing, the I/O performances has
a great impact on the overall system performance. The
performance congestion comes, When the rate of data being read
is slower than the rate of data being consumed. Or it happens,
when the rate of data being written is slower than the rate of
data being computed and produced.

The VFIO needs to fulfill the three requirements of device
assignment to a userspace process. First, the userspace driver
can access to the device resources such as I/O ports. Second,
the userspace driver can perform the DMA securely. This is
provided by the IOMMU protection mechanism. Third, the device
interrupts is delivered to the device owner in the userspace.
The way how the VFIO fulfills the three requirements and
applies them with QEMU is briefly described below.

First, the VFIO exposes the configuration registers of
physical device as the memory regions. Typically, the device
driver communicates with the device registers by the PIO or
MMIO operations to the designated memory address space. VFIO
would like to expose the address space to the user space. The
VFIO retrieves the device information such as BARs from the
PCI configuration spaces and IRQ. It reconstructs them as
different memory regions in a file. The userspace driver uses
the device file descriptor and offset to access each region
and retrieve the device information. The VFIO decomposes the
physical device to a software interface. Such software
interface is turned into the assigned device by QEMU.
Essentially, the device read and write handler in the QEMU
memory API is forwarded to the VFIO read and write handler.
Nonetheless, accessing some parts of PCI configuration
requires the KVM/QEMU emulation. PCI configuration space is
not handled as memory regions in QEMU. Some of the accesses to
the PCI configuration space is passthroughed directly, while
others, such as MSI, BARs, and ROM, need to be emulated.

Second, the VFIO programs the IOMMU to transfer the data
between the userspace driver and device in a hardware
protected manner. The VT-d IOMMU provides the device isolation
using the per-device IOVA and paging structure. The virtual
virtual address requested by the device is translated to the
physical address through the set of paging structures by
IOMMU. In the case of VM, the address space of assigned device
is embedded within the guest address space. The IOMMU is
programmed to translate such IOVA to the host physical address
which is mapped to the guest address space. Such translation
is both realized and protected by the IOMMU.

Third, the VFIO has a mechanism to describe and register the
device interrupt to signal its userspace driver. When the VM
accesses the device configuration space, it is trapped through
QEMU. QEMU configures the IRQs by the VFIO interrupt ioctls
and sets up the event notifiers between the kernel, QEMU and
guest. When the kernel signals the IRQ to QEMU, QEMU injects
it into the VM. The interrupt signaling is further speeded up
by moving QEMU out of the way. KVM supports both ioeventfd and
irqfd. ioeventfd registers PIO and MMIO regions to trigger an
event notification, when written by the VM. irqfd allows to
inject a specific interrupt to the VM by KVM. Once ioeventfd
and irqfd are coupled together, the interrupt pathway remains
in the host kernel without exiting to the userspace QEMU.
Using the VT-d, the KVM and QEMU is completed removed from the
signaling path way. It enables the direct interrupt delivery
from the assigned device to its VM without a VM exit.

\subsubsection{CPU Optimization}
To ensure the guest have the bare-metal I/O performance and
reduced system CPU utilization, our design utilizes the
following optimization.

First, the direct assigned network card and disk drive under
the VFIO framework removes the host from the forward I/O path.
The guest has the control over the assigned devices and avoids
the virtualization overhead, when accessing the device control
registers and performing the
DMA~\cite{sdm:2018,vt-d:2017,williamson:2016}. After the
assigned device services the request, it delivers the
interrupt to the guest and triggers the VM exits due to the
external interrupts and EOI respectively. Using the VT-d
posted-interrupt support and APIC virtualization from VT-x,
the guest handles the device interrupt and update the EOI
without any VM exit. Thus, the host is completed removed from
the guest I/O path for the passthrough devices. Nonetheless,
utilizing the hardware posted-interrupt support is not enough
to reduce the system CPU utilization, especially when the
guest is idle.

Second, the VM exit due to the HLT instruction is disabled.
When the guest is idle, it issues the privileged HLT
instruction and yields the processor for other processes. It
induces the VM exit and transfers the control to KVM. KVM is
busy waiting for the incoming events before blocking the
virtual processor. This induces the high CPU overhead. There
are two solutions. We can either set the polling delay to 0 or
disable the VM exit due to the HLT instruction. After setting
the delay to 0, the host does not wait in the busy loop but
make a trip to the scheduler, which is costly. The latter
solution is chosen to further prevent the host from
intervening how the guest uses the logical processor. It
results in that the idle guest remains on its processor even
when it is idle. There is no VM exit due to the HLT
instruction. One of side effects of disabling the HLT exiting
is that the guest receives the increase number of interrupts
from the assigned device. The virtualization overhead of
direct interrupt delivery are handled by the posted-interrupt
hardware.

Third, the guest has a dedicated set of logical
processors~\cite{amit:2015}. This is done by pinning the
virtual processors to the isolated logical processors in the
one-to-one relationship. Doing so not only prevents the host
processes from competing with the guest's virtual processors,
but also reduces the degrees of interrupt routing from the
assigned devices.

\figw{dtid}{9}{Direct Timer Interrupt Delivery}

\subsection{Direct Interrupt Delivery}
Our design uses the hardware-assisted posted interrupt
mechanism and achieves the direct interrupt delivery of
assigned devices and local timers without the intervention of
hypervisor. It reduces the hypervisor CPU utilization and
dedicates the CPU time to the guest. Under the normal
circumstances, the guest can not handle physical interrupts
without the hypervisor. When a physical interrupt is delivered
to the guest, it induces the following overheads. First, it
triggers the VM exit and transfer the control to the host.
Saving and loading the execution context between the root and
non-root mode waste the CPU cycles. Second, the host needs to
examine and handle the physical interrupt. If the physical
interrupt is meant for the guest, the host needs to deliver it
as the virtual interrupt upon the next VM entry. Otherwise,
the host handles it and schedules the next VM entry. Third,
when the guest's interrupt handler finishes, it writes to the
EOI register. Such write operation may induce the VM exit.
Since the guest is not aware of the distinction between the
physical and virtual interrupt, it signals the completion of
interrupt in the same way. After the guest handles the virtual
interrupt, its EOI update is normally emulated by the host.
Fourth, the host may need to use CPU cache and reduce the time
to handle the physical interrupt. This introduces the CPU
cache pollution.

\subsubsection{Posted-Interrupt Mechanism}
VT-d supports the posted-interrupt capability and deliver the
external interrupts directly from the I/O devices and external
controllers without the cost of VM exits and the hypervisor
intervention. Before utilizing such feature, the system
software needs to define the posted interrupt notification
vector. The PIN signifies the incoming external interrupt from
the assigned device is subjected to the posted-interrupt
processing. The processing is achieved by updating the
posted-interrupt descriptor dynamically. When the VMCS is
actively used by the logical processor in the non-root mode,
it is prohibited to update its data structures. The PID is the
exception. Nonetheless, there is one requirement that the PID
modifications must be done using locked read-modify-write
instructions. Here is another benefit of posted-interrupt
support. When the virtual processor is scheduled on another
vCPU, the VMM can co-migrate its interrupts from the assigned
devices by setting the corresponding bits in posted-interrupt
register of PID.

The posted-interrupt support is accomplished in three general
steps. First, the VMM programs the interrupt-remapping
hardware with the mapping between the external interrupt and
virtual interrupt. Second, when the external interrupt is
delivered to the interrupt-remapping hardware, it sets the
outstanding bit and corresponding bit of virtual interrupt in
the posted-interrupt register of PID. It generates the PIN.
The IOAPIC delivers the PIN to the appropriate LAPIC. Third,
PIN notifies the logical processor that it is the
posted-interrupt event. The logical processor starts the
posted interrupt processing and delivers the virtual interrupt
without any VM exit.

The posted-interrupt processing is described in the following
steps. First, when the external interrupt is delivered to the
guest's processor, it is acknowledged by the LAPIC. LAPIC
provides the processor core the interrupt number. Second, if
the physical interrupt is equal to the PIN, the logical
processor starts the posted interrupt processing. Third, the
processor clears the outstanding notification bit from the
posted-interrupt descriptor. Fourth, the processor
acknowledges the EOI. Fifth, the processor updates the vIRR by
synchronizing it with the PIR. Sixth, the processor acquires
the next request virtual interrupt. It updates RVI by the
maximum of previous RVI and highest index of bits set in PIR,
before it clears PIR. Seventh, the processor evaluates the
pending virtual interrupt. Eighth, the processor delivers the
virtual interrupt.

\subsubsection{Shared-PID DTID} \label{subsubsec:shared_pid_dtid}
The longer the guest stays on its CPU, the more local timer
interrupts it receives. The goal is to let the guest have its
dedicated CPUs. Our design does not only directly deliver the
timer interrupts to the guest, but also take one step further
by allowing the guest update its next timer event directly.

The local timer interrupt is delivered to the guest and
results in two scenarios. First, the timer interrupt is meant
for the guest. It induces the VM exit and the control is
transferred back to the host. The host handles the timer
interrupt and injects the virtual timer interrupt to the
guest. When the guest receives the virtual timer interrupt, it
services the timer interrupt and set up the next timer event
by updating the LAPIC timer initial count register through the
x2APIC interface. This triggers the MSR-write VM exit and the
control is transfer to the host. The host helps the guest to
set up its next timer by registering the \texttt{hrtimer}
object of guest next timer event. Second, the timer interrupt
is not meant for the guest. It induces the VM exit and
transfer the control back to the host. The host processes the
timer interrupt but does not inject the virtual timer
interrupt. Nonetheless, if the timer interrupt is not meant
for the guest, the guest should not pay the price.

The first task is to transform the local timer interrupt into
the posted interrupt, which is directly delivered to the guest
by the VT-d hardware. It requires two actions. The
timer-interrupt bit of posted-interrupt request needs to be
set, before the posted-interrupt notification is delivered to
the guest core. Since the guest is responsible for its own
timer interrupt, the guest should set the bit in the PIR.
However, such a PIR structure is embedded in the
posted-interrupt descriptor and protected by the host. The
host needs to share the PIR with the guest by isolating the
entire PID to a shared page. If the guest messes up setting
the proper bits in the PID through the EPT, it does not affect
the host normal operations. In our design, the shared PID page
is accessible by three entities: host, guest and
virtualization hardware. The second task is to allow the guest
control the timer initial count register of LAPIC timer. With
the hardware-assisted APIC virtualization, this is achieved by
updating the MSR bitmap of VM control structure. The KVM
intercept of TMICT MSR update is disabled. When the guest
configures the TMICT, the change is written to the register
directly without triggering the VM exit. The third task is to
configure the LAPIC timer chip to deliver the posted-interrupt
notification instead of the actual timer interrupt. In
summary, we reach our goal of guest having dedicated CPUs by
disabling the HLT- and timer-related VM exits.

Using the shared PID has the draw back. It induces the
spurious timer interrupts causing additional interrupt
processing in the guest. Since the guest sets the PIR
timer-interrupt bit before its next timer event, the it
induces the spurious timer interrupts. Such a fake timer
interrupt is induced in two cases. First, the guest
experiences the spurious timer interrupt when performing
I/O-bound activities with the assigned device. Let's take the
assigned network card for an example. Both the bits of timer
interrupt and network-device interrupt are set in the PIR.
Based on the Intel architecture, the timer interrupt has a
higher priority than the network interrupt does. The timer
interrupt is delivered before the network interrupt. Although
the guest should have only processed the network interrupt, it
first processes the timer and then network interrupt. Second,
upon the VM entry, the PIR is synced to the virtual
interrupt-request register because of the KVM implementation.
One of time points to evaluate the virtual interrupt delivery
is at the VM entry time. If the PIR timer-interrupt bit is
present during the copy, the fake virtual timer interrupt is
delivered into the guest after the VM entry. If the arrival of
virtual timer interrupt is earlier than the expected
expiration, the guest ignores it and processes the next
interrupt. Thus, the CPU overhead is reduced in comparison
with the full timer interrupt processing. 

\figh{nic_bonding}{6}{NIC bonding}

\subsection{Seamless VM Migration}
The direct device assignment makes it difficult to migrate the
guest to its destination~\cite{zhai:2008}. After the VM
migration, it is possible that the previously-assigned devices
may not be available at the destination. Even if the assigned
device is available, the internal state of device may not be
readable or still on its way to the destination. The host at
the destination has a hard time to passthrough the device
without the device-specific knowledge. Moreover, some devices
have the unique hardware information that cannot be
transferred, such as the MAC address of network interface
card. In the case of guest-controlled timer, it depends on the
VT-x availability at the destination. Our design takes the
approach of alternating the usage of passthrough and
respective virtual device with the acceptable service downtime
or number of missed time interrupts. We assume that the
devices and hardware supports are available. For the network
activity, the network traffic is switch from the assigned to
virtual network device, before the migration starts. The
network traffic is switched back after the guest starts up at
the destination. For the local timer interrupt, the direct
timer interrupt delivery is switched back to the indirect
delivery with the help of \texttt{hrtimer} object and TMICT
WRMSR VM exit is enabled, before the migration. The changes
are reverted after the guest starts up at the destination.

\subsubsection{NIC Bonding Between the Assigned and Virtual NIC}
\input{temp/migration_nic_bonding}

\subsubsection{Migration with Shared-PID DTID}
The direct timer interrupt delivery depends the following
factors. First, the host shares the posted-interrupt
descriptor with the guest. When the guest udpates the
timer-interrupt bit of posted-interrupt request through EPT,
it does not trigger the EPT violations and the local timer
interrupt is delivered as the posted interrupt. Second, the
guest directly configures the timer initial count register
without a WRMSR VM exit. This is achieved by updating the MSR
bitmap of VM control structure. Third, the guest needs to
correctly compute its next timer events with the
host-calibrated LAPIC timer. To compute the next timer event
from the nano-seconds to the clock cycles, the multiplication
and shift factor of the calibrated timer are required. The
host needs to convey such information to the guest. The guest
configures the TMICT with the correct timer event in clock
cycles.

Before the migration starts, both host and guest need to tear
down the DTID. In the host, it enables the TMICT WRMSR VM exit
by updating the MSR bitmap of VMCS and configures the LAPIC
time to fire the timer interrupt rather than the
posted-interrupt notification. It needs to notify the guest to
tear down its DTID. Upon receving the notification, the guest
requests the host to unmap the shared PID and restores the
timer multiplication and shift back in order to correctly
compute the timer duration. Such a timer duration is conveyed
to the host \texttt{hrtimer} object when the host handles
TMICT WRMSR VM exit. After the guest starts at the
destination, both the guest and host rebuild the DTID. The
process of rebuilding the DTID is the reverse of
aforementioned steps.
